{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"QSeaBattle","text":"<p>A quantum-inspired coordination game framework</p>"},{"location":"#what-is-qseabattle","title":"What is QSeaBattle?","text":"<p>QSeaBattle is a research and experimentation framework for studying how agents coordinate under severe communication constraints.</p> <p>Two cooperative players must make a correct decision about a hidden battlefield, but:</p> <ul> <li>One player sees everything</li> <li>The other sees almost nothing</li> <li>Only a few classical bits may be communicated</li> </ul> <p>The framework explores when and how (post)quantum-inspired correlations allow agents to outperform purely classical strategies.</p> <p>Throughout QSeaBattle, coordination beyond direct communication is modeled via shared resources (SR): abstract, pre-established correlations available to both players without signaling. Specific mechanisms\u2014such as classical correlation, quantum entanglement, or Popescu-Rohrlich type correlations\u2014are concrete realizations of shared resources. The framework deliberately avoids treating shared randomness as a primitive; all assisted strategies are expressed uniformly in terms of SR under strict information-flow constraints.</p>"},{"location":"#the-core-challenge","title":"The Core Challenge","text":"<p>At the heart of QSeaBattle is a simple but deep problem:</p> <p>How much coordination is possible with limited communication?</p> <p>In each game:</p> <ul> <li>Player A observes an \\( n \\times n \\) binary battlefield</li> <li>Player A sends only \\( m \\ll n^2 \\) bits to Player B</li> <li>Player B sees a gun position and must decide whether to shoot</li> <li>Success means guessing the correct value at that position</li> </ul> <p>This setup connects directly to foundational questions in: - Information theory - Quantum correlations - Communication complexity - Multi-agent learning</p>"},{"location":"#why-qseabattle","title":"Why QSeaBattle?","text":"<p>QSeaBattle sits at the intersection of theory, simulation, and learning.</p>"},{"location":"#theory","title":"Theory","text":"<ul> <li>Test information-theoretic limits such as Information Causality</li> <li>Study quantum-like advantages using classical simulations</li> <li>Explore Bell-type correlations and CHSH-style behaviors</li> </ul>"},{"location":"#multi-agent-learning","title":"Multi-Agent Learning","text":"<ul> <li>Emergent communication under bandwidth constraints</li> <li>Imitation learning from optimal classical strategies</li> <li>Reinforcement learning with discrete communication</li> <li>Differentiable inter-agent communication (DIAL / DRU)</li> </ul>"},{"location":"#neural-symbolic-methods","title":"Neural + Symbolic Methods","text":"<ul> <li>Combine known algorithms with trainable neural components</li> <li>Inject inductive bias via structured architectures</li> <li>Study when learning can rediscover known quantum-inspired protocols</li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<p>QSeaBattle is organized as a progressive stack, from simple baselines to advanced trainable agents.</p> <ol> <li> <p>Core Game Infrastructure    Game environment, evaluation, reproducibility, and logging</p> </li> <li> <p>Deterministic Baselines    Hand-crafted classical strategies for verification and comparison</p> </li> <li> <p>Classical Neural Players    Trainable agents without shared resources</p> </li> <li> <p>Assisted (Quantum-Inspired) Players    Classical simulation of quantum correlations using shared randomness</p> </li> <li> <p>Trainable Assisted Players    Neural architectures combining learning with quantum-inspired structure</p> </li> </ol>"},{"location":"#what-makes-qseabattle-different","title":"What Makes QSeaBattle Different?","text":"<ul> <li>Easy to install and use</li> <li>Exact reproducibility  </li> <li>Clean separation of theory and learning  </li> <li>Designed for experimentation and analysis  </li> </ul> <p>QSeaBattle is built to support ablation studies, architecture comparisons, curriculum learning, and theoretical benchmarking.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>If you\u2019re new, start here:</p> <ul> <li>Quick Start \u2013 run your first game</li> <li>Game Mechanics \u2013 understand the rules and parameters</li> <li>Deterministic Players \u2013 classical baselines</li> <li>Neural Players \u2013 learning under constraints</li> <li>Assisted Players \u2013 quantum-inspired coordination</li> <li>Training Guides \u2013 imitation and reinforcement learning</li> <li>Theory \u2013 information-theoretic foundations</li> </ul> <p>Use the navigation on the left to dive in.</p>"},{"location":"#who-is-this-for","title":"Who Is This For?","text":"<ul> <li>Researchers in quantum information</li> <li>ML practitioners working on multi-agent systems</li> <li>Students exploring communication complexity</li> <li>Anyone curious about the boundary between classical and quantum coordination</li> </ul>"},{"location":"#design-philosophy","title":"Design Philosophy","text":"<ul> <li>Modular \u2014 components are independent and composable  </li> <li>Transparent \u2014 no hidden state or magic  </li> <li>Progressive \u2014 complexity increases in layers  </li> <li>Research-ready \u2014 logging, reproducibility, and analysis built in  </li> </ul>"},{"location":"#ready-to-explore","title":"Ready to Explore?","text":"<p>Start with the one of the tutorials:</p> <ul> <li>Tutorial 1 \u2013 Quick Start Guide</li> <li>Tutorial 2 \u2013 Imitation training Neural Net Players</li> <li>Tutorial 3 \u2013 Self learning Neural Net Players</li> <li>Tutorial 4 \u2013 Trainable model with (post)quantum resources</li> <li>Tutorial 5 \u2013 Alternative trainable model with (post)quantum resources</li> </ul> <p>QSeaBattle is designed to reward curiosity \u2014 whether theoretical or practical.</p>"},{"location":"algorithms/","title":"Algorithms","text":""},{"location":"algorithms/#purpose","title":"Purpose","text":"<p>This page defines the normative algorithms for QSeaBattle player families and the shared resource (SR) interface. The goal is to make the reference behavior independently re-implementable without reading Python source.</p> <p>Note</p> <p>If any behavior in code contradicts this page, the code is considered incorrect.</p>"},{"location":"algorithms/#scope","title":"Scope","text":"<p>This page specifies:</p> <ul> <li>Core game-level semantics needed by algorithms.</li> <li>Deterministic classical baselines (Simple, Majority).</li> <li>Assisted algorithms using SR (shared resource).</li> <li>Trainable assisted algorithms (Lin and Pyr) as constrained implementations of the same information flow.</li> <li>SR semantics and sampling modes.</li> </ul> <p>This page does not specify:</p> <ul> <li>Training procedures (see Training pages).</li> <li>Implementation details of TensorFlow/Keras layers.</li> <li>Logging, tournaments, or visualization.</li> </ul>"},{"location":"algorithms/#notation-and-symbols","title":"Notation and symbols","text":"<ul> <li><code>field_size</code>: integer \\(n \\ge 1\\).</li> <li><code>n2</code>: integer \\(n^2\\).</li> <li><code>comms_size</code>: integer \\(m\\) with \\(1 \\le m \\le n2\\).</li> <li><code>field</code>: binary vector of length <code>n2</code>, flattened in a fixed order.</li> <li><code>gun</code>: one-hot vector of length <code>n2</code> indicating the queried cell index.</li> <li><code>comm</code>: communicated message from Player A to Player B, binary vector of length <code>m</code> (Lin) or a single bit (Pyr).</li> <li><code>sr</code>: shared resource value(s) available to both players without signaling.</li> </ul> <p>Shapes used throughout:</p> <ul> <li><code>field</code>: <code>np.ndarray, dtype int {0,1}, shape (n2,)</code> or <code>tf.Tensor, dtype float32, shape (B, n2)</code>.</li> <li><code>gun</code>: <code>np.ndarray, dtype int {0,1}, shape (n2,)</code> one-hot or <code>tf.Tensor, dtype float32, shape (B, n2)</code> one-hot.</li> <li><code>comm</code>: <code>np.ndarray, dtype int {0,1}, shape (m,)</code> or <code>tf.Tensor, dtype float32, shape (B, m)</code>.</li> <li><code>shoot</code>: scalar decision bit <code>np.ndarray, dtype int {0,1}, shape (1,)</code> or <code>tf.Tensor, dtype float32, shape (B, 1)</code>.</li> </ul> <p>Warning</p> <p>All math symbols MUST be interpreted using the variable names above. Do not mix different meanings of <code>m</code>, <code>n2</code>, <code>field_size</code>.</p>"},{"location":"algorithms/#shared-resource-sr","title":"Shared resource (SR)","text":""},{"location":"algorithms/#definition","title":"Definition","text":"<p>SR (shared resource) is any pre-established auxiliary resource accessible to both Player A and Player B without communication and without signaling. SR may be classical, post-quantum, or simulated.</p> <p><code>PRAssistedLayer</code> is one concrete SR mechanism that provides structured correlations.</p>"},{"location":"algorithms/#sr-interface-contract","title":"SR interface contract","text":"<p>SR MUST satisfy:</p> <ul> <li>No signaling: Player B MUST NOT obtain information about <code>field</code> except via <code>comm</code> and SR that is independent of <code>field</code> given the chosen SR mode.</li> <li>Symmetry: Player A and Player B MUST interpret SR values in the same indexing convention.</li> <li>Mode control: Any <code>sr_mode</code> parameter MUST be defined as a choice of shared resource mechanism, not \"randomness\".</li> </ul> <p>SR MAY be used in one of two modes:</p> <ul> <li>Expected-mode: SR outcomes are replaced by their expectation under the SR distribution (deterministic, differentiable).</li> <li>Sample-mode: SR outcomes are sampled (stochastic).</li> </ul> <p>Preconditions:</p> <ul> <li>SR configuration is fixed before a game begins.</li> <li>SR does not depend on runtime observations (<code>field</code>, <code>gun</code>) except through allowed conditional selection rules described below.</li> </ul> <p>Postconditions:</p> <ul> <li>SR usage preserves the information flow constraints described in Invariants.</li> </ul> <p>Errors:</p> <ul> <li>Any SR mechanism that can encode <code>field</code> into SR outcomes is invalid.</li> </ul>"},{"location":"algorithms/#core-game-semantics-used-by-all-algorithms","title":"Core game semantics used by all algorithms","text":""},{"location":"algorithms/#game-inputs-and-outputs","title":"Game inputs and outputs","text":"<p>For each game instance:</p> <ul> <li>Player A receives <code>field</code>.</li> <li>Player B receives <code>gun</code>.</li> <li>Player A sends <code>comm</code> to Player B.</li> <li>Player B outputs <code>shoot</code> as a guess of the battlefield value at the gun index.</li> </ul> <p>Success condition (conceptual):</p> <ul> <li>Let <code>k = argmax(gun)</code> for one-hot <code>gun</code>.</li> <li>The game is won if <code>shoot == field[k]</code>.</li> </ul> <p>Note</p> <p>Algorithms below specify how <code>comm</code> and <code>shoot</code> are computed; the environment defines win/loss bookkeeping.</p>"},{"location":"algorithms/#deterministic-baseline-algorithms","title":"Deterministic baseline algorithms","text":""},{"location":"algorithms/#simple-coverage-strategy","title":"Simple (coverage) strategy","text":"<p>Intent: communicate <code>m</code> cells directly and guess randomly outside coverage.</p> <p>Algorithm:</p> <ul> <li>Partition indices into a fixed agreed set <code>C</code> of size <code>m</code> and its complement.</li> <li>Player A sets <code>comm[j] = field[C[j]]</code> for \\(j = 0..m-1\\).</li> <li>Player B computes <code>k = argmax(gun)</code>.</li> <li>If <code>k</code> is in <code>C</code>, Player B outputs the matching communicated bit.</li> <li>Else Player B outputs <code>0</code> or <code>1</code> using a fixed baseline rule (for example, always <code>0</code>, or a fixed prior).</li> </ul> <p>Preconditions:</p> <ul> <li><code>1 &lt;= m &lt;= n2</code>.</li> <li>Both players share the same ordered index set <code>C</code>.</li> </ul> <p>Postconditions:</p> <ul> <li>If <code>k</code> is in coverage, <code>shoot</code> matches <code>field[k]</code> in noiseless communication.</li> </ul> <p>Errors:</p> <ul> <li><code>ValueError</code> if <code>m</code> invalid or coverage mapping inconsistent.</li> </ul>"},{"location":"algorithms/#majority-segment-majority-strategy","title":"Majority (segment-majority) strategy","text":"<p>Intent: communicate one bit per segment of the field.</p> <p>Algorithm:</p> <ul> <li>Partition the flattened field indices into <code>m</code> contiguous segments of equal length <code>L = n2 / m</code> (requires <code>m | n2</code>).</li> <li>Player A computes, for each segment, the majority bit (ties map to <code>1</code>).</li> <li>Player A sends these <code>m</code> bits as <code>comm</code>.</li> <li>Player B computes <code>k = argmax(gun)</code> and determines which segment contains <code>k</code>.</li> <li>Player B outputs the corresponding segment bit.</li> </ul> <p>Preconditions:</p> <ul> <li><code>field_size &gt;= 1</code>.</li> <li><code>1 &lt;= m &lt;= n2</code>.</li> <li><code>m | n2</code>.</li> </ul> <p>Postconditions:</p> <ul> <li><code>comm</code> is deterministic given <code>field</code>.</li> <li><code>shoot</code> depends only on <code>comm</code> and <code>gun</code>.</li> </ul> <p>Errors:</p> <ul> <li><code>ValueError</code> if <code>m</code> does not divide <code>n2</code>.</li> </ul>"},{"location":"algorithms/#assisted-algorithms-with-sr","title":"Assisted algorithms with SR","text":"<p>This section specifies algorithms that may outperform purely classical deterministic baselines by using SR correlations while respecting no-signaling constraints.</p>"},{"location":"algorithms/#assisted-lin-algorithm-family","title":"Assisted (Lin) algorithm family","text":"<p>This family uses <code>m</code>-bit communication with linear-style primitives.</p>"},{"location":"algorithms/#lin-teacher-primitives","title":"Lin teacher primitives","text":"<p>Teacher layers define reference transformations:</p> <ul> <li>Measurement-A: produces <code>meas_a</code> from <code>field</code>.</li> <li>Combine-A: produces <code>comm</code> from <code>meas_a</code> and SR outcomes.</li> <li>Measurement-B: produces <code>meas_b</code> from <code>gun</code>.</li> <li>Combine-B: produces <code>shoot</code> from <code>meas_b</code>, SR outcomes, and <code>comm</code>.</li> </ul> <p>Types and shapes (batch form):</p> <ul> <li><code>field</code>: <code>tf.Tensor, dtype float32, shape (B, n2)</code>.</li> <li><code>gun</code>: <code>tf.Tensor, dtype float32, shape (B, n2)</code> one-hot.</li> <li><code>meas_a</code>: <code>tf.Tensor, dtype float32, shape (B, n2)</code> (Lin design default).</li> <li><code>meas_b</code>: <code>tf.Tensor, dtype float32, shape (B, n2)</code> (Lin design default).</li> <li><code>comm</code>: <code>tf.Tensor, dtype float32, shape (B, m)</code>.</li> <li><code>shoot</code>: <code>tf.Tensor, dtype float32, shape (B, 1)</code>.</li> </ul> <p>Note</p> <p>Lin exact measurement/combine semantics depend on the selected Lin reference strategy (for example, parity prototype). The teacher primitives MUST be the single source of truth for these semantics.</p>"},{"location":"algorithms/#lin-end-to-end-reference-flow","title":"Lin end-to-end reference flow","text":"<p>Algorithm (single sample):</p> <ul> <li> <p>Player A:</p> </li> <li> <p>Compute <code>meas_a = MeasureA(field)</code>.</p> </li> <li>Obtain SR outcomes required by Combine-A according to <code>sr_mode</code>.</li> <li>Compute <code>comm = CombineA(meas_a, sr_outcomes_a)</code>.</li> <li> <p>Player B:</p> </li> <li> <p>Compute <code>meas_b = MeasureB(gun)</code>.</p> </li> <li>Obtain SR outcomes required by Combine-B according to <code>sr_mode</code>.</li> <li>Compute <code>shoot = CombineB(meas_b, sr_outcomes_b, comm)</code>.</li> </ul> <p>Preconditions:</p> <ul> <li><code>field_size &gt;= 1</code>.</li> <li><code>1 &lt;= m &lt;= n2</code>.</li> <li>SR configuration is compatible with the Combine rules.</li> </ul> <p>Postconditions:</p> <ul> <li><code>comm</code> depends on <code>field</code> only through MeasureA and CombineA.</li> <li><code>shoot</code> depends on <code>field</code> only through <code>comm</code> and SR outcomes.</li> </ul> <p>Errors:</p> <ul> <li><code>ValueError</code> on shape mismatch or invalid SR configuration.</li> </ul>"},{"location":"algorithms/#assisted-pyr-algorithm-family","title":"Assisted (Pyr) algorithm family","text":"<p>This family uses a pyramid reduction structure and typically enforces <code>comms_size = 1</code>.</p>"},{"location":"algorithms/#pyramid-structural-constraints","title":"Pyramid structural constraints","text":"<ul> <li><code>n2 = field_size^2</code> MUST be a power of two.</li> <li><code>comms_size</code> MUST equal <code>1</code>.</li> <li>The pyramid has <code>K = log2(n2)</code> levels.</li> <li>Level \\(l\\) has active length \\(L_l = n2 / 2^l\\) for \\(l = 0..K-1\\).</li> <li>Each level maps length \\(L\\) to \\(L/2\\).</li> </ul> <p>Preconditions:</p> <ul> <li><code>field_size &gt;= 1</code>.</li> <li><code>n2</code> is a power of two.</li> <li><code>m = 1</code>.</li> </ul> <p>Errors:</p> <ul> <li><code>ValueError</code> if <code>n2</code> not power of two or <code>m != 1</code>.</li> </ul>"},{"location":"algorithms/#pyr-teacher-primitives","title":"Pyr teacher primitives","text":"<p>At each level:</p> <ul> <li>Measurement-A: <code>meas_a_l = MeasureA_l(field_l)</code> where <code>field_l</code> has length \\(L\\) and output has length \\(L/2\\).</li> <li>Combine-A: <code>field_{l+1} = CombineA_l(field_l, sr_outcome_l)</code> output length \\(L/2\\).</li> <li>Measurement-B: <code>meas_b_l = MeasureB_l(gun_l)</code> input length \\(L\\), output length \\(L/2\\).</li> <li>Combine-B: <code>(gun_{l+1}, comm_{l+1}) = CombineB_l(gun_l, sr_outcome_l, comm_l)</code> where <code>comm_l</code> is a single bit.</li> </ul> <p>Types and shapes (single sample):</p> <ul> <li><code>field_l</code>: <code>np.ndarray, dtype int {0,1}, shape (L,)</code>.</li> <li><code>gun_l</code>: <code>np.ndarray, dtype int {0,1}, shape (L,)</code> one-hot.</li> <li><code>sr_outcome_l</code>: <code>np.ndarray, dtype int {0,1}, shape (L/2,)</code>.</li> <li><code>comm_l</code>: <code>np.ndarray, dtype int {0,1}, shape (1,)</code>.</li> </ul>"},{"location":"algorithms/#pyr-end-to-end-reference-flow","title":"Pyr end-to-end reference flow","text":"<p>Algorithm (single sample):</p> <ul> <li> <p>Initialize:</p> </li> <li> <p><code>field_0 = field</code> reshaped/flattened to length <code>n2</code>.</p> </li> <li><code>gun_0 = gun</code> one-hot length <code>n2</code>.</li> <li><code>comm_0</code> is a single bit initialized by Player A at level 0 (teacher-defined rule).</li> <li> <p>For each level \\(l = 0..K-1\\):</p> </li> <li> <p>Player A:</p> <ul> <li>Compute <code>meas_a_l = MeasureA_l(field_l)</code>.</li> <li>Obtain SR outcomes <code>sr_outcome_l</code> for this level.</li> <li>Compute <code>field_{l+1} = CombineA_l(field_l, sr_outcome_l)</code>.</li> <li>Update <code>comm_l</code> according to the Pyr-A teacher rule and send the current <code>comm_l</code> (or final <code>comm_K</code>) to Player B depending on the protocol definition.</li> <li> <p>Player B:</p> </li> <li> <p>Compute <code>meas_b_l = MeasureB_l(gun_l)</code>.</p> </li> <li>Obtain SR outcomes <code>sr_outcome_l</code> for this level.</li> <li>Compute <code>(gun_{l+1}, comm_{l+1}) = CombineB_l(gun_l, sr_outcome_l, comm_l)</code>.</li> <li>Output:</li> </ul> </li> <li> <p>Player B outputs <code>shoot = comm_K</code> or the protocol-defined final bit.</p> </li> </ul> <p>Warning</p> <p>The exact rule for how <code>comm</code> is initialized and when it is transmitted MUST match the Pyr teacher implementation used for dataset generation.</p>"},{"location":"algorithms/#trainable-assisted-models-as-constrained-implementations","title":"Trainable assisted models as constrained implementations","text":""},{"location":"algorithms/#teacher-vs-trainable-relationship","title":"Teacher vs trainable relationship","text":"<p>For both Lin and Pyr:</p> <ul> <li>Teacher primitives define the reference mapping used to generate supervised targets.</li> <li>Trainable models MUST be architectures that cannot violate the information flow constraints and are trained to approximate the teacher mapping.</li> </ul> <p>Normative requirement:</p> <ul> <li> <p>A trainable assisted model MUST NOT have any input path that bypasses:</p> </li> <li> <p><code>field -&gt; comm</code> only through Player A,</p> </li> <li><code>comm + gun -&gt; shoot</code> only through Player B,</li> <li>SR usage only through SR interfaces.</li> </ul>"},{"location":"algorithms/#equivalence-targets","title":"Equivalence targets","text":"<p>When trained on the corresponding imitation datasets:</p> <ul> <li><code>LinTrainableAssistedModelA</code> SHOULD approximate <code>Lin teacher A</code> mapping: <code>field -&gt; comm</code>.</li> <li><code>LinTrainableAssistedModelB</code> SHOULD approximate <code>Lin teacher B</code> mapping: <code>(gun, comm) -&gt; shoot</code>.</li> <li><code>PyrTrainableAssistedModelA</code> SHOULD approximate <code>Pyr teacher A</code> per-level mappings.</li> <li><code>PyrTrainableAssistedModelB</code> SHOULD approximate <code>Pyr teacher B</code> per-level mappings.</li> </ul> <p>Note</p> <p>This page does not claim optimality of learned models. It specifies the intended target behavior and constraints.</p>"},{"location":"algorithms/#preconditions-postconditions-errors-summary","title":"Preconditions, postconditions, errors summary","text":""},{"location":"algorithms/#global-preconditions","title":"Global preconditions","text":"<ul> <li><code>field_size &gt;= 1</code>.</li> <li><code>n2 = field_size^2</code>.</li> <li><code>1 &lt;= comms_size &lt;= n2</code> unless specified otherwise.</li> <li>For Pyr: <code>n2</code> power of two and <code>comms_size = 1</code>.</li> <li>All bit-vectors are binary in <code>{0,1}</code> (or float32 equivalents <code>{0.0,1.0}</code> for TF).</li> </ul>"},{"location":"algorithms/#global-postconditions","title":"Global postconditions","text":"<ul> <li>Player B output <code>shoot</code> is a single bit.</li> <li>Player B uses only <code>gun</code>, received <code>comm</code>, and SR outcomes.</li> <li>SR does not violate no-signaling constraints.</li> </ul>"},{"location":"algorithms/#global-errors","title":"Global errors","text":"<ul> <li><code>ValueError</code> for invalid shapes, invalid parameter domains, or violated structural constraints.</li> <li><code>RuntimeError</code> for detected internal inconsistency (for example, mismatched level lists between Player A and Player B).</li> </ul>"},{"location":"algorithms/#testing-hooks","title":"Testing hooks","text":"<p>Suggested invariants to test algorithm correctness without training:</p> <ul> <li>Majority segmentation covers <code>[0, n2)</code> with no overlaps and total length <code>n2</code>.</li> <li>For Pyr: level sizes are <code>[n2, n2/2, ..., 2]</code> and count is <code>log2(n2)</code>.</li> <li>For Pyr: each level halves the active length for both field and gun representations.</li> <li>For Lin: <code>comm</code> has shape <code>(m,)</code> and <code>shoot</code> has shape <code>(1,)</code> for single-sample execution.</li> <li>No-signaling smoke test: holding <code>comm</code> fixed, varying <code>field</code> MUST NOT change Player B output distribution in expected-mode SR.</li> </ul>"},{"location":"algorithms/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>A fully explicit pseudocode listing for each teacher primitive (Measure/Combine for Lin and Pyr) may be added if the reference teachers are updated or extended.</li> </ul>"},{"location":"algorithms/#deviations","title":"Deviations","text":"<ul> <li>None.</li> </ul>"},{"location":"algorithms/#changelog","title":"Changelog","text":"<ul> <li>2026-01-16 (Rob Hendriks): Initial <code>algorithms.md</code> drafted as normative baseline for assisted and trainable-assisted algorithm families.</li> </ul>"},{"location":"assisted_player_a/","title":"AssistedPlayerA","text":"<p>Role: Player A implementation that computes a single communication bit from a flattened binary field using iterative compression and shared randomness resources.</p> <p>Location: <code>Q_Sea_Battle.Archive.assisted_player_a.AssistedPlayerA</code></p>"},{"location":"assisted_player_a/#derived-constraints","title":"Derived constraints","text":"<ul> <li>Let field_size be <code>self.game_layout.field_size</code> (int, constraints Unknown), and let \\(n2 = field\\_size^2\\) (int, \\(n2 \\ge 0\\)).</li> <li>Input <code>field</code> must be a 1D array of length \\(n2\\) with values in <code>{0,1}</code> (enforced at runtime).</li> <li>At each compression level, the intermediate field length must be even (enforced at runtime); successful completion therefore requires \\(n2\\) to be a power of 2 and \\(n2 \\ge 1\\) (not explicitly checked up-front, but otherwise a <code>ValueError</code> or <code>RuntimeError</code> will occur).</li> </ul>"},{"location":"assisted_player_a/#constructor","title":"Constructor","text":"Parameter Type Description game_layout <code>GameLayout</code>, constraints Unknown Game configuration; stored via <code>PlayerA.__init__(game_layout)</code> and later used for <code>field_size</code>. parent <code>\"AssistedPlayers\"</code>, must be instance of <code>AssistedPlayers</code> Owning factory providing access to shared randomness boxes; validated with <code>isinstance</code> and stored as <code>self.parent</code>. <p>Preconditions</p> <ul> <li><code>parent</code> is an instance of <code>AssistedPlayers</code>.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.game_layout</code> is initialized by <code>PlayerA</code>.</li> <li><code>self.parent</code> is set to <code>parent</code>.</li> </ul> <p>Errors</p> <ul> <li><code>TypeError</code>: if <code>parent</code> is not an <code>AssistedPlayers</code> instance.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.Archive.assisted_player_a import AssistedPlayerA\nfrom Q_Sea_Battle.Archive.game_layout import GameLayout\nfrom Q_Sea_Battle.Archive.assisted_players import AssistedPlayers\n\nlayout = GameLayout(...)  # Not specified in this module\nfactory = AssistedPlayers(...)  # Not specified in this module\nplayer_a = AssistedPlayerA(game_layout=layout, parent=factory)\n</code></pre>"},{"location":"assisted_player_a/#public-methods","title":"Public Methods","text":""},{"location":"assisted_player_a/#decide","title":"decide","text":"<p>Signature: <code>decide(self, field: np.ndarray, supp: Any | None = None) -&gt; np.ndarray</code></p> <p>Compute the communication bit from the field by iteratively pairing bits, constructing a measurement array, querying shared randomness for an outcome, and collapsing until a single bit remains.</p> <p>Parameters</p> <ul> <li><code>field</code>: <code>np.ndarray, dtype int, values {0,1}, shape (n2,)</code> where \\(n2 = field\\_size^2\\); will be converted via <code>np.asarray(field, dtype=int)</code>.</li> <li><code>supp</code>: <code>Any | None</code>, constraints Unknown; unused (explicitly deleted).</li> </ul> <p>Returns</p> <ul> <li><code>np.ndarray, dtype int, values {0,1}, shape (1,)</code>; the single communication bit as a length-1 array.</li> </ul> <p>Preconditions</p> <ul> <li><code>field</code> is 1D with <code>field.shape[0] == n2</code>.</li> <li><code>field</code> contains only 0/1 values.</li> <li>For all iterations, <code>intermediate_field.size</code> is even (implies an even-length progression down to size 1).</li> </ul> <p>Postconditions</p> <ul> <li>Returns a length-1 array containing the final collapsed bit.</li> </ul> <p>Errors</p> <ul> <li><code>ValueError</code>: if <code>field</code> is not 1D or does not have length <code>n2</code>.</li> <li><code>ValueError</code>: if <code>field</code> contains values outside <code>{0,1}</code>.</li> <li><code>ValueError</code>: if <code>intermediate_field</code> length is odd at any compression level.</li> <li><code>RuntimeError</code>: if the final <code>intermediate_field</code> does not have length 1.</li> </ul> <p>Example</p> <pre><code>import numpy as np\nfrom Q_Sea_Battle.Archive.assisted_player_a import AssistedPlayerA\n\n# player_a is an AssistedPlayerA instance.\n# n2 must equal player_a.game_layout.field_size ** 2.\nfield = np.zeros((player_a.game_layout.field_size ** 2,), dtype=int)\ncomm = player_a.decide(field)\nassert comm.shape == (1,)\nassert comm.dtype == int\n</code></pre>"},{"location":"assisted_player_a/#data-state","title":"Data &amp; State","text":"<ul> <li><code>parent</code>: <code>AssistedPlayers</code>, constraints Unknown; set in the constructor and used by <code>decide</code> via <code>self.parent.shared_randomness(level)</code>.</li> <li><code>game_layout</code>: <code>GameLayout</code>, constraints Unknown; inherited from <code>PlayerA</code> and used by <code>decide</code> to compute <code>n2 = field_size ** 2</code>.</li> </ul>"},{"location":"assisted_player_a/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"assisted_player_a/#deviations","title":"Deviations","text":"<ul> <li>No design notes were provided; deviations are not specified.</li> </ul>"},{"location":"assisted_player_a/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li><code>decide</code> assumes that repeated halving reaches length 1 without remainder; if this class is used with layouts where \\(n2\\) is not a power of 2, the method may raise <code>ValueError</code> (odd intermediate length) or <code>RuntimeError</code> (final length not 1).</li> <li>The implementation relies on <code>self.parent.shared_randomness(level)</code> returning an object that supports <code>measurement_a(measurement)</code> where <code>measurement</code> is <code>np.ndarray, dtype int, shape (half,)</code> and the returned <code>outcome_a</code> is indexable with length <code>half</code>; the concrete interface is defined outside this module.</li> </ul>"},{"location":"assisted_player_a/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.Archive.players_base.PlayerA</code> (base class; behavior not specified in this module)</li> <li><code>Q_Sea_Battle.Archive.game_layout.GameLayout</code> (layout providing <code>field_size</code>)</li> <li><code>Q_Sea_Battle.Archive.assisted_players.AssistedPlayers</code> (provider of shared randomness; imported locally in <code>__init__</code>)</li> </ul>"},{"location":"assisted_player_a/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version as indicated by module docstring; implements <code>AssistedPlayerA</code> with iterative compression using shared randomness.</li> </ul>"},{"location":"assisted_player_b/","title":"AssistedPlayerB","text":"<p>Role: Player B implementation that decides to shoot using hierarchical shared randomness outcomes plus a communication bit.</p> <p>Location: <code>Q_Sea_Battle.Archive.assisted_player_b.AssistedPlayerB</code></p>"},{"location":"assisted_player_b/#constructor","title":"Constructor","text":"Parameter Type Description game_layout GameLayout, constraints: not specified, shape: not applicable Game configuration passed to the base <code>PlayerB</code> constructor. parent AssistedPlayers, constraints: must be an instance of <code>AssistedPlayers</code>, shape: not applicable Owning factory providing access to shared randomness boxes; validated with <code>isinstance</code>. <p>Preconditions: <code>parent</code> must be an instance of <code>AssistedPlayers</code>. Postconditions: <code>self.parent</code> is set to <code>parent</code>; <code>PlayerB</code> base initialization is completed with <code>game_layout</code>. Errors: Raises <code>TypeError</code> if <code>parent</code> is not an <code>AssistedPlayers</code> instance. Example:</p> <p>Construct an AssistedPlayerB</p> <pre><code>from Q_Sea_Battle.Archive.assisted_player_b import AssistedPlayerB\n# from Q_Sea_Battle.Archive.assisted_players import AssistedPlayers\n# from Q_Sea_Battle.Archive.game_layout import GameLayout\n# game_layout = GameLayout(...)\n# parent = AssistedPlayers(...)\n# player_b = AssistedPlayerB(game_layout=game_layout, parent=parent)\n</code></pre>"},{"location":"assisted_player_b/#public-methods","title":"Public Methods","text":""},{"location":"assisted_player_b/#decidegun-comm-suppnone","title":"decide(gun, comm, supp=None)","text":"<p>Decide whether to shoot by tracing a one-hot <code>gun</code> vector through successive pairwise reductions, querying one shared randomness box per level, collecting one outcome bit per level at the active pair index, then XOR-ing (parity) all collected bits with the communication bit.</p> <p>Parameters:</p> <ul> <li><code>gun</code>: np.ndarray, dtype int {0,1}, shape (n2,), constraints: must be 1D; length must equal \\(n2 = \\mathrm{field\\_size}^2\\); must be one-hot (sum equals 1).</li> <li><code>comm</code>: np.ndarray, dtype int {0,1}, shape (1,), constraints: must be 1D; length must equal 1.</li> <li><code>supp</code>: Any | None, constraints: unused (deleted), shape: not applicable.</li> </ul> <p>Returns:</p> <ul> <li>int, constraints: in {0,1}; 1 means shoot, 0 means do not shoot; shape: scalar.</li> </ul> <p>Errors:</p> <ul> <li>Raises <code>ValueError</code> if <code>gun</code> is not 1D of length <code>n2</code>, contains values not in {0,1}, or is not one-hot.</li> <li>Raises <code>ValueError</code> if <code>comm</code> is not 1D of length 1 or contains values not in {0,1}.</li> <li>Raises <code>ValueError</code> during processing if <code>intermediate_gun</code> has odd length at any level, ceases to be one-hot, has zero or more than one active pair (pair equal to (0,1) or (1,0)), or if <code>measurement</code> (a.k.a. measurement string) has sum not in {0,1}.</li> </ul> <p>Example:</p> <p>Decide with one-hot gun and one-bit comm</p> <pre><code>import numpy as np\n# player_b = AssistedPlayerB(game_layout=..., parent=...)\n# n2 = player_b.game_layout.field_size ** 2\n# gun = np.zeros(n2, dtype=int)\n# gun[0] = 1\n# comm = np.array([1], dtype=int)\n# shoot = player_b.decide(gun=gun, comm=comm)\n# assert shoot in (0, 1)\n</code></pre>"},{"location":"assisted_player_b/#data-state","title":"Data &amp; State","text":"<ul> <li><code>parent</code>: AssistedPlayers, constraints: set at construction and must satisfy <code>isinstance(parent, AssistedPlayers)</code> at initialization time; shape: not applicable.</li> <li><code>game_layout</code>: GameLayout, constraints: provided via base class <code>PlayerB</code>; shape: not applicable (exact storage and invariants in <code>PlayerB</code> are not specified in this module).</li> </ul>"},{"location":"assisted_player_b/#planned-design-spec","title":"Planned (design-spec)","text":"<p>Not specified.</p>"},{"location":"assisted_player_b/#deviations","title":"Deviations","text":"<p>Not specified.</p>"},{"location":"assisted_player_b/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>This module performs a local import of <code>AssistedPlayers</code> inside <code>__init__</code> to avoid import cycles; if refactoring imports, preserve cycle-free initialization behavior.</li> <li>The method enforces strict one-hot invariants at every reduction level; any changes to the shared randomness protocol must keep these checks aligned with the intended specification.</li> </ul>"},{"location":"assisted_player_b/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.Archive.game_layout.GameLayout</code></li> <li><code>Q_Sea_Battle.Archive.players_base.PlayerB</code></li> <li><code>Q_Sea_Battle.Archive.assisted_players.AssistedPlayers</code> (imported locally inside <code>__init__</code>)</li> </ul>"},{"location":"assisted_player_b/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version (per module docstring).</li> </ul>"},{"location":"assisted_players/","title":"AssistedPlayers","text":"<p>Role: Factory for assisted players with shared randomness. Location: <code>Q_Sea_Battle.Archive.assisted_players.AssistedPlayers</code></p>"},{"location":"assisted_players/#derived-constraints","title":"Derived constraints","text":"<ul> <li>Let field_size be <code>game_layout.field_size</code> (int, constraints: &gt; 0, scalar).</li> <li>Let comms_size be <code>game_layout.comms_size</code> (int, constraints: must equal 1, scalar).</li> <li>Let \\(n2 = field\\_size^2\\) (int, constraints: &gt; 0 and power of two, scalar).</li> <li>Let \\(n = \\log_2(n2)\\) (int, constraints: \\(2^n = n2\\), scalar).</li> <li>The internal shared randomness array length is \\(n\\) (int, constraints: \\(n \\ge 0\\), scalar), and the per-level lengths are \\([2^{n-1}, 2^{n-2}, \\ldots, 2^0]\\) (list[int], constraints: each &gt; 0 when \\(n&gt;0\\)).</li> </ul>"},{"location":"assisted_players/#constructor","title":"Constructor","text":"Parameter Type Description game_layout GameLayout, constraints: <code>comms_size == 1</code> and <code>field_size &gt; 0</code> and <code>field_size ** 2</code> is a power of two, scalar Game configuration. p_high float, constraints: Not specified, scalar Correlation parameter used for all shared resources. Stored as <code>float(p_high)</code>. <p>Preconditions</p> <ul> <li><code>game_layout.comms_size == 1</code>.</li> <li><code>n2 = game_layout.field_size ** 2</code> satisfies <code>n2 &gt; 0</code>.</li> <li><code>n2</code> is a power of two (bit-test check in <code>__init__</code>, and re-validated as an exact power of two inside <code>_create_shared_randomness_array()</code>).</li> </ul> <p>Postconditions</p> <ul> <li><code>self.game_layout</code> is initialized via <code>Players.__init__(game_layout)</code> (behavior of base class not specified here).</li> <li><code>self.p_high</code> is set to <code>float(p_high)</code> (float, scalar).</li> <li><code>self._shared_randomness_array</code> is created by <code>_create_shared_randomness_array()</code> (list[SharedRandomness], shape (n,)).</li> <li><code>self._playerA</code> and <code>self._playerB</code> are set to <code>None</code> (each is <code>AssistedPlayerA | None</code> / <code>AssistedPlayerB | None</code>, scalar).</li> </ul> <p>Errors</p> <ul> <li>Raises <code>ValueError</code> if <code>game_layout.comms_size != 1</code>.</li> <li>Raises <code>ValueError</code> if <code>game_layout.field_size ** 2 &lt;= 0</code>.</li> <li>Raises <code>ValueError</code> if <code>game_layout.field_size ** 2</code> is not a power of two (checked in <code>__init__</code>), or not an exact power of two (checked again in <code>_create_shared_randomness_array()</code>).</li> </ul> <p>Example</p> <p>Construct and retrieve players</p> <pre><code>from Q_Sea_Battle.Archive.assisted_players import AssistedPlayers\n\nassisted = AssistedPlayers(game_layout=layout, p_high=0.9)\nplayer_a, player_b = assisted.players()\n</code></pre>"},{"location":"assisted_players/#public-methods","title":"Public Methods","text":""},{"location":"assisted_players/#players","title":"players","text":"<ul> <li>Signature: <code>players(self) -&gt; Tuple[PlayerA, PlayerB]</code></li> <li>Returns: <code>tuple[PlayerA, PlayerB], constraints: length 2, shape (2,)</code> containing <code>(player_a, player_b)</code>.</li> <li>Behavior: Creates <code>AssistedPlayerA(self.game_layout, parent=self)</code> and <code>AssistedPlayerB(self.game_layout, parent=self)</code> on first call and caches them; later calls return the cached instances.</li> </ul>"},{"location":"assisted_players/#reset","title":"reset","text":"<ul> <li>Signature: <code>reset(self) -&gt; None</code></li> <li>Returns: <code>None, constraints: N/A, scalar</code>.</li> <li>Behavior: Recreates fresh shared randomness boxes by assigning <code>self._shared_randomness_array = self._create_shared_randomness_array()</code>; does not modify cached player references.</li> </ul>"},{"location":"assisted_players/#shared_randomness","title":"shared_randomness","text":"<ul> <li>Signature: <code>shared_randomness(self, index: int) -&gt; SharedRandomness</code></li> <li>Parameters:</li> <li><code>index</code>: <code>int, constraints: must be a valid index into internal array, scalar</code>.</li> <li>Returns: <code>SharedRandomness, constraints: element of internal array, scalar</code>.</li> <li>Errors: Raises <code>IndexError</code> if <code>index</code> is out of bounds (propagated from list indexing).</li> </ul>"},{"location":"assisted_players/#data-state","title":"Data &amp; State","text":"<ul> <li><code>p_high</code>: <code>float, constraints: Not specified, scalar</code>; correlation parameter used to construct <code>SharedRandomness</code> instances.</li> <li><code>_shared_randomness_array</code>: <code>list[SharedRandomness], constraints: length n where $n=\\log_2(field\\_size^2)$, shape (n,)</code>; created at construction and recreated on <code>reset()</code>.</li> <li><code>_playerA</code>: <code>AssistedPlayerA | None, constraints: None until first</code>players()<code>call, scalar</code>; cached.</li> <li><code>_playerB</code>: <code>AssistedPlayerB | None, constraints: None until first</code>players()<code>call, scalar</code>; cached.</li> </ul>"},{"location":"assisted_players/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"assisted_players/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"assisted_players/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li><code>_create_shared_randomness_array()</code> validates the exact power-of-two condition via <code>n = int(np.log2(n2))</code> and <code>2 ** n == n2</code>; this duplicates the earlier bit-test validation in <code>__init__</code>.</li> <li><code>reset()</code> recreates shared randomness resources but does not clear <code>_playerA</code>/<code>_playerB</code>; cached players will continue to reference the same <code>AssistedPlayers</code> parent, and will observe the updated shared resources only if they query via the parent at use time (the players\u2019 internal behavior is not specified in this module).</li> </ul>"},{"location":"assisted_players/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.Archive.players_base.Players</code></li> <li><code>Q_Sea_Battle.Archive.shared_randomness.SharedRandomness</code></li> <li><code>Q_Sea_Battle.Archive.assisted_player_a.AssistedPlayerA</code></li> <li><code>Q_Sea_Battle.Archive.assisted_player_b.AssistedPlayerB</code></li> <li><code>Q_Sea_Battle.Archive.game_layout.GameLayout</code></li> </ul>"},{"location":"assisted_players/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version (per module docstring).</li> </ul>"},{"location":"conventions/","title":"Conventions","text":""},{"location":"conventions/#purpose","title":"Purpose","text":"<p>This document defines global notation, terminology, and cross-cutting conventions used throughout the QSeaBattle specification.</p> <p>Unless explicitly marked as informative, statements in this file are normative.</p>"},{"location":"conventions/#notation","title":"Notation","text":""},{"location":"conventions/#sizes-and-shapes","title":"Sizes and shapes","text":"<ul> <li><code>field_size</code> is the board side length.</li> <li><code>n2 = field_size * field_size</code> is the total number of cells.</li> <li>Vectors representing the full field or gun are flattened to length <code>n2</code>.</li> <li>Batch dimension is denoted by <code>B</code>.</li> </ul> <p>Shape conventions - <code>field</code>: <code>np.ndarray, dtype int {0,1}, shape (n2,)</code> or <code>tf.Tensor, dtype float32, shape (B, n2)</code> - <code>gun</code>: <code>np.ndarray, dtype int {0,1}, shape (n2,)</code> (one-hot) or <code>tf.Tensor, dtype float32, shape (B, n2)</code> - <code>comm</code>: <code>np.ndarray, dtype int {0,1}, shape (m,)</code> or <code>tf.Tensor, dtype float32, shape (B, m)</code> - <code>shoot</code>: <code>int {0,1}</code> or <code>tf.Tensor, dtype float32, shape (B, 1)</code></p>"},{"location":"conventions/#binary-values","title":"Binary values","text":"<p>Unless stated otherwise: - NumPy binary arrays use values in <code>{0,1}</code>. - TensorFlow training utilities may use <code>float32</code> values in <code>{0.0, 1.0}</code>.</p>"},{"location":"conventions/#terminology","title":"Terminology","text":""},{"location":"conventions/#game-vs-tournament","title":"Game vs tournament","text":"<ul> <li>A game is one execution of the pipeline: generate field and gun -&gt; A decides -&gt; noise -&gt; B decides -&gt; reward.</li> <li>A tournament is a batch of games under identical layout parameters.</li> </ul>"},{"location":"conventions/#players-and-models","title":"Players and models","text":"<ul> <li>Players are decision-makers that implement <code>PlayerA.decide</code> and <code>PlayerB.decide</code>.</li> <li>Models are reusable computational components (for example, neural models) that may back a player.</li> <li>A trainable model may store per-decision state (for example, per-layer outcomes) required for Player B.</li> </ul>"},{"location":"conventions/#shared-resource-sr","title":"Shared resource (SR)","text":"<ul> <li>SR (shared resource) is a two-party resource available to both players without communication.</li> <li>SR is not communication and MUST NOT increase <code>comms_size</code>.</li> <li>PRAssistedLayer is a specific type of SR.</li> </ul>"},{"location":"conventions/#determinism-seeding-and-reproducibility","title":"Determinism, seeding, and reproducibility","text":"<ul> <li>Any utility that samples randomness (data generation, evaluation) MUST accept a <code>seed</code> parameter or use a   reproducible RNG strategy.</li> <li>Runtime gameplay MAY be stochastic due to SR and channel noise.</li> <li>Evaluation runs SHOULD support deterministic replay via fixed seeds.</li> </ul>"},{"location":"conventions/#channel-noise","title":"Channel noise","text":"<ul> <li>Channel noise flips each bit of <code>comm</code> independently with probability <code>channel_noise</code>.</li> <li>Noise MUST be applied after Player A decides and before Player B decides.</li> </ul>"},{"location":"conventions/#contract-language","title":"Contract language","text":"<p>The following keywords are normative: - MUST / MUST NOT: required for compliance. - SHOULD / SHOULD NOT: recommended; deviations require justification. - MAY: optional behavior permitted by the spec.</p>"},{"location":"conventions/#interface-conventions","title":"Interface conventions","text":""},{"location":"conventions/#decide","title":"<code>decide(...)</code>","text":"<ul> <li><code>decide</code> methods MUST be pure with respect to their explicit inputs, except for documented internal state needed for   coordination (for example, per-layer outcomes).</li> <li>If <code>comms_size == 1</code>, <code>comm</code> MUST still be represented as an array or tensor of shape <code>(m,)</code> or <code>(B, 1)</code>.</li> </ul>"},{"location":"conventions/#dtypes","title":"Dtypes","text":"<ul> <li>NumPy arrays are acceptable for environment-level logic.</li> <li>TensorFlow tensors are acceptable for trainable models and training utilities.</li> <li>A function MUST document whether it expects NumPy arrays or TensorFlow tensors if ambiguous.</li> </ul>"},{"location":"conventions/#error-handling","title":"Error handling","text":"<ul> <li>Shape mismatches MUST raise <code>ValueError</code> (preferred) rather than silently reshaping.</li> <li>Double-use of a shared resource (SR) instance MUST raise an error (typically <code>ValueError</code> or <code>RuntimeError</code>).</li> </ul>"},{"location":"conventions/#module-naming-conventions","title":"Module naming conventions","text":""},{"location":"conventions/#teacher-and-trainable-re-export-modules","title":"Teacher and trainable re-export modules","text":"<ul> <li>Pyramid teacher primitives are exported via <code>Q_Sea_Battle.pyr_teacher_layers</code>.</li> <li>Pyramid trainable models are exported via <code>Q_Sea_Battle.pyr_trainable_models</code>.</li> <li>Linear teacher primitives are exported via <code>Q_Sea_Battle.lin_teacher_layers</code>.</li> <li>Linear trainable models are exported via <code>Q_Sea_Battle.lin_trainable_models</code>.</li> </ul>"},{"location":"conventions/#utilities-modules","title":"Utilities modules","text":"<ul> <li>Utility modules use the suffix <code>_utilities.py</code>.</li> <li>Modules SHOULD NOT use the suffix <code>_utils.py</code>.</li> </ul>"},{"location":"conventions/#document-status","title":"Document status","text":"<ul> <li>Chapters describing components are normative unless explicitly labeled informative.</li> <li><code>docs/algorithms.md</code> is informative and does not override contracts.</li> </ul>"},{"location":"conventions/#changelog","title":"Changelog","text":"<ul> <li>2026-01-16 - Rob Hendriks: Update SR terminology and module naming conventions.</li> </ul>"},{"location":"dru_utilities/","title":"dru_utilities","text":"<p>Role: DRU (Discretize / Regularize Unit) utilities for transforming communication message logits into differentiable probabilities during training and discrete bits during execution. Location: <code>Q_Sea_Battle.dru_utilities</code></p>"},{"location":"dru_utilities/#overview","title":"Overview","text":"<p>This module implements helper functions for the Discretize / Regularize Unit (DRU) used in DIAL-style training of communicating agents. It provides a differentiable mapping (<code>dru_train</code>) that adds Gaussian noise and applies a logistic nonlinearity during centralized training, and a discretizing mapping (<code>dru_execute</code>) that hard-thresholds logits to bits for decentralized execution. The module is intentionally free of trainable parameters and supports both NumPy arrays and TensorFlow tensors.</p>"},{"location":"dru_utilities/#public-api","title":"Public API","text":""},{"location":"dru_utilities/#functions","title":"Functions","text":""},{"location":"dru_utilities/#_is_tf_tensorx-any-bool","title":"<code>_is_tf_tensor(x: Any) -&gt; bool</code>","text":"<p>Signature: <code>_is_tf_tensor(x: Any) -&gt; bool</code> Purpose: Return <code>True</code> if <code>x</code> is a TensorFlow tensor. Arguments: - <code>x</code> (<code>Any</code>): Value to test. Returns: - <code>bool</code>: <code>True</code> if <code>tf.is_tensor(x)</code> is <code>True</code>, otherwise <code>False</code>. Errors: - Not specified. Example: <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.dru_utilities import _is_tf_tensor\n\nx = tf.constant([1.0, 2.0])\nassert _is_tf_tensor(x) is True\n</code></pre></p>"},{"location":"dru_utilities/#dru_trainmessage_logits-arraylike-sigma-float-20-clip_range-tuplefloat-float-none-100-100-arraylike","title":"<code>dru_train(message_logits: ArrayLike, sigma: float = 2.0, clip_range: Tuple[float, float] | None = (-10.0, 10.0)) -&gt; ArrayLike</code>","text":"<p>Signature: <code>dru_train(message_logits: ArrayLike, sigma: float = 2.0, clip_range: Tuple[float, float] | None = (-10.0, 10.0)) -&gt; ArrayLike</code> Purpose: Apply the differentiable DRU mapping used during centralized training: additive Gaussian noise on logits followed by a logistic/sigmoid transformation, optionally clipping the noisy logits for numerical stability. Arguments: - <code>message_logits</code> (<code>ArrayLike</code>): Logits for communication dimensions; may be a scalar, NumPy array, or TensorFlow tensor of shape <code>(..., m)</code>. - <code>sigma</code> (<code>float</code>, default <code>2.0</code>): Standard deviation of Gaussian noise added to logits; must be non-negative. - <code>clip_range</code> (<code>Tuple[float, float] | None</code>, default <code>(-10.0, 10.0)</code>): Optional <code>(min, max)</code> to clip noisy logits before applying the logistic; if <code>None</code>, no clipping is applied. Returns: - <code>ArrayLike</code>: Same type and shape as <code>message_logits</code>, with values in <code>(0, 1)</code>; TensorFlow outputs are differentiable with respect to <code>message_logits</code>. Errors: - <code>ValueError</code>: If <code>sigma &lt; 0.0</code>. Example: <pre><code>import numpy as np\nimport tensorflow as tf\nfrom Q_Sea_Battle.dru_utilities import dru_train\n\n# NumPy usage\nlogits_np = np.array([0.0, 2.0, -2.0], dtype=np.float32)\nprobs_np = dru_train(logits_np, sigma=0.0)  # deterministic sigmoid\nprint(probs_np)\n\n# TensorFlow usage (differentiable)\nlogits_tf = tf.constant([[0.0, 1.0, -1.0]], dtype=tf.float32)\nwith tf.GradientTape() as tape:\n    tape.watch(logits_tf)\n    probs_tf = dru_train(logits_tf, sigma=0.0)\ngrads = tape.gradient(probs_tf, logits_tf)\nprint(probs_tf, grads)\n</code></pre></p>"},{"location":"dru_utilities/#dru_executemessage_logits-arraylike-threshold-float-00-arraylike","title":"<code>dru_execute(message_logits: ArrayLike, threshold: float = 0.0) -&gt; ArrayLike</code>","text":"<p>Signature: <code>dru_execute(message_logits: ArrayLike, threshold: float = 0.0) -&gt; ArrayLike</code> Purpose: Apply the discretizing DRU mapping used during decentralized execution: element-wise hard thresholding of logits to bits. Arguments: - <code>message_logits</code> (<code>ArrayLike</code>): Logits for communication dimensions; may be a scalar, NumPy array, or TensorFlow tensor of shape <code>(..., m)</code>. - <code>threshold</code> (<code>float</code>, default <code>0.0</code>): Logit-space threshold used to produce discrete bits; <code>0.0</code> corresponds to probability threshold <code>0.5</code>. Returns: - <code>ArrayLike</code>: For NumPy inputs, a NumPy array of <code>int</code> with values in <code>{0, 1}</code> and the same shape as <code>message_logits</code>. For TensorFlow inputs, a <code>tf.Tensor</code> of <code>tf.float32</code> with values in <code>{0.0, 1.0}</code>. Errors: - Not specified. Example: <pre><code>import numpy as np\nimport tensorflow as tf\nfrom Q_Sea_Battle.dru_utilities import dru_execute\n\nlogits_np = np.array([-0.1, 0.0, 0.2], dtype=np.float32)\nbits_np = dru_execute(logits_np, threshold=0.0)\nprint(bits_np)  # [0 0 1]\n\nlogits_tf = tf.constant([-0.1, 0.0, 0.2], dtype=tf.float32)\nbits_tf = dru_execute(logits_tf, threshold=0.0)\nprint(bits_tf)  # tf.Tensor([0. 0. 1.], shape=(3,), dtype=float32)\n</code></pre></p>"},{"location":"dru_utilities/#constants","title":"Constants","text":"<ul> <li>None.</li> </ul>"},{"location":"dru_utilities/#types","title":"Types","text":"<ul> <li><code>ArrayLike = Union[float, np.ndarray, tf.Tensor]</code></li> </ul>"},{"location":"dru_utilities/#dependencies","title":"Dependencies","text":"<ul> <li><code>numpy</code> (<code>np</code>): Used for NumPy-based computation and sampling Gaussian noise in the NumPy path.</li> <li><code>tensorflow</code> (<code>tf</code>): Used for TensorFlow-based computation, noise sampling in the TensorFlow path, and differentiable sigmoid mapping.</li> <li><code>typing</code>: Uses <code>Any</code>, <code>Tuple</code>, and <code>Union</code> for type annotations.</li> <li><code>Q_Sea_Battle.logit_utilities.logit_to_prob</code>: Used to convert logits to probabilities in the NumPy path (and referenced in documentation for consistency).</li> </ul>"},{"location":"dru_utilities/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"dru_utilities/#deviations","title":"Deviations","text":"<ul> <li>The module docstring states that the logistic nonlinearity is implemented via <code>Q_Sea_Battle.logit_utilities.logit_to_prob</code> for consistency, but the TensorFlow path uses <code>tf.nn.sigmoid</code> directly (documented as equivalent to <code>logit_to_prob</code> when using logits).</li> </ul>"},{"location":"dru_utilities/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>Randomness/reproducibility: <code>dru_train</code> relies on global seeds for <code>np.random</code> and <code>tf.random</code> set elsewhere; tests should set seeds explicitly when deterministic behavior is required.</li> <li>Keep TensorFlow operations on-tensor in the TensorFlow path to preserve gradient flow through <code>message_logits</code>.</li> <li>Avoid adding trainable parameters to this module; it is intended to be a fixed transformation given inputs and noise settings.</li> </ul>"},{"location":"dru_utilities/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.logit_utilities.logit_to_prob</code> (used for stable/log-consistent probability computations in the NumPy path).</li> </ul>"},{"location":"dru_utilities/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial implementation of DRU utilities (<code>dru_train</code>, <code>dru_execute</code>) and helper <code>_is_tf_tensor</code>.</li> </ul>"},{"location":"game/","title":"Game","text":"<p>Role: Orchestrates a single QSeaBattle game between two players by coordinating a <code>GameEnv</code> and a <code>Players</code> factory.</p> <p>Location: <code>Q_Sea_Battle.game.Game</code></p>"},{"location":"game/#constructor","title":"Constructor","text":"Parameter Type Description <code>game_env</code> <code>GameEnv</code>, not specified, shape N/A Game environment instance. <code>players</code> <code>Players</code>, not specified, shape N/A Players factory providing Player A and B. <p>Preconditions</p> <ul> <li><code>game_env</code> is not <code>None</code>.</li> <li><code>players</code> is not <code>None</code>.</li> <li><code>game_env</code> provides methods: <code>reset()</code>, <code>provide()</code>, <code>apply_channel_noise(comm)</code>, <code>evaluate(shoot)</code>.</li> <li><code>players</code> provides methods: <code>reset()</code>, <code>players()</code> returning two player objects with method <code>decide(...)</code>.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.game_env</code> references the provided <code>game_env</code>.</li> <li><code>self.players</code> references the provided <code>players</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified (constructor contains no explicit validation or exception handling).</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.game import Game\nfrom Q_Sea_Battle.game_env import GameEnv\nfrom Q_Sea_Battle.players_base import Players\n\ngame_env = GameEnv(...)  # Not specified\nplayers = Players(...)   # Not specified\n\ngame = Game(game_env=game_env, players=players)\n</code></pre>"},{"location":"game/#public-methods","title":"Public Methods","text":""},{"location":"game/#play-tuplefloat-npndarray-npndarray-npndarray-int","title":"<code>play() -&gt; Tuple[float, np.ndarray, np.ndarray, np.ndarray, int]</code>","text":"<p>Play a single game round by resetting environment and players, obtaining player instances, providing <code>(field, gun)</code> from the environment, having Player A produce a communication, applying channel noise, having Player B decide whether to shoot, and evaluating the reward.</p> <p>Returns</p> <ul> <li><code>reward</code>: <code>float</code>, not specified, shape N/A.</li> <li><code>field</code>: <code>np.ndarray</code>, not specified, shape not specified (documented as flattened in docstring).</li> <li><code>gun</code>: <code>np.ndarray</code>, not specified, shape not specified (documented as flattened in docstring).</li> <li><code>comm</code>: <code>np.ndarray</code>, not specified, shape not specified (returned value is the noisy communication; documented as flattened in docstring).</li> <li><code>shoot</code>: <code>int</code>, constraints not specified, shape N/A (constructed as <code>int(shoot)</code> from Player B decision).</li> </ul> <p>Errors</p> <ul> <li>Not specified (method contains no explicit exception handling; may propagate exceptions raised by <code>GameEnv</code>, <code>Players</code>, or player instances).</li> </ul> <p>Example</p> <pre><code>reward, field, gun, comm, shoot = game.play()\n</code></pre>"},{"location":"game/#data-state","title":"Data &amp; State","text":"<ul> <li><code>game_env</code>: <code>GameEnv</code>, not specified, shape N/A; stored reference to the environment used for <code>reset()</code>, <code>provide()</code>, <code>apply_channel_noise(...)</code>, and <code>evaluate(...)</code>.</li> <li><code>players</code>: <code>Players</code>, not specified, shape N/A; stored reference to the players factory used for <code>reset()</code> and <code>players()</code>.</li> </ul>"},{"location":"game/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"game/#deviations","title":"Deviations","text":"<ul> <li>The <code>play()</code> docstring states it returns <code>(reward, field, gun, comm, shoot)</code>, where <code>comm</code> refers to the communication; the implementation returns <code>comm_noisy</code> (noisy communication) in the <code>comm</code> position.</li> </ul>"},{"location":"game/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li><code>play()</code> assumes <code>GameEnv.provide()</code> returns <code>(field, gun)</code> in a format compatible with <code>player_a.decide(field, supp=None)</code> and <code>player_b.decide(gun, comm_noisy, supp=None)</code>; the precise dtypes/shapes are not enforced in code.</li> <li><code>shoot</code> is cast to <code>int</code> on return; if <code>player_b.decide(...)</code> returns a non-scalar or non-castable object, <code>int(shoot)</code> will raise.</li> </ul>"},{"location":"game/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.game_env.GameEnv</code></li> <li><code>Q_Sea_Battle.players_base.Players</code></li> </ul>"},{"location":"game/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial implementation of single-game orchestration via <code>Game.play()</code>.</li> </ul>"},{"location":"game_env/","title":"GameEnv","text":"<p>Role: Core environment for generating game state (field, gun), providing player inputs, evaluating rewards, and applying communication channel noise. Location: <code>Q_Sea_Battle.game_env.GameEnv</code></p>"},{"location":"game_env/#derived-constraints","title":"Derived constraints","text":"<ul> <li>Let <code>field_size = n</code> from <code>self.game_layout.field_size</code>; then <code>n2 = n * n</code> and the flattened <code>field</code> and <code>gun</code> are shape <code>(n2,)</code>.</li> <li><code>field</code> values are integers in <code>{0,1}</code> and shape <code>(n, n)</code>.</li> <li><code>gun</code> is a one-hot integer array in <code>{0,1}</code> with exactly one <code>1</code> and shape <code>(n, n)</code>.</li> <li>Let <code>comms_size = m</code>; then <code>comm</code> vectors passed to <code>apply_channel_noise</code> are intended to have shape <code>(m,)</code> with values in <code>{0,1}</code> (length is not validated by the implementation).</li> </ul>"},{"location":"game_env/#constructor","title":"Constructor","text":"Parameter Type Description game_layout Optional[GameLayout], nullable, shape N/A Optional game configuration; if <code>None</code>, a default <code>GameLayout()</code> is constructed. <p>Preconditions</p> <ul> <li>None.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.game_layout</code> is set to the provided <code>GameLayout</code> or a new default instance.</li> <li><code>self.field</code> is <code>None</code>.</li> <li><code>self.gun</code> is <code>None</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.game_env import GameEnv\n\nenv = GameEnv()\nenv.reset()\nfield_flat, gun_flat = env.provide()\nr = env.evaluate(1)\n</code></pre>"},{"location":"game_env/#public-methods","title":"Public Methods","text":""},{"location":"game_env/#reset","title":"reset","text":"<p>Signature: <code>reset(self) -&gt; None</code></p> <p>Reset the environment state for a new game by creating a new random field and a new random one-hot gun position.</p> <p>Arguments</p> <ul> <li>None.</li> </ul> <p>Returns</p> <ul> <li><code>None</code>, shape N/A.</li> </ul> <p>Preconditions</p> <ul> <li><code>self.game_layout.field_size</code> is expected to be usable as an integer <code>n</code> such that arrays of shape <code>(n, n)</code> and length <code>n2 = n * n</code> can be created.</li> <li><code>self.game_layout.enemy_probability</code> is expected to be usable as a float probability for Bernoulli sampling.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.field</code>: <code>np.ndarray, dtype int, values {0,1}, shape (field_size, field_size)</code>.</li> <li><code>self.gun</code>: <code>np.ndarray, dtype int, values {0,1}, shape (field_size, field_size)</code>, with exactly one element equal to <code>1</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>env.reset()\n</code></pre>"},{"location":"game_env/#provide","title":"provide","text":"<p>Signature: <code>provide(self) -&gt; Tuple[np.ndarray, np.ndarray]</code></p> <p>Provide inputs to the players by returning the current <code>field</code> and <code>gun</code> arrays in flattened form (copies).</p> <p>Arguments</p> <ul> <li>None.</li> </ul> <p>Returns</p> <ul> <li><code>Tuple[np.ndarray, np.ndarray]</code>: <code>(field, gun)</code> where each is <code>np.ndarray, dtype int, values {0,1}, shape (n2,)</code> with <code>n2 = field_size * field_size</code>.</li> </ul> <p>Preconditions</p> <ul> <li><code>reset()</code> has been called successfully so that <code>self.field</code> and <code>self.gun</code> are not <code>None</code>.</li> </ul> <p>Postconditions</p> <ul> <li>Returns copies of internal arrays; mutating the returned arrays does not mutate <code>self.field</code> / <code>self.gun</code>.</li> </ul> <p>Errors</p> <ul> <li><code>RuntimeError</code>: If <code>self.field is None</code> or <code>self.gun is None</code> (environment not reset).</li> </ul> <p>Example</p> <pre><code>env.reset()\nfield_flat, gun_flat = env.provide()\n</code></pre>"},{"location":"game_env/#evaluate","title":"evaluate","text":"<p>Signature: <code>evaluate(self, shoot: int) -&gt; float</code></p> <p>Evaluate the reward for a shooting decision.</p> <p>Arguments</p> <ul> <li><code>shoot</code>: <code>int, values {0,1}, shape N/A</code>. (The implementation casts via <code>int(shoot)</code>; other values are not validated.)</li> </ul> <p>Returns</p> <ul> <li><code>float, values {0.0, 1.0}, shape N/A</code>: <code>1.0</code> if the decision matches the true cell value at the gun position, otherwise <code>0.0</code>.</li> </ul> <p>Preconditions</p> <ul> <li><code>reset()</code> has been called successfully so that <code>self.field</code> and <code>self.gun</code> are not <code>None</code>.</li> <li><code>self.gun</code> contains exactly one <code>1</code> so that exactly one field cell is selected.</li> </ul> <p>Postconditions</p> <ul> <li>No mutation of <code>self.field</code> or <code>self.gun</code> is performed.</li> </ul> <p>Errors</p> <ul> <li><code>RuntimeError</code>: If <code>self.field is None</code> or <code>self.gun is None</code> (environment not reset).</li> <li><code>RuntimeError</code>: If <code>self.gun</code> does not contain exactly one <code>1</code> (i.e., selected cell count is not 1).</li> </ul> <p>Example</p> <pre><code>env.reset()\nreward = env.evaluate(0)\n</code></pre>"},{"location":"game_env/#apply_channel_noise","title":"apply_channel_noise","text":"<p>Signature: <code>apply_channel_noise(self, comm: np.ndarray) -&gt; np.ndarray</code></p> <p>Apply independent bit-flip noise to a communication vector with per-bit flip probability <code>channel_noise</code>.</p> <p>Arguments</p> <ul> <li><code>comm</code>: <code>np.ndarray, dtype int convertible, intended values {0,1}, intended shape (comms_size,)</code> where <code>comms_size = m</code>. The implementation converts using <code>np.asarray(comm, dtype=int)</code> and does not validate values or length.</li> </ul> <p>Returns</p> <ul> <li><code>np.ndarray, dtype int, shape comm.shape</code>: Noisy communication vector.</li> <li>If <code>channel_noise &lt;= 0.0</code>, returns an unchanged copy of <code>comm</code> (after conversion to <code>dtype int</code>).</li> <li>If <code>channel_noise &gt;= 1.0</code>, returns <code>1 - comm</code> (bitwise flip for 0/1 semantics).</li> <li>Otherwise flips each entry independently with probability <code>channel_noise</code>.</li> </ul> <p>Preconditions</p> <ul> <li><code>self.game_layout.channel_noise</code> is expected to be usable as a float <code>c</code>.</li> </ul> <p>Postconditions</p> <ul> <li>Returns a new array (copy) for <code>c &lt;= 0.0</code> and for <code>0.0 &lt; c &lt; 1.0</code>.</li> <li>For <code>c &gt;= 1.0</code>, the expression <code>1 - comm</code> produces a new array.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>env.reset()\ncomm = np.array([0, 1, 1, 0], dtype=int)\nnoisy = env.apply_channel_noise(comm)\n</code></pre>"},{"location":"game_env/#data-state","title":"Data &amp; State","text":"<ul> <li><code>game_layout</code>: <code>GameLayout, non-null, shape N/A</code>. Configuration object used to read <code>field_size</code>, <code>enemy_probability</code>, and <code>channel_noise</code>.</li> <li><code>field</code>: <code>Optional[np.ndarray], nullable</code>. When set: <code>np.ndarray, dtype int, values {0,1}, shape (field_size, field_size)</code>.</li> <li><code>gun</code>: <code>Optional[np.ndarray], nullable</code>. When set: <code>np.ndarray, dtype int, values {0,1}, shape (field_size, field_size)</code>, intended to be one-hot with exactly one <code>1</code>.</li> </ul>"},{"location":"game_env/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"game_env/#deviations","title":"Deviations","text":"<ul> <li>No design notes were provided; no deviations can be assessed.</li> </ul>"},{"location":"game_env/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li><code>provide()</code> returns flattened copies; if new methods return views instead, explicitly document mutation and aliasing behavior.</li> <li><code>apply_channel_noise()</code> assumes 0/1 semantics but does not validate <code>comm</code> contents; if validation is added, document the resulting errors and constraints.</li> </ul>"},{"location":"game_env/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.game_layout.GameLayout</code> (configuration dependency; imported as <code>from .game_layout import GameLayout</code>).</li> </ul>"},{"location":"game_env/#changelog","title":"Changelog","text":"<ul> <li>Version: 0.1 (module docstring).</li> </ul>"},{"location":"game_layout/","title":"GameLayout","text":"<p>Role: Provide an immutable, validated configuration for a QSeaBattle game and its tournament logging schema. Location: <code>Q_Sea_Battle.game_layout.GameLayout</code></p>"},{"location":"game_layout/#derived-constraints","title":"Derived constraints","text":"<p>Definitions used throughout this specification: \\(n = \\texttt{field_size}\\), \\(n2 = n^2\\), \\(m = \\texttt{comms_size}\\). Constraints enforced at initialization: \\(n &gt; 0\\), \\(n2\\) is a power of two, \\(m &gt; 0\\), \\(m\\) divides \\(n2\\), \\(\\texttt{enemy_probability} \\in [0.0, 1.0]\\), \\(\\texttt{channel_noise} \\in [0.0, 1.0]\\), \\(\\texttt{number_of_games_in_tournament} &gt; 0\\), and \\(\\texttt{log_columns}\\) is a list of strings.</p>"},{"location":"game_layout/#constructor","title":"Constructor","text":"Parameter Type Description field_size int, constraints: \\(&gt; 0\\) and \\(n2=\\texttt{field_size}^2\\) is a power of two, shape: scalar Size \\(n\\) of one dimension of the square field; flattened field length is \\(n2\\). comms_size int, constraints: \\(&gt; 0\\) and divides \\(n2\\), shape: scalar Communication vector length \\(m\\); must satisfy \\(n2 \\bmod m = 0\\). enemy_probability float, constraints: \\(0.0 \\le p \\le 1.0\\), shape: scalar Probability that a cell in the field equals 1. channel_noise float, constraints: \\(0.0 \\le p \\le 1.0\\), shape: scalar Probability that a bit is flipped in the channel. number_of_games_in_tournament int, constraints: \\(&gt; 0\\), shape: scalar Number of games per tournament. log_columns List[str], constraints: all elements are str, shape: (k,) Column names for the tournament log. <p>Preconditions</p> <p>The instance is validated in <code>__post_init__</code>; callers must provide values meeting the constraints above or expect an exception.</p> <p>Postconditions</p> <p>The created instance is immutable (<code>@dataclass(frozen=True)</code>) and has validated field values.</p> <p>Errors</p> <ul> <li>Raises <code>TypeError</code> if <code>field_size</code>, <code>comms_size</code>, or <code>number_of_games_in_tournament</code> is not an <code>int</code>.</li> <li>Raises <code>TypeError</code> if <code>log_columns</code> is not a <code>list</code> of <code>str</code>.</li> <li>Raises <code>ValueError</code> if <code>field_size &lt;= 0</code>.</li> <li>Raises <code>ValueError</code> if \\(n2\\) is not a power of two.</li> <li>Raises <code>ValueError</code> if <code>comms_size &lt;= 0</code> or if <code>comms_size</code> does not divide \\(n2\\).</li> <li>Raises <code>ValueError</code> if <code>enemy_probability</code> is not in \\([0.0, 1.0]\\).</li> <li>Raises <code>ValueError</code> if <code>channel_noise</code> is not in \\([0.0, 1.0]\\).</li> <li>Raises <code>ValueError</code> if <code>number_of_games_in_tournament &lt;= 0</code>.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.game_layout import GameLayout\n\nlayout = GameLayout(field_size=4, comms_size=2, enemy_probability=0.25, channel_noise=0.0, number_of_games_in_tournament=100)\n</code></pre>"},{"location":"game_layout/#public-methods","title":"Public Methods","text":""},{"location":"game_layout/#from_dict","title":"from_dict","text":"<p>Create a <code>GameLayout</code> instance from a dictionary of parameters, ignoring unknown keys and using dataclass defaults for missing keys.</p> Parameters Type Description parameters Dict, constraints: keys may include any dataclass field names; unknown keys are ignored, shape: mapping Dictionary containing parameter overrides. Returns Type Description layout GameLayout, constraints: validated instance, shape: scalar A new validated <code>GameLayout</code> instance. <p>Errors</p> <p>May raise <code>TypeError</code> or <code>ValueError</code> as described for the constructor because the created instance is validated via <code>__post_init__</code>.</p> <p>Example</p> <pre><code>from Q_Sea_Battle.game_layout import GameLayout\n\nlayout = GameLayout.from_dict({\"field_size\": 4, \"comms_size\": 1, \"unknown\": 123})\n</code></pre>"},{"location":"game_layout/#to_dict","title":"to_dict","text":"<p>Return a dictionary representation containing all dataclass fields and their current values.</p> Parameters Type Description (no parameters) None, constraints: not applicable, shape: not applicable Not specified. Returns Type Description layout_dict Dict, constraints: keys are the dataclass field names, shape: mapping Dictionary with all layout parameters and their current values. <p>Example</p> <pre><code>from Q_Sea_Battle.game_layout import GameLayout\n\nd = GameLayout().to_dict()\n</code></pre>"},{"location":"game_layout/#data-state","title":"Data &amp; State","text":"<ul> <li><code>field_size</code>: int, constraints: \\(&gt; 0\\) and \\(n2=\\texttt{field_size}^2\\) is a power of two, shape: scalar.</li> <li><code>comms_size</code>: int, constraints: \\(&gt; 0\\) and divides \\(n2\\), shape: scalar.</li> <li><code>enemy_probability</code>: float, constraints: \\(0.0 \\le p \\le 1.0\\), shape: scalar.</li> <li><code>channel_noise</code>: float, constraints: \\(0.0 \\le p \\le 1.0\\), shape: scalar.</li> <li><code>number_of_games_in_tournament</code>: int, constraints: \\(&gt; 0\\), shape: scalar.</li> <li><code>log_columns</code>: List[str], constraints: all elements are str, shape: (k,).</li> <li>Immutability: instances are frozen; direct mutation of fields is not allowed by the dataclass.</li> </ul>"},{"location":"game_layout/#planned-design-spec","title":"Planned (design-spec)","text":"<p>Not specified.</p>"},{"location":"game_layout/#deviations","title":"Deviations","text":"<p>Not specified.</p>"},{"location":"game_layout/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>Validation is implemented in <code>__post_init__</code>; changes to constraints should be reflected there to preserve the immutability + validate-on-creation contract.</li> <li><code>_is_power_of_two</code> uses a bitwise test and assumes an <code>int</code> input; keep it consistent with the existing validation flow.</li> </ul>"},{"location":"game_layout/#related","title":"Related","text":"<ul> <li><code>dataclasses.dataclass</code> (frozen dataclass behavior)</li> <li><code>GameLayout._is_power_of_two</code> (internal validation helper)</li> </ul>"},{"location":"game_layout/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version (module docstring version).</li> </ul>"},{"location":"invariants/","title":"Invariants","text":""},{"location":"invariants/#purpose","title":"Purpose","text":"<p>This page lists global invariants: one-sentence truths that MUST hold across the QSeaBattle codebase.</p> <p>These are intended to be: - easy to audit during code review, - easy to test, - stable over time.</p>"},{"location":"invariants/#game-and-environment","title":"Game and environment","text":"<ul> <li><code>field_size</code> is the board side length and <code>n2 = field_size * field_size</code>.</li> <li>The field and gun vectors MUST have identical shape <code>(n2,)</code> (or <code>(B, n2)</code> when batched).</li> <li>The gun vector MUST be one-hot, for example <code>gun.sum() == 1</code> for a single sample.</li> <li><code>n2</code> MUST be both a perfect square and a power of two.</li> <li><code>comms_size = m</code> MUST divide <code>n2</code>.</li> <li>Channel noise MUST be applied only to <code>comm</code>, never to <code>field</code> or <code>gun</code>.</li> <li>Rewards MUST be deterministic given <code>(field, gun, shoot)</code>.</li> </ul>"},{"location":"invariants/#communication","title":"Communication","text":"<ul> <li>Communication bandwidth is exactly <code>comms_size</code> bits per game.</li> <li>If <code>comms_size == 1</code>, communication MUST still be represented as shape <code>(m,)</code> or <code>(B, 1)</code> (never a scalar).</li> </ul>"},{"location":"invariants/#assisted-and-shared-resources","title":"Assisted and shared resources","text":"<ul> <li>SR (shared resource) MUST NOT be treated as an extra communication channel.</li> <li>Each SR instance MAY be measured at most once by each party.</li> <li>For pyramid strategies, exactly one SR measurement is consumed per layer per party.</li> </ul>"},{"location":"invariants/#trainable-assisted-models-lin-and-pyr","title":"Trainable assisted models (Lin and Pyr)","text":"<ul> <li>Player A models MUST NOT access gun information.</li> <li>Player B models MUST NOT access the full field information.</li> <li>Any learned model MUST preserve the same input and output interfaces as the classical policy it approximates.</li> <li>Model state required for reconstruction (for example, per-layer outcomes) MUST be captured during Player A's decision   and used by Player B; it MUST NOT be recomputed using hidden information.</li> </ul>"},{"location":"invariants/#data-generation-and-training","title":"Data generation and training","text":"<ul> <li>Imitation-learning targets MUST be produced by a spec-compliant classical policy.</li> <li>Training utilities MUST NOT introduce new information paths (no shortcuts, no extra inputs).</li> <li>Weight transfer utilities MUST NOT change topology; only parameter values may be copied.</li> </ul>"},{"location":"invariants/#logging-and-evaluation","title":"Logging and evaluation","text":"<ul> <li>Each tournament log row MUST correspond to exactly one game.</li> <li>Evaluation utilities MUST NOT mutate player behavior beyond documented resets.</li> </ul>"},{"location":"invariants/#testability-expectation","title":"Testability expectation","text":"<ul> <li>Every invariant above SHOULD have at least one unit test or property test that fails if the invariant is violated.</li> </ul>"},{"location":"invariants/#changelog","title":"Changelog","text":"<ul> <li>2026-01-16 - Rob Hendriks: Update SR terminology and module naming conventions.</li> </ul>"},{"location":"lin_combine_layer_a/","title":"LinCombineLayerA","text":"<p>Role: Learnable TensorFlow Keras layer that maps measurement outcomes to communication logits.</p> <p>Location: <code>Q_Sea_Battle.lin_combine_layer_a.LinCombineLayerA</code></p>"},{"location":"lin_combine_layer_a/#constructor","title":"Constructor","text":"Parameter Type Description comms_size int, constraint: \\(m \\ge 1\\), shape: scalar Number of communication channels (\\(m\\)). Stored as <code>self.comms_size</code> after <code>int(...)</code>. hidden_units int or Sequence[int], constraint: each value \\(u \\ge 1\\), shape: scalar or \\((L,)\\) Hidden layer widths for an MLP; an int creates a single hidden layer, a sequence creates a stack of Dense-ReLU layers. Normalized to <code>tuple[int, ...]</code> and stored as <code>self.hidden_units</code>. name str or None, constraint: if None then defaults to <code>\"LinCombineLayerA\"</code>, shape: scalar Keras layer name passed to <code>tf.keras.layers.Layer.__init__</code>. **kwargs dict[str, Any], constraint: must be accepted by <code>tf.keras.layers.Layer</code>, shape: mapping Forwarded to the Keras base layer constructor. <p>Preconditions</p> <ul> <li><code>comms_size</code> must be convertible to <code>int</code>.</li> <li><code>hidden_units</code> must be an <code>int</code> or a <code>collections.abc.Sequence</code> of values convertible to <code>int</code>.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.comms_size: int</code> is set.</li> <li><code>self.hidden_units: tuple[int, ...]</code> is set.</li> <li><code>self._mlp: list[tf.keras.layers.Layer]</code> is created as a list of <code>tf.keras.layers.Dense(..., activation=\"relu\")</code> layers.</li> <li><code>self._out: tf.keras.layers.Dense</code> is created with <code>units=self.comms_size</code> and <code>activation=None</code>.</li> </ul> <p>Errors</p> <ul> <li>Raises <code>TypeError</code> if <code>hidden_units</code> is not an <code>int</code> and not a <code>Sequence</code>.</li> <li>Raises <code>ValueError</code> if any element of <code>hidden_units</code> cannot be converted to <code>int</code> (or if <code>comms_size</code> cannot be converted to <code>int</code>), as raised by <code>int(...)</code> conversion.</li> <li>Any additional errors may be raised by <code>tf.keras.layers.Layer.__init__</code> when invalid <code>name</code>/<code>kwargs</code> are provided.</li> </ul> <p>Example</p> <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.lin_combine_layer_a import LinCombineLayerA\n\nlayer = LinCombineLayerA(comms_size=8, hidden_units=(64, 64))\n\noutcomes = tf.random.uniform(shape=(32, 100), dtype=tf.float32)  # (B, n2)\nlogits = layer(outcomes, training=True)  # (B, m)\n</code></pre>"},{"location":"lin_combine_layer_a/#public-methods","title":"Public Methods","text":""},{"location":"lin_combine_layer_a/#call","title":"call","text":"<p>Signature: <code>call(self, outcomes: tf.Tensor, training: bool = False) -&gt; tf.Tensor</code></p> <p>Parameters</p> <ul> <li>outcomes: tf.Tensor, dtype: not specified (converted via <code>tf.convert_to_tensor</code>), shape \\((B, n2)\\) or \\((n2,)\\).</li> <li>training: bool, constraint: boolean, shape: scalar.</li> </ul> <p>Returns</p> <ul> <li>tf.Tensor, dtype: not specified, shape \\((B, m)\\) if input rank is 2, else shape \\((m,)\\) if input rank is 1.</li> </ul> <p>Behavior</p> <ul> <li>Converts <code>outcomes</code> to a tensor with <code>tf.convert_to_tensor</code>.</li> <li>If <code>outcomes</code> has rank 1 (shape \\((n2,)\\)), expands to shape \\((1, n2)\\), runs the MLP and output layer, then squeezes axis 0 to return shape \\((m,)\\).</li> <li>If <code>outcomes</code> has rank 2 (shape \\((B, n2)\\)), returns logits of shape \\((B, m)\\).</li> <li>Applies each hidden Dense layer in <code>self._mlp</code> with <code>activation=\"relu\"</code>, then applies <code>self._out</code> Dense with <code>activation=None</code> (logits).</li> </ul> <p>Errors</p> <ul> <li>May raise TensorFlow/Keras shape or rank errors if <code>outcomes</code> has rank other than 1 or 2, or if Dense layers cannot be applied to the provided shape/dtype.</li> </ul>"},{"location":"lin_combine_layer_a/#data-state","title":"Data &amp; State","text":"<ul> <li>self.comms_size: int, constraint: not validated in code beyond <code>int(...)</code>, shape: scalar.</li> <li>self.hidden_units: tuple[int, ...], constraint: elements are <code>int</code> after normalization, shape: \\((L,)\\).</li> <li>self._mlp: list[tf.keras.layers.Layer], constraint: list length \\(L = \\text{len}(\\text{self.hidden_units})\\), shape: list.</li> <li>self._out: tf.keras.layers.Dense, constraint: units \\(= m\\), shape: layer object.</li> </ul>"},{"location":"lin_combine_layer_a/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"lin_combine_layer_a/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"lin_combine_layer_a/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li><code>_normalize_hidden_units(hidden_units)</code> is a private helper that normalizes <code>hidden_units</code> to <code>tuple[int, ...]</code>; keep it consistent with any future serialization/config logic if added.</li> <li>The <code>call</code> method explicitly supports rank-1 input by expanding and later squeezing; changes to rank handling should preserve the documented input/output shape conventions \\((n2,) \\leftrightarrow (m,)\\) and \\((B, n2) \\leftrightarrow (B, m)\\).</li> </ul>"},{"location":"lin_combine_layer_a/#related","title":"Related","text":"<ul> <li>TensorFlow Keras base class: <code>tf.keras.layers.Layer</code></li> <li>Dense layers used internally: <code>tf.keras.layers.Dense</code></li> </ul>"},{"location":"lin_combine_layer_a/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial implementation of <code>LinCombineLayerA</code> with configurable hidden MLP and linear output logits.</li> </ul>"},{"location":"lin_combine_layer_b/","title":"LinCombineLayerB","text":"<p>Role: Learnable mapping (outcomes, comm) -&gt; shoot logit (a single logit output).</p> <p>Location: <code>Q_Sea_Battle.lin_combine_layer_b.LinCombineLayerB</code></p>"},{"location":"lin_combine_layer_b/#constructor","title":"Constructor","text":"Parameter Type Description comms_size int, constraint: \\(comms\\_size \\ge 0\\), scalar Number of communication channels \\(m\\); used for shape checks only (no checks are implemented in code). hidden_units int or collections.abc.Sequence[int], constraint: each element castable to int, scalar or shape (k,) Hidden layer widths; an int becomes a single hidden layer, a sequence becomes multiple layers; each Dense uses ReLU activation. name str or None, constraint: any string or None, scalar Layer name; if None, defaults to <code>\"LinCombineLayerB\"</code>. **kwargs dict[str, Unknown], constraint: forwarded to <code>tf.keras.layers.Layer</code>, shape N/A Additional keyword arguments passed to the base Keras Layer. <p>Preconditions - <code>hidden_units</code> must be an <code>int</code> or a sequence of values convertible to <code>int</code>. - <code>outcomes</code> and <code>comm</code> provided to <code>call(...)</code> must be convertible to <code>tf.Tensor</code>. - The last dimension sizes of <code>outcomes</code> and <code>comm</code> must be compatible for concatenation along axis <code>-1</code> after any optional rank-1 expansion.</p> <p>Postconditions - <code>self.comms_size</code> is set to <code>int(comms_size)</code>. - <code>self.hidden_units</code> is normalized to <code>tuple[int, ...]</code>. - An internal MLP is created: <code>len(self.hidden_units)</code> Dense layers with ReLU, followed by a final Dense layer with 1 unit and no activation.</p> <p>Errors - Raises <code>TypeError</code> / <code>ValueError</code> if <code>hidden_units</code> contains elements that cannot be converted to <code>int</code>. - TensorFlow shape errors may be raised at runtime during concatenation or Dense application if tensor shapes are incompatible.</p> <p>Example</p> <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.lin_combine_layer_b import LinCombineLayerB\n\nB, n2, m = 4, 25, 3\nlayer = LinCombineLayerB(comms_size=m, hidden_units=(64, 32))\n\noutcomes = tf.random.uniform((B, n2), dtype=tf.float32)\ncomm = tf.random.uniform((B, m), dtype=tf.float32)\n\nshoot_logit = layer(outcomes, comm, training=True)\nprint(shoot_logit.shape)  # (4, 1)\n</code></pre>"},{"location":"lin_combine_layer_b/#public-methods","title":"Public Methods","text":""},{"location":"lin_combine_layer_b/#call","title":"call","text":"<p><code>call(outcomes: tf.Tensor, comm: tf.Tensor, training: bool = False) -&gt; tf.Tensor</code></p> <p>Parameters - outcomes: tf.Tensor, dtype Not specified (converted via <code>tf.convert_to_tensor</code>), shape (B, n2) or (n2,) where \\(n2\\) is the outcomes feature size. - comm: tf.Tensor, dtype Not specified (converted via <code>tf.convert_to_tensor</code>), shape (B, m) or (m,) where \\(m\\) is the communication feature size. - training: bool, constraint: True or False, scalar; forwarded to Dense layers.</p> <p>Returns - tf.Tensor, dtype Not specified, shape (B, 1) if <code>outcomes</code> rank is 2; shape (1,) if <code>outcomes</code> rank is 1 (squeezed along axis 0).</p> <p>Behavior - Converts <code>outcomes</code> and <code>comm</code> to tensors. - If <code>outcomes</code> has rank 1, expands to shape (1, n2) and remembers to squeeze the output back to rank 1. - If <code>comm</code> has rank 1, expands to shape (1, m). - Concatenates <code>[outcomes, comm]</code> along the last axis, applies the hidden Dense layers (ReLU), then applies the final Dense(1) to produce a logit.</p> <p>Errors - TensorFlow runtime errors may occur if ranks are not 1 or 2, or if batch dimensions are incompatible for concatenation, or if Dense layers receive incompatible input shapes.</p>"},{"location":"lin_combine_layer_b/#data-state","title":"Data &amp; State","text":"<ul> <li>comms_size: int, constraint: \\(comms\\_size \\ge 0\\), scalar; stored for shape-check intent (no explicit checks are implemented).</li> <li>hidden_units: tuple[int, ...], constraint: each element is an int, shape (k,); normalized from the constructor input.</li> <li>_mlp: list[tf.keras.layers.Layer], constraint: Dense layers, shape (k,); the hidden layers (each <code>tf.keras.layers.Dense(u, activation=\"relu\")</code>).</li> <li>_out: tf.keras.layers.Dense, constraint: units=1 and activation=None, scalar layer instance.</li> </ul>"},{"location":"lin_combine_layer_b/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"lin_combine_layer_b/#deviations","title":"Deviations","text":"<ul> <li>The module docstring states <code>comms_size</code> is \"used for shape checks only\", but the implementation stores <code>self.comms_size</code> without performing any shape validation against <code>comm</code>.</li> </ul>"},{"location":"lin_combine_layer_b/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>Keep tensor rank handling consistent: currently only rank-1 inputs are expanded to a batch of 1; other ranks are not explicitly handled.</li> <li>If adding shape checks involving <code>comms_size</code>, ensure they work for both batched and unbatched inputs and do not break graph execution.</li> </ul>"},{"location":"lin_combine_layer_b/#related","title":"Related","text":"<ul> <li><code>_normalize_hidden_units(hidden_units: int | Sequence[int]) -&gt; tuple[int, ...]</code> (module-private helper; not part of the public API).</li> </ul>"},{"location":"lin_combine_layer_b/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial implementation of <code>LinCombineLayerB</code> as a small MLP producing a single shoot logit from concatenated <code>(outcomes, comm)</code> inputs.</li> </ul>"},{"location":"lin_measurement_layer_a/","title":"LinMeasurementLayerA","text":"<p>Role: Learnable mapping from a flattened field vector to per-cell measurement probabilities in \\([0, 1]\\) via an MLP ending in a sigmoid layer. Location: <code>Q_Sea_Battle.lin_measurement_layer_a.LinMeasurementLayerA</code></p>"},{"location":"lin_measurement_layer_a/#constructor","title":"Constructor","text":"Parameter Type Description n2 int, constraint \\(n2 &gt; 0\\) Field vector length (number of cells). Used as the output dimension of the final Dense layer and validated against the input last dimension in <code>call</code>. hidden_units Sequence[int], elements constraint each \\(u\\) castable to int Hidden layer widths for the MLP; each element produces one <code>tf.keras.layers.Dense(u, activation=\"relu\")</code> in <code>build</code>. Default: <code>(64,)</code>. name Optional[str], constraint either <code>None</code> or any string Layer name passed to <code>tf.keras.layers.Layer</code>. Default: <code>\"LinMeasurementLayerA\"</code>. **kwargs dict[str, Any], not specified Forwarded to <code>tf.keras.layers.Layer</code> superclass constructor. <p>Preconditions: <code>n2 &gt; 0</code>. Postconditions: <code>self.n2</code> is set to <code>int(n2)</code>; <code>self.hidden_units</code> is set to <code>tuple(int(u) for u in hidden_units)</code>; <code>_mlp</code> is initialized empty and <code>_built_mlp</code> is <code>False</code>. Errors: Raises <code>ValueError</code> if <code>n2 &lt;= 0</code>.  </p> <p>Example</p> <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.lin_measurement_layer_a import LinMeasurementLayerA\n\nn2 = 100\nlayer = LinMeasurementLayerA(n2=n2, hidden_units=(64, 64))\n\nx = tf.zeros((8, n2), dtype=tf.float32)\ny = layer(x, training=True)\nassert y.shape == x.shape\n</code></pre>"},{"location":"lin_measurement_layer_a/#public-methods","title":"Public Methods","text":""},{"location":"lin_measurement_layer_a/#build","title":"build","text":"<ul> <li>Signature: <code>build(self, input_shape) -&gt; None</code></li> <li>Parameters:</li> <li><code>input_shape</code>: Unknown, not specified; passed through to <code>tf.keras.layers.Layer.build</code>.</li> <li>Returns: None.</li> <li>Behavior: Lazily constructs the MLP exactly once; for each <code>u</code> in <code>self.hidden_units</code>, appends a <code>tf.keras.layers.Dense(u, activation=\"relu\")</code>, then appends a final <code>tf.keras.layers.Dense(self.n2, activation=\"sigmoid\")</code>; sets <code>_built_mlp = True</code> and calls <code>super().build(input_shape)</code>.</li> <li>Errors: Not specified.</li> </ul>"},{"location":"lin_measurement_layer_a/#call","title":"call","text":"<ul> <li>Signature: <code>call(self, fields, training: bool = False)</code></li> <li>Parameters:</li> <li><code>fields</code>: tf.Tensor-like, dtype any numeric; accepted shapes <code>(B, n2)</code> or <code>(n2,)</code>; converted via <code>tf.convert_to_tensor(fields)</code> and cast to <code>tf.float32</code> if not floating.</li> <li><code>training</code>: bool, constraint any boolean; passed as <code>training=training</code> to each Dense layer call.</li> <li>Returns: <code>tf.Tensor</code>, dtype float32, shape <code>(B, n2)</code> if input rank is 2, else shape <code>(n2,)</code> if input rank is 1; values are in \\([0, 1]\\) due to sigmoid output activation.</li> <li>Errors: Raises <code>ValueError</code> if input rank is not 1 or 2; raises <code>ValueError</code> if the last dimension is known and is not equal to <code>self.n2</code>.</li> </ul>"},{"location":"lin_measurement_layer_a/#data-state","title":"Data &amp; State","text":"<ul> <li><code>n2</code>: int, constraint \\(n2 &gt; 0\\); field vector length and final output width.</li> <li><code>hidden_units</code>: tuple[int, ...], constraint each element is an <code>int</code> derived from <code>hidden_units</code>; defines hidden Dense layers.</li> <li><code>_mlp</code>: list[tf.keras.layers.Layer], initial shape <code>len(_mlp) == 0</code> before <code>build</code>; after <code>build</code>, contains <code>len(hidden_units) + 1</code> Dense layers in order.</li> <li><code>_built_mlp</code>: bool; <code>False</code> before <code>build</code>, <code>True</code> after the first successful <code>build</code>.</li> </ul>"},{"location":"lin_measurement_layer_a/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"lin_measurement_layer_a/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"lin_measurement_layer_a/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li><code>build</code> is intentionally idempotent via <code>_built_mlp</code>; modifying layer construction should preserve that behavior to avoid duplicate sublayers on repeated builds.</li> <li><code>call</code> supports rank-1 inputs by temporarily expanding to rank-2 and then squeezing back; maintain the squeeze/unsqueeze contract if changing shape handling.</li> </ul>"},{"location":"lin_measurement_layer_a/#related","title":"Related","text":"<ul> <li><code>tf.keras.layers.Layer</code></li> <li><code>tf.keras.layers.Dense</code></li> </ul>"},{"location":"lin_measurement_layer_a/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial implementation of a learnable measurement layer mapping field vectors to per-cell probabilities via an MLP with sigmoid output.</li> </ul>"},{"location":"lin_measurement_layer_b/","title":"LinMeasurementLayerB","text":"<p>Role: Learnable mapping from a flattened gun vector to per-cell measurement probabilities in \\([0, 1]\\).</p> <p>Location: <code>Q_Sea_Battle.lin_measurement_layer_b.LinMeasurementLayerB</code></p>"},{"location":"lin_measurement_layer_b/#derived-constraints","title":"Derived constraints","text":"<ul> <li>Let <code>n2</code> be the flattened field size (number of cells); <code>n2</code> must be a positive <code>int</code>.</li> <li><code>call()</code> accepts rank-1 or rank-2 inputs only; the last dimension must equal <code>n2</code> when statically known.</li> </ul>"},{"location":"lin_measurement_layer_b/#constructor","title":"Constructor","text":"Parameter Type Description n2 int, constraint: <code>n2 &gt; 0</code>, shape: scalar Number of cells in the flattened representation; determines the output width of the final dense layer. hidden_units Sequence[int], constraint: each element convertible to <code>int</code>, shape: <code>(L,)</code> Sizes of intermediate dense layers; each layer uses ReLU activation. Default: <code>(64,)</code>. name Optional[str], constraint: any valid Keras layer name or <code>None</code>, shape: scalar Layer name passed to <code>tf.keras.layers.Layer</code>. Default: <code>\"LinMeasurementLayerB\"</code>. **kwargs dict[str, Unknown], constraint: passed through to <code>tf.keras.layers.Layer</code>, shape: mapping Additional Keras layer keyword arguments. <p>Preconditions: <code>n2 &gt; 0</code>.</p> <p>Postconditions: <code>self.n2 == int(n2)</code>; <code>self.hidden_units</code> is a <code>tuple[int, ...]</code>; internal MLP layer list is initialized empty and is constructed on first <code>build()</code>.</p> <p>Errors: Raises <code>ValueError</code> if <code>n2 &lt;= 0</code>.</p> <p>Example</p> <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.lin_measurement_layer_b import LinMeasurementLayerB\n\nn2 = 100\nlayer = LinMeasurementLayerB(n2=n2, hidden_units=(64, 32))\n\nguns_batch = tf.random.uniform(shape=(8, n2), dtype=tf.float32)\nprobs_batch = layer(guns_batch, training=True)  # shape (8, n2)\n\nguns_single = tf.random.uniform(shape=(n2,), dtype=tf.float32)\nprobs_single = layer(guns_single)  # shape (n2,)\n</code></pre>"},{"location":"lin_measurement_layer_b/#public-methods","title":"Public Methods","text":""},{"location":"lin_measurement_layer_b/#buildinput_shape-none","title":"build(input_shape) -&gt; None","text":"<p>Create weights based on input shape.</p> <ul> <li>Parameter <code>input_shape</code>: Unknown, constraint: Keras-compatible input shape descriptor, shape: Not specified.</li> <li>Returns: <code>None</code>, constraint: not applicable, shape: not applicable.</li> </ul> <p>Preconditions: None specified.</p> <p>Postconditions: If not already built, appends <code>len(hidden_units)</code> hidden <code>tf.keras.layers.Dense</code> layers with ReLU activation and one output <code>tf.keras.layers.Dense</code> layer with <code>units == n2</code> and sigmoid activation; sets internal built flag; calls <code>super().build(input_shape)</code>.</p> <p>Errors: Not specified.</p>"},{"location":"lin_measurement_layer_b/#callguns-training-bool-false","title":"call(guns, training: bool = False)","text":"<p>Forward pass.</p> <ul> <li>Parameter <code>guns</code>: <code>tf.Tensor</code>-convertible, dtype: any (non-floating will be cast to <code>tf.float32</code>), shape: <code>(B, n2)</code> or <code>(n2,)</code>.</li> <li>Parameter <code>training</code>: <code>bool</code>, constraint: any boolean, shape: scalar.</li> <li>Returns: <code>tf.Tensor</code>, dtype: floating (cast to <code>tf.float32</code> if input is non-floating), constraint: elementwise in \\([0, 1]\\), shape: same as <code>guns</code> (returns <code>(B, n2)</code> for batched input and <code>(n2,)</code> for rank-1 input).</li> </ul> <p>Preconditions: Input rank must be 1 or 2; last dimension must equal <code>n2</code> when statically known.</p> <p>Postconditions: Applies the internal MLP (Dense/ReLU layers followed by Dense/sigmoid) to produce probabilities; preserves original rank by temporarily expanding rank-1 inputs and squeezing the output back.</p> <p>Errors: Raises <code>ValueError</code> if input rank is not 1 or 2; raises <code>ValueError</code> if the statically known last dimension is not <code>n2</code>.</p>"},{"location":"lin_measurement_layer_b/#data-state","title":"Data &amp; State","text":"<ul> <li><code>n2</code>: <code>int</code>, constraint: <code>n2 &gt; 0</code>, shape: scalar; output width and expected input last dimension.</li> <li><code>hidden_units</code>: <code>tuple[int, ...]</code>, constraint: elements are <code>int</code>, shape: <code>(L,)</code>; hidden layer widths.</li> <li><code>_mlp</code>: <code>list[tf.keras.layers.Layer]</code>, constraint: contains Keras layers, shape: <code>(L+1,)</code> after build; internal sequence of <code>Dense</code> layers.</li> <li><code>_built_mlp</code>: <code>bool</code>, constraint: boolean, shape: scalar; indicates whether <code>_mlp</code> has been constructed.</li> </ul>"},{"location":"lin_measurement_layer_b/#planned-design-spec","title":"Planned (design-spec)","text":"<p>Not specified.</p>"},{"location":"lin_measurement_layer_b/#deviations","title":"Deviations","text":"<p>Not specified.</p>"},{"location":"lin_measurement_layer_b/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li><code>build()</code> is idempotent via <code>_built_mlp</code>; if modifying layer construction, keep repeated calls safe.</li> <li><code>call()</code> enforces rank 1 or 2 and validates the last dimension only when statically known (<code>x.shape[-1] is not None</code>).</li> </ul>"},{"location":"lin_measurement_layer_b/#related","title":"Related","text":"<ul> <li><code>tf.keras.layers.Layer</code></li> <li><code>tf.keras.layers.Dense</code></li> </ul>"},{"location":"lin_measurement_layer_b/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial implementation as a learnable sigmoid MLP mapping from gun vectors to per-cell probabilities.</li> </ul>"},{"location":"lin_teacher_layers/","title":"Q_Sea_Battle.lin_teacher_layers","text":"<p>Role: Re-export hub for linear (Lin) teacher layer primitives used to build reference (teacher) logic and trainable assisted models.</p> <p>Location: <code>Q_Sea_Battle.lin_teacher_layers</code></p>"},{"location":"lin_teacher_layers/#overview","title":"Overview","text":"<p>This module provides a single import surface for the linear layer primitives used in the project\u2019s \u201cteacher\u201d (reference) pipeline and dataset specifications. It defines no layers itself; instead, it re-exports four <code>tf.keras.layers.Layer</code> implementations from sibling modules and declares them in <code>__all__</code>.</p> <p>Terminology used by the module documentation: SR (shared resource) refers to any pre-shared auxiliary resource available to both players without communication; the term \u201cshared randomness\u201d is explicitly not used in this project.</p>"},{"location":"lin_teacher_layers/#public-api","title":"Public API","text":""},{"location":"lin_teacher_layers/#functions","title":"Functions","text":"<p>Not specified.</p>"},{"location":"lin_teacher_layers/#constants","title":"Constants","text":"<ul> <li><code>__all__</code>: <code>list[str]</code> (export list) containing: <code>\"LinMeasurementLayerA\"</code>, <code>\"LinMeasurementLayerB\"</code>, <code>\"LinCombineLayerA\"</code>, <code>\"LinCombineLayerB\"</code>.</li> </ul>"},{"location":"lin_teacher_layers/#types","title":"Types","text":"<p>Not specified.</p>"},{"location":"lin_teacher_layers/#dependencies","title":"Dependencies","text":"<ul> <li>Imports (relative): <code>.lin_measurement_layer_a.LinMeasurementLayerA</code>, <code>.lin_measurement_layer_b.LinMeasurementLayerB</code>, <code>.lin_combine_layer_a.LinCombineLayerA</code>, <code>.lin_combine_layer_b.LinCombineLayerB</code></li> <li>External dependencies: Not specified in this module (the docstring mentions <code>tf.keras.layers.Layer</code> as the implementation base class for the exported primitives, but TensorFlow is not imported here).</li> </ul>"},{"location":"lin_teacher_layers/#planned-design-spec","title":"Planned (design-spec)","text":"<p>Not specified.</p>"},{"location":"lin_teacher_layers/#deviations","title":"Deviations","text":"<p>Not specified.</p>"},{"location":"lin_teacher_layers/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>This module is intended to remain a thin re-export layer; add new exports by importing them and updating <code>__all__</code> in the same order they should appear for consumers.</li> <li>Avoid adding side effects here; keep imports and export-list maintenance only.</li> </ul>"},{"location":"lin_teacher_layers/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.lin_measurement_layer_a</code> (source of <code>LinMeasurementLayerA</code>)</li> <li><code>Q_Sea_Battle.lin_measurement_layer_b</code> (source of <code>LinMeasurementLayerB</code>)</li> <li><code>Q_Sea_Battle.lin_combine_layer_a</code> (source of <code>LinCombineLayerA</code>)</li> <li><code>Q_Sea_Battle.lin_combine_layer_b</code> (source of <code>LinCombineLayerB</code>)</li> <li><code>pr_assisted_layer.py</code> (mentioned in docstring as defining <code>PRAssistedLayer</code>; not part of this module\u2019s code)</li> </ul>"},{"location":"lin_teacher_layers/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial module providing re-exports for linear teacher layer primitives (per module docstring).</li> </ul>"},{"location":"lin_trainable_assisted_imitation_utilities/","title":"lin_trainable_assisted_imitation_utilities","text":"<p>Role: Generate reproducible synthetic supervised imitation datasets (parity-prototype targets), convert them to <code>tf.data.Dataset</code>, and provide minimal Keras weight-transfer + single-layer training helpers for linear trainable assisted layers in QSeaBattle. Location: <code>Q_Sea_Battle.lin_trainable_assisted_imitation_utilities</code></p>"},{"location":"lin_trainable_assisted_imitation_utilities/#overview","title":"Overview","text":"<p>This module provides utilities to imitation-train linear assisted layers by generating synthetic supervised datasets with parity-based targets, converting datasets into TensorFlow input pipelines, and transferring trained weights into other layer/model instances. Generated datasets are NumPy array dictionaries with leading dimension <code>num_samples</code>; arrays are float32 with values in <code>{0.0, 1.0}</code> for TensorFlow friendliness. All generators are reproducible via an explicit RNG seed.</p> <p>Terminology used by the module: SR (shared resource) is a pre-shared auxiliary resource available without communication; <code>PRAssistedLayer</code> is referenced as a specific SR type (not defined in this module).</p>"},{"location":"lin_trainable_assisted_imitation_utilities/#public-api","title":"Public API","text":""},{"location":"lin_trainable_assisted_imitation_utilities/#functions","title":"Functions","text":""},{"location":"lin_trainable_assisted_imitation_utilities/#_rngseed-optionalint-nprandomgenerator","title":"<code>_rng(seed: Optional[int]) -&gt; np.random.Generator</code>","text":"<p>Purpose: Create a reproducible NumPy random generator from an optional seed.</p> <p>Arguments: - <code>seed</code>: Optional RNG seed.</p> <p>Returns: - A <code>np.random.Generator</code> instance.</p> <p>Errors: - Not specified.</p> <p>Example: <pre><code>from Q_Sea_Battle.lin_trainable_assisted_imitation_utilities import _rng\n\nr = _rng(123)\nx = r.integers(0, 10, size=(3,))\n</code></pre></p>"},{"location":"lin_trainable_assisted_imitation_utilities/#_n2_from_layoutlayout-any-int","title":"<code>_n2_from_layout(layout: Any) -&gt; int</code>","text":"<p>Purpose: Compute <code>n\u00b2</code> (field size squared) from a layout-like object or raw integer.</p> <p>Arguments: - <code>layout</code>: Either an object with attribute <code>field_size</code>, or a raw integer interpreted as <code>field_size</code>.</p> <p>Returns: - <code>n2</code>: An integer equal to <code>field_size * field_size</code>.</p> <p>Errors: - Not specified.</p> <p>Example: <pre><code>from Q_Sea_Battle.lin_trainable_assisted_imitation_utilities import _n2_from_layout\n\nn2 = _n2_from_layout(5)  # 25\n</code></pre></p>"},{"location":"lin_trainable_assisted_imitation_utilities/#_m_from_layoutlayout-any-int","title":"<code>_m_from_layout(layout: Any) -&gt; int</code>","text":"<p>Purpose: Extract <code>m</code> (communication bits size) from a layout-like object, defaulting to <code>1</code> if unspecified.</p> <p>Arguments: - <code>layout</code>: Layout-like object; if it has <code>comms_size</code>, that value is used.</p> <p>Returns: - <code>m</code>: Integer comms size.</p> <p>Errors: - Not specified.</p> <p>Example: <pre><code>from types import SimpleNamespace\nfrom Q_Sea_Battle.lin_trainable_assisted_imitation_utilities import _m_from_layout\n\nlayout = SimpleNamespace(comms_size=3)\nm = _m_from_layout(layout)  # 3\n</code></pre></p>"},{"location":"lin_trainable_assisted_imitation_utilities/#_as_float01x-npndarray-npndarray","title":"<code>_as_float01(x: np.ndarray) -&gt; np.ndarray</code>","text":"<p>Purpose: Ensure an array is <code>float32</code> (intended to represent values in <code>{0.0, 1.0}</code>).</p> <p>Arguments: - <code>x</code>: NumPy array.</p> <p>Returns: - A NumPy array cast to <code>np.float32</code> (or returned unchanged if already <code>float32</code>).</p> <p>Errors: - Not specified.</p> <p>Example: <pre><code>import numpy as np\nfrom Q_Sea_Battle.lin_trainable_assisted_imitation_utilities import _as_float01\n\nx = np.array([0, 1, 1], dtype=np.int64)\ny = _as_float01(x)  # float32\n</code></pre></p>"},{"location":"lin_trainable_assisted_imitation_utilities/#_parity_bitsx01-npndarray-npndarray","title":"<code>_parity_bits(x01: np.ndarray) -&gt; np.ndarray</code>","text":"<p>Signature: <code>_parity_bits(x01: np.ndarray) -&gt; np.ndarray</code></p> <p>Purpose: Compute parity (XOR reduction) over the last dimension.</p> <p>Arguments: - <code>x01</code>: Array with last dimension <code>N</code>, values intended in <code>{0, 1}</code> (int/bool/float accepted; floats are thresholded at <code>&gt; 0.5</code>).</p> <p>Returns: - <code>parity</code>: Array with shape <code>x01.shape[:-1]</code>, dtype <code>int64</code>, values in <code>{0, 1}</code>.</p> <p>Errors: - Not specified.</p> <p>Example: <pre><code>import numpy as np\nfrom Q_Sea_Battle.lin_trainable_assisted_imitation_utilities import _parity_bits\n\nx = np.array([[0, 1, 1], [1, 1, 1]], dtype=np.float32)\np = _parity_bits(x)  # array([0, 1])\n</code></pre></p>"},{"location":"lin_trainable_assisted_imitation_utilities/#_layer_is_builtlayer-tfkeraslayerslayer-bool","title":"<code>_layer_is_built(layer: tf.keras.layers.Layer) -&gt; bool</code>","text":"<p>Purpose: Check whether a Keras layer is built and has weights.</p> <p>Arguments: - <code>layer</code>: A <code>tf.keras.layers.Layer</code>.</p> <p>Returns: - <code>True</code> if <code>layer.built</code> is truthy and <code>layer.weights</code> is not <code>None</code>, else <code>False</code>.</p> <p>Errors: - Not specified.</p> <p>Example: <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.lin_trainable_assisted_imitation_utilities import _layer_is_built\n\nlayer = tf.keras.layers.Dense(4)\nbuilt = _layer_is_built(layer)  # likely False until first call/build\n</code></pre></p>"},{"location":"lin_trainable_assisted_imitation_utilities/#generate_measurement_dataset_alayout-any-num_samples-int-p_one-float-05-seed-optionalint-none-arraydict","title":"<code>generate_measurement_dataset_a(layout: Any, num_samples: int, p_one: float = 0.5, seed: Optional[int] = None) -&gt; ArrayDict</code>","text":"<p>Purpose: Generate supervised pairs for <code>LinMeasurementLayerA</code> where the target equals the field (<code>meas_target == field</code>).</p> <p>Arguments: - <code>layout</code>: Layout-like object or int; used to determine <code>n2 = field_size\u00b2</code>. - <code>num_samples</code>: Number of samples to generate. - <code>p_one</code>: Probability of a <code>1</code> in each field bit (Bernoulli/binomial). - <code>seed</code>: Optional RNG seed for reproducibility.</p> <p>Returns: - A dict with keys:   - <code>\"field\"</code>: shape <code>(num_samples, n2)</code>, float32 in <code>{0.0, 1.0}</code>   - <code>\"meas_target\"</code>: shape <code>(num_samples, n2)</code>, identical copy of <code>\"field\"</code></p> <p>Errors: - Not specified.</p> <p>Example: <pre><code>from Q_Sea_Battle.lin_trainable_assisted_imitation_utilities import generate_measurement_dataset_a\n\nds = generate_measurement_dataset_a(layout=5, num_samples=128, p_one=0.3, seed=1)\nfield = ds[\"field\"]\ntarget = ds[\"meas_target\"]\n</code></pre></p>"},{"location":"lin_trainable_assisted_imitation_utilities/#generate_measurement_dataset_blayout-any-num_samples-int-seed-optionalint-none-arraydict","title":"<code>generate_measurement_dataset_b(layout: Any, num_samples: int, seed: Optional[int] = None) -&gt; ArrayDict</code>","text":"<p>Purpose: Generate supervised pairs for <code>LinMeasurementLayerB</code> where the input gun vector is one-hot and the target equals the gun (<code>meas_target == gun</code>).</p> <p>Arguments: - <code>layout</code>: Layout-like object or int; used to determine <code>n2 = field_size\u00b2</code>. - <code>num_samples</code>: Number of samples to generate. - <code>seed</code>: Optional RNG seed for reproducibility.</p> <p>Returns: - A dict with keys:   - <code>\"gun\"</code>: shape <code>(num_samples, n2)</code>, one-hot float32 vectors   - <code>\"meas_target\"</code>: shape <code>(num_samples, n2)</code>, identical copy of <code>\"gun\"</code></p> <p>Errors: - Not specified.</p> <p>Example: <pre><code>from Q_Sea_Battle.lin_trainable_assisted_imitation_utilities import generate_measurement_dataset_b\n\nds = generate_measurement_dataset_b(layout=5, num_samples=64, seed=42)\ngun = ds[\"gun\"]\ntarget = ds[\"meas_target\"]\n</code></pre></p>"},{"location":"lin_trainable_assisted_imitation_utilities/#generate_combine_dataset_alayout-any-num_samples-int-seed-optionalint-none-arraydict","title":"<code>generate_combine_dataset_a(layout: Any, num_samples: int, seed: Optional[int] = None) -&gt; ArrayDict</code>","text":"<p>Purpose: Generate supervised pairs for <code>LinCombineLayerA</code> where <code>comm_target</code> is the parity of <code>outcomes_a</code>, replicated to <code>m</code> bits.</p> <p>Arguments: - <code>layout</code>: Layout-like object or int; used to determine <code>n2 = field_size\u00b2</code> and <code>m = comms_size</code> (default <code>1</code>). - <code>num_samples</code>: Number of samples to generate. - <code>seed</code>: Optional RNG seed for reproducibility.</p> <p>Returns: - A dict with keys:   - <code>\"outcomes_a\"</code>: shape <code>(num_samples, n2)</code>, random float32 in <code>{0.0, 1.0}</code>   - <code>\"comm_target\"</code>: shape <code>(num_samples, m)</code>, float32 where each row is <code>parity(outcomes_a[row])</code> replicated <code>m</code> times</p> <p>Errors: - Not specified.</p> <p>Example: <pre><code>from types import SimpleNamespace\nfrom Q_Sea_Battle.lin_trainable_assisted_imitation_utilities import generate_combine_dataset_a\n\nlayout = SimpleNamespace(field_size=5, comms_size=3)\nds = generate_combine_dataset_a(layout=layout, num_samples=256, seed=7)\n</code></pre></p>"},{"location":"lin_trainable_assisted_imitation_utilities/#generate_combine_dataset_blayout-any-num_samples-int-seed-optionalint-none-arraydict","title":"<code>generate_combine_dataset_b(layout: Any, num_samples: int, seed: Optional[int] = None) -&gt; ArrayDict</code>","text":"<p>Purpose: Generate supervised triples for <code>LinCombineLayerB</code> where <code>shoot_target</code> is <code>parity(outcomes_b) XOR parity(comm)</code>.</p> <p>Arguments: - <code>layout</code>: Layout-like object or int; used to determine <code>n2 = field_size\u00b2</code> and <code>m = comms_size</code> (default <code>1</code>). - <code>num_samples</code>: Number of samples to generate. - <code>seed</code>: Optional RNG seed for reproducibility.</p> <p>Returns: - A dict with keys:   - <code>\"outcomes_b\"</code>: shape <code>(num_samples, n2)</code>, random float32 in <code>{0.0, 1.0}</code>   - <code>\"comm\"</code>: shape <code>(num_samples, m)</code>, random float32 in <code>{0.0, 1.0}</code>   - <code>\"shoot_target\"</code>: shape <code>(num_samples, 1)</code>, float32 in <code>{0.0, 1.0}</code></p> <p>Errors: - Not specified.</p> <p>Example: <pre><code>from types import SimpleNamespace\nfrom Q_Sea_Battle.lin_trainable_assisted_imitation_utilities import generate_combine_dataset_b\n\nlayout = SimpleNamespace(field_size=5, comms_size=2)\nds = generate_combine_dataset_b(layout=layout, num_samples=128, seed=99)\n</code></pre></p>"},{"location":"lin_trainable_assisted_imitation_utilities/#to_tf_datasetdataset-unionarraydict-sequencemappingstr-any-x_keys-sequencestr-y_key-str-batch_size-int-32-shuffle-bool-true-seed-optionalint-none-tfdatadataset","title":"<code>to_tf_dataset(dataset: Union[ArrayDict, Sequence[Mapping[str, Any]]], x_keys: Sequence[str], y_key: str, batch_size: int = 32, shuffle: bool = True, seed: Optional[int] = None) -&gt; tf.data.Dataset</code>","text":"<p>Purpose: Convert a generated dataset (dict-of-arrays or list-of-rows) into a batched <code>tf.data.Dataset</code> yielding <code>(x, y)</code> for Keras training.</p> <p>Arguments: - <code>dataset</code>: Either a mapping of arrays with leading dimension <code>N</code>, or a sequence of row mappings (e.g. <code>list[dict]</code>). - <code>x_keys</code>: Keys used as model inputs; if length is <code>1</code>, <code>x</code> is a single tensor; if length &gt; <code>1</code>, <code>x</code> is a tuple of tensors in this same order. - <code>y_key</code>: Key used as the target tensor. - <code>batch_size</code>: Batch size used by <code>.batch(...)</code>. - <code>shuffle</code>: Whether to shuffle before batching. - <code>seed</code>: Shuffle seed for reproducible ordering.</p> <p>Returns: - A <code>tf.data.Dataset</code> yielding <code>(x, y)</code> batches, with tensors cast to float32 via <code>_as_float01</code>.</p> <p>Errors: - <code>ImportError</code>: If TensorFlow is required but unavailable (guard present as <code>if tf is None</code>). - <code>ValueError</code>: If dict-of-arrays has inconsistent leading dimensions. - <code>ValueError</code>: If sequence-of-rows is empty.</p> <p>Example: <pre><code>from Q_Sea_Battle.lin_trainable_assisted_imitation_utilities import generate_combine_dataset_b, to_tf_dataset\n\nnp_ds = generate_combine_dataset_b(layout=5, num_samples=1000, seed=0)\ntf_ds = to_tf_dataset(np_ds, x_keys=[\"outcomes_b\", \"comm\"], y_key=\"shoot_target\", batch_size=64, shuffle=True, seed=0)\n</code></pre></p>"},{"location":"lin_trainable_assisted_imitation_utilities/#transfer_layer_weightssource_layer-tfkeraslayerslayer-target_layer-tfkeraslayerslayer-none","title":"<code>transfer_layer_weights(source_layer: tf.keras.layers.Layer, target_layer: tf.keras.layers.Layer) -&gt; None</code>","text":"<p>Purpose: Copy trained weights from one built Keras layer into another, with explicit validation and clear errors on mismatch.</p> <p>Arguments: - <code>source_layer</code>: Built layer providing weights. - <code>target_layer</code>: Built layer receiving weights.</p> <p>Returns: - <code>None</code>.</p> <p>Errors: - <code>ValueError</code>: If either layer is not built or has no weights. - <code>ValueError</code>: If number of weights differs. - <code>ValueError</code>: If any corresponding weight shapes differ (error message includes index and weight names when available).</p> <p>Example: <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.lin_trainable_assisted_imitation_utilities import transfer_layer_weights\n\nsrc = tf.keras.layers.Dense(8)\ntgt = tf.keras.layers.Dense(8)\n_ = src(tf.zeros((1, 4)))\n_ = tgt(tf.zeros((1, 4)))\ntransfer_layer_weights(src, tgt)\n</code></pre></p>"},{"location":"lin_trainable_assisted_imitation_utilities/#transfer_assisted_model_a_layer_weightstrained_measure_layer-tfkeraslayerslayer-trained_combine_layer-tfkeraslayerslayer-model_a-any-none","title":"<code>transfer_assisted_model_a_layer_weights(trained_measure_layer: tf.keras.layers.Layer, trained_combine_layer: tf.keras.layers.Layer, model_a: Any) -&gt; None</code>","text":"<p>Purpose: Copy trained measurement and combine layer weights into an object representing a full <code>LinTrainableAssistedModelA</code>-like model.</p> <p>Arguments: - <code>trained_measure_layer</code>: Trained measurement layer to copy from. - <code>trained_combine_layer</code>: Trained combine layer to copy from. - <code>model_a</code>: Target model object expected to have <code>measure_layer</code> and <code>combine_layer</code> attributes.</p> <p>Returns: - <code>None</code>.</p> <p>Errors: - <code>AttributeError</code>: If <code>model_a</code> lacks <code>measure_layer</code> or <code>combine_layer</code>. - Propagates <code>ValueError</code> from <code>transfer_layer_weights</code> for build/shape mismatches.</p> <p>Example: <pre><code>from Q_Sea_Battle.lin_trainable_assisted_imitation_utilities import transfer_assisted_model_a_layer_weights\n\n# model_a must expose .measure_layer and .combine_layer; exact class is not specified in this module.\ntransfer_assisted_model_a_layer_weights(trained_measure_layer, trained_combine_layer, model_a)\n</code></pre></p>"},{"location":"lin_trainable_assisted_imitation_utilities/#transfer_assisted_model_b_layer_weightstrained_measure_layer-tfkeraslayerslayer-trained_combine_layer-tfkeraslayerslayer-model_b-any-none","title":"<code>transfer_assisted_model_b_layer_weights(trained_measure_layer: tf.keras.layers.Layer, trained_combine_layer: tf.keras.layers.Layer, model_b: Any) -&gt; None</code>","text":"<p>Purpose: Symmetric helper to copy trained measurement and combine layer weights into a <code>LinTrainableAssistedModelB</code>-like object.</p> <p>Arguments: - <code>trained_measure_layer</code>: Trained measurement layer to copy from. - <code>trained_combine_layer</code>: Trained combine layer to copy from. - <code>model_b</code>: Target model object expected to have <code>measure_layer</code> and <code>combine_layer</code> attributes.</p> <p>Returns: - <code>None</code>.</p> <p>Errors: - <code>AttributeError</code>: If <code>model_b</code> lacks <code>measure_layer</code> or <code>combine_layer</code>. - Propagates <code>ValueError</code> from <code>transfer_layer_weights</code> for build/shape mismatches.</p> <p>Example: <pre><code>from Q_Sea_Battle.lin_trainable_assisted_imitation_utilities import transfer_assisted_model_b_layer_weights\n\ntransfer_assisted_model_b_layer_weights(trained_measure_layer, trained_combine_layer, model_b)\n</code></pre></p>"},{"location":"lin_trainable_assisted_imitation_utilities/#train_layerlayer-ds-loss-epochs-int-metricsnone","title":"<code>train_layer(layer, ds, loss, epochs: int, metrics=None)</code>","text":"<p>Signature: <code>train_layer(layer, ds, loss, epochs: int, metrics=None)</code></p> <p>Purpose: Train a single Keras layer (or callable) via supervised imitation by wrapping it into a minimal <code>tf.keras.Model</code> and running <code>model.fit(...)</code>.</p> <p>Arguments: - <code>layer</code>: <code>tf.keras.layers.Layer</code> or callable; must accept either a single tensor or multiple tensor inputs. - <code>ds</code>: <code>tf.data.Dataset</code> yielding <code>(x, y)</code>, where <code>x</code> is a tensor or tuple/list of tensors. - <code>loss</code>: Keras loss object or string. - <code>epochs</code>: Number of training epochs. - <code>metrics</code>: Optional list of Keras metrics; defaults to empty list.</p> <p>Returns: - A compiled and trained <code>tf.keras.Model</code> wrapping the provided <code>layer</code>.</p> <p>Errors: - Not specified; underlying TensorFlow/Keras errors may occur (e.g., incompatible shapes, invalid loss).</p> <p>Example: <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.lin_trainable_assisted_imitation_utilities import generate_measurement_dataset_a, to_tf_dataset, train_layer\n\nnp_ds = generate_measurement_dataset_a(layout=5, num_samples=512, seed=0)\nds = to_tf_dataset(np_ds, x_keys=[\"field\"], y_key=\"meas_target\", batch_size=32, shuffle=True, seed=0)\n\nlayer = tf.keras.layers.Dense(25, activation=\"sigmoid\")\nmodel = train_layer(layer=layer, ds=ds, loss=\"binary_crossentropy\", epochs=3, metrics=[tf.keras.metrics.BinaryAccuracy()])\n</code></pre></p>"},{"location":"lin_trainable_assisted_imitation_utilities/#constants","title":"Constants","text":"<ul> <li>None.</li> </ul>"},{"location":"lin_trainable_assisted_imitation_utilities/#types","title":"Types","text":"<ul> <li><code>ArrayDict</code>: Type alias for <code>Dict[str, np.ndarray]</code>.</li> </ul>"},{"location":"lin_trainable_assisted_imitation_utilities/#dependencies","title":"Dependencies","text":"<ul> <li><code>numpy</code> (<code>np</code>): random generation, array creation, parity computation, shape checks.</li> <li><code>tensorflow</code> (<code>tf</code>): <code>tf.data.Dataset</code>, tensor conversion, Keras layers/models/training.</li> <li><code>dataclasses.asdict</code>, <code>dataclasses.is_dataclass</code>: Imported but not used in this module.</li> <li><code>typing</code> utilities: <code>Any</code>, <code>Dict</code>, <code>Iterable</code>, <code>List</code>, <code>Mapping</code>, <code>MutableMapping</code>, <code>Optional</code>, <code>Sequence</code>, <code>Tuple</code>, <code>Union</code> (only a subset is used).</li> </ul>"},{"location":"lin_trainable_assisted_imitation_utilities/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Unknown (no design notes provided).</li> </ul>"},{"location":"lin_trainable_assisted_imitation_utilities/#deviations","title":"Deviations","text":"<ul> <li><code>to_tf_dataset</code> contains a guard <code>if tf is None</code>, but <code>tensorflow</code> is imported unconditionally as <code>tf</code> in this module; behavior when TensorFlow is missing is not specified beyond the raised <code>ImportError</code>.</li> <li><code>asdict</code> and <code>is_dataclass</code> are imported but unused.</li> <li><code>_as_float01</code> enforces <code>float32</code> but does not clamp/validate values beyond dtype conversion; values are assumed to be in <code>{0.0, 1.0}</code> by construction/caller.</li> </ul>"},{"location":"lin_trainable_assisted_imitation_utilities/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>Keep dataset generators reproducible by routing randomness through <code>_rng(seed)</code> and ensuring all returned arrays use leading dimension <code>(num_samples, ...)</code> with <code>np.float32</code>.</li> <li>When adding new dataset generators, preserve the dict-of-arrays contract so <code>to_tf_dataset</code> can consume them.</li> <li>Weight transfer helpers require layers to be built; ensure callers build layers (e.g., via a forward pass) before calling <code>transfer_layer_weights</code>.</li> <li><code>train_layer</code> infers input structure from the first batch of <code>ds</code>; ensure datasets yield consistent structures and fully-defined non-batch shapes.</li> </ul>"},{"location":"lin_trainable_assisted_imitation_utilities/#related","title":"Related","text":"<ul> <li>Referenced (not defined here): <code>LinMeasurementLayerA</code>, <code>LinMeasurementLayerB</code>, <code>LinCombineLayerA</code>, <code>LinCombineLayerB</code>, <code>LinTrainableAssistedModelA</code>, <code>LinTrainableAssistedModelB</code>, <code>PRAssistedLayer</code>.</li> </ul>"},{"location":"lin_trainable_assisted_imitation_utilities/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version (as stated in module docstring).</li> </ul>"},{"location":"lin_trainable_assisted_model_a/","title":"LinTrainableAssistedModelA","text":"<p>Role: Player A linear trainable-assisted baseline model that maps a field tensor to communication logits via measurement, assisted resource processing, and combination layers.</p> <p>Location: <code>Q_Sea_Battle.lin_trainable_assisted_model_a.LinTrainableAssistedModelA</code></p>"},{"location":"lin_trainable_assisted_model_a/#constructor","title":"Constructor","text":"Parameter Type Description field_size int, constraint: convertible via <code>int(field_size)</code>, scalar Field side length; used to derive \\(n2 = field\\_size \\times field\\_size\\). comms_size int, constraint: convertible via <code>int(comms_size)</code>, scalar Output communication size; forwarded to <code>LinCombineLayerA(comms_size=...)</code>. sr_mode str, constraint: any string, scalar Shared resource mode; forwarded to <code>PRAssistedLayer(mode=...)</code>; default <code>\"expected\"</code>. seed int | None, constraint: any integer or <code>None</code>, scalar Seed forwarded to <code>PRAssistedLayer(seed=...)</code>; default <code>0</code>. p_high float, constraint: convertible via <code>float(p_high)</code>, scalar High probability forwarded to <code>PRAssistedLayer(p_high=...)</code>; default <code>0.9</code>. resource_index int, constraint: convertible via <code>int(resource_index)</code>, scalar Resource index forwarded to <code>PRAssistedLayer(resource_index=...)</code>; default <code>0</code>. hidden_units_meas Sequence[int], constraint: sequence of ints, shape (m,) Hidden units configuration forwarded to <code>LinMeasurementLayerA(hidden_units=...)</code>; default <code>(64,)</code>. hidden_units_combine int | Sequence[int], constraint: int or sequence of ints, scalar or shape (m,) Hidden units configuration forwarded to <code>LinCombineLayerA(hidden_units=...)</code>; default <code>(64, 64)</code>. name str | None, constraint: any string or <code>None</code>, scalar Keras model name; if <code>None</code>, uses <code>\"LinTrainableAssistedModelA\"</code>. **kwargs Any, constraint: arbitrary keyword args Forwarded to <code>tf.keras.Model.__init__</code>. <p>Preconditions: <code>field_size</code> and <code>comms_size</code> are provided and convertible to <code>int</code>; <code>field_size</code> should be meaningful for forming \\(n2 = field\\_size^2\\) (not otherwise validated in code); <code>field_batch</code> inputs provided later should be convertible to <code>tf.Tensor</code> of dtype float32 in <code>_ensure_batched</code>. Postconditions: Attributes are created: <code>field_size</code> (int, scalar), <code>comms_size</code> (int, scalar), <code>n2</code> (int, scalar), <code>measurement</code> (LinMeasurementLayerA), <code>pr_assisted</code> (PRAssistedLayer), <code>combine</code> (LinCombineLayerA), plus backward-compatible aliases <code>measure_layer</code>, <code>sr_layer</code>, <code>combine_layer</code> pointing to the same objects. Errors: Not specified; any exceptions raised by TensorFlow/Keras or the referenced layers\u2019 constructors may propagate.  </p> <p>Example</p> <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.lin_trainable_assisted_model_a import LinTrainableAssistedModelA\n\nmodel = LinTrainableAssistedModelA(field_size=5, comms_size=8)\nx = tf.zeros((2, 25), dtype=tf.float32)\ny = model(x, training=False)\n</code></pre>"},{"location":"lin_trainable_assisted_model_a/#public-methods","title":"Public Methods","text":""},{"location":"lin_trainable_assisted_model_a/#_ensure_batched","title":"_ensure_batched","text":"<p>Signature: <code>_ensure_batched(x: tf.Tensor) -&gt; Tuple[tf.Tensor, bool]</code> (static method)</p> <p>Arguments: - x: tf.Tensor, dtype: any convertible to float32 via <code>tf.convert_to_tensor(..., dtype=tf.float32)</code>, shape (n2,) or (B, n2) where B is batch size</p> <p>Returns: - batched_x: tf.Tensor, dtype float32, shape (1, n2) if input rank is 1 else shape (B, n2) - was_batched: bool, constraint: <code>False</code> if input rank is 1 else <code>True</code>, scalar</p> <p>Behavior: Converts <code>x</code> to a float32 tensor; if <code>x.shape.rank == 1</code>, expands a batch dimension at axis 0 and returns <code>(expanded, False)</code>, otherwise returns <code>(x, True)</code>. Errors: Not specified; conversion/shape rank access errors may propagate from TensorFlow.</p>"},{"location":"lin_trainable_assisted_model_a/#compute_with_internal","title":"compute_with_internal","text":"<p>Signature: <code>compute_with_internal(self, field_batch: tf.Tensor, training: bool = False) -&gt; Tuple[tf.Tensor, List[tf.Tensor], List[tf.Tensor]]</code></p> <p>Arguments: - field_batch: tf.Tensor, dtype: any convertible to float32, shape (n2,) or (B, n2) - training: bool, constraint: boolean, scalar</p> <p>Returns: - comm_logits: tf.Tensor, dtype: Not specified (produced by <code>LinCombineLayerA</code>), shape (B, comms_size) if the combine layer follows the typical convention (exact shape not specified in this module) - measurements: List[tf.Tensor], length 1; element 0 is <code>meas_for_resource</code>: tf.Tensor, dtype float32, shape (B, n2) - internals: List[tf.Tensor], length 1; element 0 is <code>outcomes</code>: tf.Tensor, dtype: Not specified (produced by <code>PRAssistedLayer</code>), shape: Not specified</p> <p>Behavior: Ensures <code>field_batch</code> is batched float32; computes measurement probabilities via <code>self.measurement(field_batch, training=training)</code>; if <code>getattr(self.pr_assisted, \"mode\", \"expected\") == \"sample\"</code>, binarizes measurements with threshold <code>&gt;= 0.5</code> (float32), otherwise uses probabilities as-is; calls <code>self.pr_assisted</code> with a dict containing <code>current_measurement</code>, zero tensors for <code>previous_measurement</code> and <code>previous_outcome</code>, and <code>first_measurement</code> set to ones of shape (B, 1); combines assisted outcomes via <code>self.combine(outcomes, training=training)</code>; returns logits plus internal tensors wrapped in lists. Errors: Not specified; any exceptions raised by the underlying layers or TensorFlow ops may propagate.</p>"},{"location":"lin_trainable_assisted_model_a/#call","title":"call","text":"<p>Signature: <code>call(self, field_batch: tf.Tensor, training: bool = False) -&gt; tf.Tensor</code></p> <p>Arguments: - field_batch: tf.Tensor, dtype: any convertible to float32, shape (n2,) or (B, n2) - training: bool, constraint: boolean, scalar</p> <p>Returns: - comm_logits: tf.Tensor, dtype: Not specified (produced by <code>LinCombineLayerA</code>), shape: Not specified in this module</p> <p>Behavior: Delegates to <code>compute_with_internal(..., training=training)</code> and returns only <code>comm_logits</code>. Errors: Not specified; propagates errors from <code>compute_with_internal</code>.</p>"},{"location":"lin_trainable_assisted_model_a/#data-state","title":"Data &amp; State","text":"<ul> <li>field_size: int, constraint: <code>int(field_size)</code> conversion applied, scalar; used to derive <code>n2</code>.</li> <li>comms_size: int, constraint: <code>int(comms_size)</code> conversion applied, scalar.</li> <li>n2: int, constraint: equals <code>field_size * field_size</code>, scalar; flattened field length.</li> <li>measurement: LinMeasurementLayerA, constraint: constructed as <code>LinMeasurementLayerA(n2=self.n2, hidden_units=hidden_units_meas)</code>.</li> <li>pr_assisted: PRAssistedLayer, constraint: constructed as <code>PRAssistedLayer(length=self.n2, p_high=float(p_high), mode=str(sr_mode), resource_index=int(resource_index), seed=seed, name=\"PRAssistedLayerA\")</code>.</li> <li>combine: LinCombineLayerA, constraint: constructed as <code>LinCombineLayerA(comms_size=self.comms_size, hidden_units=hidden_units_combine)</code>.</li> <li>measure_layer: LinMeasurementLayerA, alias of <code>measurement</code> (backward-compatible).</li> <li>sr_layer: PRAssistedLayer, alias of <code>pr_assisted</code> (backward-compatible).</li> <li>combine_layer: LinCombineLayerA, alias of <code>combine</code> (backward-compatible).</li> </ul>"},{"location":"lin_trainable_assisted_model_a/#planned-design-spec","title":"Planned (design-spec)","text":"<p>Not specified.</p>"},{"location":"lin_trainable_assisted_model_a/#deviations","title":"Deviations","text":"<p>Not specified.</p>"},{"location":"lin_trainable_assisted_model_a/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>Keep input handling consistent with <code>_ensure_batched</code>: this module assumes rank-1 inputs represent a single flattened field and will be expanded to shape (1, n2).</li> <li><code>compute_with_internal</code> uses <code>getattr(self.pr_assisted, \"mode\", \"expected\")</code> at runtime; changes to <code>PRAssistedLayer</code> mode handling can affect the measurement binarization path.</li> </ul>"},{"location":"lin_trainable_assisted_model_a/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.lin_measurement_layer_a.LinMeasurementLayerA</code></li> <li><code>Q_Sea_Battle.pr_assisted_layer.PRAssistedLayer</code></li> <li><code>Q_Sea_Battle.lin_combine_layer_a.LinCombineLayerA</code></li> </ul>"},{"location":"lin_trainable_assisted_model_a/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version documented from module text.</li> </ul>"},{"location":"lin_trainable_assisted_model_b/","title":"LinTrainableAssistedModelB","text":"<p>Role: Player B linear trainable-assisted baseline model that consumes Player A\u2019s previous measurement and PR-assisted outcome tensors to produce a shoot logit.</p> <p>Location: <code>Q_Sea_Battle.lin_trainable_assisted_model_b.LinTrainableAssistedModelB</code></p>"},{"location":"lin_trainable_assisted_model_b/#derived-constraints","title":"Derived constraints","text":"<p>Define <code>field_size</code> as the side length of the square field, <code>comms_size</code> as the size of the communication vector, and <code>n2 = field_size * field_size</code> as the flattened field length.</p>"},{"location":"lin_trainable_assisted_model_b/#constructor","title":"Constructor","text":"Parameter Type Description field_size int, constraint: \\(field\\_size \\ge 1\\) Side length of the square field; used to compute <code>n2 = field_size * field_size</code>. comms_size int, constraint: \\(comms\\_size \\ge 1\\) Length of the communication vector consumed by the combine layer. sr_mode str, constraint: unspecified allowed values, default <code>\"expected\"</code> Mode passed to <code>PRAssistedLayer</code> as <code>mode</code>. If the resulting <code>self.pr_assisted.mode == \"sample\"</code>, measurements are thresholded to binary via <code>meas_probs_b &gt;= 0.5</code>. seed int | None, constraint: unspecified, default <code>0</code> Seed passed to <code>PRAssistedLayer</code>. p_high float, constraint: unspecified, default <code>0.9</code> <code>p_high</code> passed to <code>PRAssistedLayer</code>. resource_index int, constraint: unspecified, default <code>0</code> <code>resource_index</code> passed to <code>PRAssistedLayer</code>. hidden_units_meas Sequence[int], constraint: unspecified, default <code>(64,)</code> Hidden layer configuration passed to <code>LinMeasurementLayerB(hidden_units=...)</code>. hidden_units_combine int | Sequence[int], constraint: unspecified, default <code>(64, 64)</code> Hidden layer configuration passed to <code>LinCombineLayerB(hidden_units=...)</code>. name str | None, constraint: unspecified, default <code>None</code> Keras model name; if <code>None</code>, uses <code>\"LinTrainableAssistedModelB\"</code>. **kwargs Any, constraint: unspecified Forwarded to <code>tf.keras.Model.__init__</code>. <p>Preconditions: <code>field_size</code> and <code>comms_size</code> must be convertible to <code>int</code>.</p> <p>Postconditions: Sets <code>self.field_size: int</code>, <code>self.comms_size: int</code>, <code>self.n2: int</code>; constructs <code>self.measurement: LinMeasurementLayerB</code>, <code>self.pr_assisted: PRAssistedLayer</code>, <code>self.combine: LinCombineLayerB</code>; also defines backward-compatible aliases <code>measure_layer</code>, <code>pr_layer</code>, <code>resource_layer</code>, <code>sr_layer</code>, <code>combine_layer</code> referencing those layers.</p> <p>Errors: Not specified (aside from potential errors raised by sublayer constructors or invalid type conversions).</p> <p>Example</p> <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.lin_trainable_assisted_model_b import LinTrainableAssistedModelB\n\nmodel = LinTrainableAssistedModelB(field_size=5, comms_size=3, sr_mode=\"expected\")\n\nB = 2\nn2 = 25\ngun = tf.zeros((B, n2), dtype=tf.float32)\ncomm = tf.zeros((B, 3), dtype=tf.float32)\nprev_meas_list = [tf.zeros((B, n2), dtype=tf.float32)]\nprev_out_list = [tf.zeros((B, n2), dtype=tf.float32)]\n\nshoot_logit = model([gun, comm, prev_meas_list, prev_out_list], training=False)\n</code></pre>"},{"location":"lin_trainable_assisted_model_b/#public-methods","title":"Public Methods","text":""},{"location":"lin_trainable_assisted_model_b/#_ensure_batched","title":"_ensure_batched","text":"<p>Static method: <code>LinTrainableAssistedModelB._ensure_batched(x) -&gt; (x_batched, already_batched)</code></p> <p>Parameters:</p> <ul> <li><code>x</code>: tf.Tensor, dtype convertible to float32, shape (n2,) or (B, n2) or any tensor where rank is used to decide batching; rank-1 inputs are expanded on axis 0.</li> </ul> <p>Returns:</p> <ul> <li><code>x_batched</code>: tf.Tensor, dtype float32, shape (1, N) if input rank is 1, else same shape as input (rank not modified); <code>N</code> is the input\u2019s last-dimension size when rank-1.</li> <li><code>already_batched</code>: bool, constraint: <code>False</code> if input rank is 1 else <code>True</code>.</li> </ul> <p>Errors: Not specified.</p>"},{"location":"lin_trainable_assisted_model_b/#compute_with_internal","title":"compute_with_internal","text":"<p>Method: <code>compute_with_internal(gun_batch, comm_batch, prev_meas_list, prev_out_list, training=False) -&gt; (shoot_logit, meas_list, out_list)</code></p> <p>Parameters:</p> <ul> <li><code>gun_batch</code>: tf.Tensor, dtype float32, shape (B, n2) (rank-1 shape (n2,) is accepted and converted to (1, n2)); last dimension must equal <code>n2</code>.</li> <li><code>comm_batch</code>: tf.Tensor, dtype float32, shape (B, comms_size) (rank-1 shape (comms_size,) is accepted and converted to (1, comms_size)); last dimension must equal <code>comms_size</code>.</li> <li><code>prev_meas_list</code>: List[tf.Tensor] (or tf.Tensor accepted and normalized to a list), dtype float32, shape list length \\(\\ge 1\\) where element 0 has shape (B, n2).</li> <li><code>prev_out_list</code>: List[tf.Tensor] (or tf.Tensor accepted and normalized to a list), dtype float32, shape list length \\(\\ge 1\\) where element 0 has shape (B, n2).</li> <li><code>training</code>: bool, constraint: unspecified, default <code>False</code>; forwarded to <code>self.measurement(...)</code> and <code>self.combine(...)</code>.</li> </ul> <p>Returns:</p> <ul> <li><code>shoot_logit</code>: tf.Tensor, dtype float32, shape (B, 1) (if combine returns rank-1 (B,), it is expanded to (B, 1)).</li> <li><code>meas_list</code>: List[tf.Tensor], constraint: list length 1, containing <code>meas_for_resource_b</code>: tf.Tensor, dtype float32, shape (B, n2); if <code>self.pr_assisted.mode == \"sample\"</code>, this tensor is binary-valued in {0.0, 1.0} via thresholding at 0.5, otherwise it equals <code>meas_probs_b</code> cast to float32.</li> <li><code>out_list</code>: List[tf.Tensor], constraint: list length 1, containing <code>outcomes_b</code>: tf.Tensor, dtype not specified by this module, shape (B, n2) as produced by <code>self.pr_assisted(...)</code>.</li> </ul> <p>Errors:</p> <ul> <li>Raises <code>ValueError</code> if <code>comm_batch</code> is not rank-2 after batching normalization, or if its last dimension is not <code>comms_size</code>.</li> <li>Raises <code>ValueError</code> if <code>gun_batch</code> is not rank-2 after batching normalization, or if its last dimension is not <code>n2</code>.</li> <li>Raises <code>ValueError</code> if <code>prev_meas_list</code> or <code>prev_out_list</code> is empty after normalization to list/tuple.</li> <li>Raises <code>ValueError</code> if <code>prev_meas</code> or <code>prev_out</code> is not rank-2, or if their last dimensions are not <code>n2</code>.</li> <li>Raises <code>ValueError</code> if <code>meas_for_resource_b</code> is not rank-2 or its last dimension is not <code>n2</code>.</li> </ul> <p>PR-assisted invocation contract</p> <p>This method calls <code>self.pr_assisted</code> with a dict containing keys <code>\"current_measurement\"</code>, <code>\"previous_measurement\"</code>, <code>\"previous_outcome\"</code>, and <code>\"first_measurement\"</code> where <code>\"first_measurement\"</code> is a zeros tensor of shape (B, 1) and dtype float32.</p>"},{"location":"lin_trainable_assisted_model_b/#call","title":"call","text":"<p>Method: <code>call(inputs, training=False, **kwargs) -&gt; shoot_logit</code></p> <p>Parameters:</p> <ul> <li><code>inputs</code>: list, constraint: must be a list/tuple of length 4 in the order <code>[gun_batch, comm_batch, prev_meas_list, prev_out_list]</code>; element constraints are as in <code>compute_with_internal</code>.</li> <li><code>training</code>: bool, constraint: unspecified, default <code>False</code>; forwarded to <code>compute_with_internal</code>.</li> <li><code>**kwargs</code>: Any, constraint: accepted but not used by this implementation.</li> </ul> <p>Returns:</p> <ul> <li><code>shoot_logit</code>: tf.Tensor, dtype float32, shape (B, 1).</li> </ul> <p>Errors:</p> <ul> <li>Raises <code>ValueError</code> if <code>inputs</code> is not a list/tuple of length 4.</li> <li>Propagates <code>ValueError</code> from <code>compute_with_internal</code> for shape/rank violations.</li> </ul>"},{"location":"lin_trainable_assisted_model_b/#data-state","title":"Data &amp; State","text":"<p>Instance attributes created by the constructor:</p> <ul> <li><code>field_size</code>: int, constraint: \\(field\\_size \\ge 1\\), scalar.</li> <li><code>comms_size</code>: int, constraint: \\(comms\\_size \\ge 1\\), scalar.</li> <li><code>n2</code>: int, constraint: \\(n2 = field\\_size * field\\_size\\), scalar.</li> <li><code>measurement</code>: LinMeasurementLayerB, constraint: constructed with <code>n2=self.n2</code> and <code>hidden_units=hidden_units_meas</code>.</li> <li><code>pr_assisted</code>: PRAssistedLayer, constraint: constructed with <code>length=self.n2</code>, <code>p_high</code>, <code>mode=sr_mode</code>, <code>resource_index</code>, <code>seed</code>, and <code>name=\"PRAssistedLayerB\"</code>.</li> <li><code>combine</code>: LinCombineLayerB, constraint: constructed with <code>comms_size=self.comms_size</code> and <code>hidden_units=hidden_units_combine</code>.</li> <li>Backward-compatible aliases: <code>measure_layer</code>, <code>pr_layer</code>, <code>resource_layer</code>, <code>sr_layer</code>, <code>combine_layer</code> referencing the corresponding primary layer attributes.</li> </ul>"},{"location":"lin_trainable_assisted_model_b/#planned-design-spec","title":"Planned (design-spec)","text":"<p>Not specified.</p>"},{"location":"lin_trainable_assisted_model_b/#deviations","title":"Deviations","text":"<p>Not specified.</p>"},{"location":"lin_trainable_assisted_model_b/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li><code>compute_with_internal</code> attempts two calling conventions for <code>LinCombineLayerB</code>: first positional <code>(outcomes_b, comm_batch, training=...)</code>, then a dict <code>{\"outcomes\": outcomes_b, \"comm\": comm_batch}</code> if a <code>TypeError</code> occurs; maintain compatibility if evolving <code>LinCombineLayerB</code>.</li> <li>The <code>\"first_measurement\"</code> flag provided to <code>PRAssistedLayer</code> is always zeros with shape (B, 1); any changes to the PR-assisted contract should account for this fixed behavior.</li> <li>The <code>call</code> signature accepts <code>**kwargs</code> but does not use it; adding usage should be done carefully to preserve Keras compatibility.</li> </ul>"},{"location":"lin_trainable_assisted_model_b/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.lin_measurement_layer_b.LinMeasurementLayerB</code></li> <li><code>Q_Sea_Battle.lin_combine_layer_b.LinCombineLayerB</code></li> <li><code>Q_Sea_Battle.pr_assisted_layer.PRAssistedLayer</code></li> </ul>"},{"location":"lin_trainable_assisted_model_b/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial documented version from module header.</li> </ul>"},{"location":"lin_trainable_models/","title":"Q_Sea_Battle.lin_trainable_models","text":"<p>Role: Re-export module for trainable linear (Lin) assisted models for Player A and Player B.</p> <p>Location: <code>Q_Sea_Battle.lin_trainable_models</code></p>"},{"location":"lin_trainable_models/#overview","title":"Overview","text":"<p>This module provides a consolidated import location for trainable linear assisted models used by two different players (A and B). It defines project terminology around shared resources (SR) and explicitly avoids the term \"shared randomness\" within this project.</p> <p>Terminology (as specified by the module docstring): SR (shared resource) is any pre-shared auxiliary resource available to both players without communication; <code>PRAssistedLayer</code> (defined in <code>pr_assisted_layer.py</code>) is a specific type of SR.</p>"},{"location":"lin_trainable_models/#public-api","title":"Public API","text":""},{"location":"lin_trainable_models/#functions","title":"Functions","text":"<p>Not specified.</p>"},{"location":"lin_trainable_models/#constants","title":"Constants","text":"<ul> <li><code>__all__</code>: <code>[\"LinTrainableAssistedModelA\", \"LinTrainableAssistedModelB\"]</code>   Purpose: Declares the public symbols re-exported by this module.</li> </ul>"},{"location":"lin_trainable_models/#types","title":"Types","text":"<p>Not specified.</p>"},{"location":"lin_trainable_models/#dependencies","title":"Dependencies","text":"<ul> <li><code>Q_Sea_Battle.lin_trainable_assisted_model_a.LinTrainableAssistedModelA</code> (imported and re-exported)</li> <li><code>Q_Sea_Battle.lin_trainable_assisted_model_b.LinTrainableAssistedModelB</code> (imported and re-exported)</li> </ul>"},{"location":"lin_trainable_models/#planned-design-spec","title":"Planned (design-spec)","text":"<p>Not specified.</p>"},{"location":"lin_trainable_models/#deviations","title":"Deviations","text":"<p>Not specified.</p>"},{"location":"lin_trainable_models/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>This module is a re-export layer; functional behavior and implementation details are expected to live in the imported modules.</li> <li>Keep <code>__all__</code> aligned with the intended public surface for stable imports.</li> </ul>"},{"location":"lin_trainable_models/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.lin_trainable_assisted_model_a</code> (source of <code>LinTrainableAssistedModelA</code>)</li> <li><code>Q_Sea_Battle.lin_trainable_assisted_model_b</code> (source of <code>LinTrainableAssistedModelB</code>)</li> <li><code>Q_Sea_Battle.pr_assisted_layer</code> (mentioned as defining <code>PRAssistedLayer</code>; not imported here)</li> </ul>"},{"location":"lin_trainable_models/#changelog","title":"Changelog","text":"<ul> <li>Version: 0.1 (as specified in the module docstring)</li> </ul>"},{"location":"linear_algorithm/","title":"Linear Algorithm","text":""},{"location":"linear_algorithm/#overview","title":"Overview","text":"<p>The linear algorithm is a classical assisted strategy for random access codes (RACs) based on parity (XOR) encoding and decoding. The linear algorithm relies on a shared resource and defines the measurement stragegy for this shared resources, as well as the interpretation of the outcomes.</p> <p>In QSeaBattle, the linear algorithm is implemented as a teacher strategy that generates supervised imitation targets for trainable models. The implementation is inspired by the linear inner-product\u2013based constructions discussed by van Dam in his work on communication complexity and nonlocal correlations.</p>"},{"location":"linear_algorithm/#conceptual-background-van-dam","title":"Conceptual Background (van Dam)","text":"<p>Van Dam shows that Boolean functions can be expressed as sums of products over \\(\\{0,1\\}\\), i.e. linear combinations modulo 2, and that distributed parties with access to shared resources can evaluate inner products efficiently by exchanging a single bit. In this view, linear RAC strategies encode information in parity bits, and decoding consists of XOR-ing communicated bits with locally computed parities.</p> <p>In the language of van Dam, Alice and Bob share a distributed system \\(\\Phi_{AB}\\) that produces correlated outcomes based on their local measurement choices. In QSeaBattle, this role is played by a shared-resource layer (<code>PRAssistedLayer</code>) that mediates correlations between Alice\u2019s and Bob\u2019s measurements.</p>"},{"location":"linear_algorithm/#the-linear-algorithm-encoding-decoding-as-implemented","title":"The Linear Algorithm \u2013 Encoding &amp; Decoding (as Implemented)","text":""},{"location":"linear_algorithm/#state-and-notation","title":"State and notation","text":"<ul> <li>Let \\(n^2 = \\texttt{field\\_size}^2\\) be the number of field bits.</li> <li>Alice\u2019s input field is \\(x \\in \\{0,1\\}^{n^2}\\).</li> <li>Bob\u2019s index state is represented by a one-hot gun vector \\(g \\in \\{0,1\\}^{n^2}\\).</li> <li>Alice sends a communication vector \\(c \\in \\{0,1\\}^m\\).</li> <li>Alice and Bob share a distributed resource \\(\\Phi_{AB}\\) producing outcome vectors in \\(\\{0,1\\}^{n^2}\\).</li> <li>All additions are modulo 2 (XOR).</li> </ul>"},{"location":"linear_algorithm/#alice-side-model-a","title":"Alice side (Model A)","text":""},{"location":"linear_algorithm/#measurement-a-identity-measurement","title":"Measurement A (identity measurement)","text":"<p>Alice\u2019s measurement is linear and trivial: each measurement outcome equals the corresponding field bit, $$ m_i^{(A)} = x_i. $$</p>"},{"location":"linear_algorithm/#shared-resource-interaction-first-measurement","title":"Shared-resource interaction (first measurement)","text":"<p>The measurement vector \\(m^{(A)}\\) is passed to her part of the shared resource \\(\\Phi_{AB}\\) as the current measurement. Since this is the first interaction with the shared resource, the previous measurement and outcome are set to zero, and a <code>first_measurement</code> flag is raised.</p> <p>The shared resource produces an outcome vector $$ o^{(A)} \\in {0,1}^{n^2}. $$</p>"},{"location":"linear_algorithm/#combine-a-communication-generation","title":"Combine A (communication generation)","text":"<p>Alice computes the parity of all shared-resource outcomes, $$ p^{(A)} = \\bigoplus_{i=1}^{n^2} o^{(A)}_i. $$</p> <p>This single parity bit is then replicated to form the communication vector: $$ c = (p^{(A)}, p^{(A)}, \\dots, p^{(A)}) \\in {0,1}^m. $$</p>"},{"location":"linear_algorithm/#bob-side-model-b","title":"Bob side (Model B)","text":""},{"location":"linear_algorithm/#measurement-b-identity-measurement-on-the-gun","title":"Measurement B (identity measurement on the gun)","text":"<p>Bob\u2019s measurement is also linear and identity-like: $$ m_i^{(B)} = g_i. $$</p>"},{"location":"linear_algorithm/#shared-resource-interaction-second-measurement","title":"Shared-resource interaction (second measurement)","text":"<p>Bob feeds his measurement \\(m^{(B)}\\) into the his part of the same shared resource \\(\\Phi_{AB}\\), together with Alice\u2019s previous measurement and outcome. The <code>first_measurement</code> flag is now unset.</p> <p>The shared resource produces a second outcome vector $$ o^{(B)} \\in {0,1}^{n^2}. $$</p>"},{"location":"linear_algorithm/#combine-b-final-decision","title":"Combine B (final decision)","text":"<p>Bob computes: - the parity of his shared-resource outcomes, $$ p^{(B)} = \\bigoplus_{i=1}^{n2} o^{(B)}i, $$ - the parity of the received communication vector, $$ p^{(c)} = \\bigoplus c_j. $$}^{m</p> <p>The final output (shoot decision) is $$ \\text{shoot} = p^{(B)} \\oplus p^{(c)}. $$</p>"},{"location":"linear_algorithm/#interpretation","title":"Interpretation","text":"<p>Operationally, the algorithm realizes the following idea:</p> <ul> <li>Alice encodes global information about her input field into a single parity bit, mediated by shared-resource correlations.</li> <li>Bob combines this communicated parity with his own parity, derived from the shared resource and his index, to recover the desired bit.</li> </ul>"},{"location":"linear_algorithm/#conditions-and-limitations","title":"Conditions and Limitations","text":"<ul> <li>The algorithm is not claimed to be optimal for classical RACs.</li> <li>Correctness and performance depend on:</li> <li>uniform input distributions,</li> <li>noiseless classical communication,</li> <li>the behavior of the shared-resource layer.</li> <li>The shared resource affects only the parity values and does not alter the gun or field states.</li> <li>The strategy serves as a baseline teacher for imitation learning, not as a physical or information-theoretic bound.</li> </ul>"},{"location":"linear_algorithm/#qseabattle-implementation-notes","title":"QSeaBattle implementation notes","text":"<ul> <li>The algorithm is implemented across:</li> <li><code>LinTrainableAssistedModelA</code>,</li> <li><code>LinTrainableAssistedModelB</code>,</li> <li>dataset generators in <code>lin_trainable_assisted_imitation_utilities.py</code>.</li> <li>Measurement layers implement identity mappings.</li> <li>Combine layers implement parity (XOR-reduction) logic.</li> <li>The shared-resource layer corresponds conceptually to van Dam\u2019s distributed system \\(\\Phi_{AB}\\).</li> </ul>"},{"location":"linear_algorithm/#key-references","title":"Key References","text":"<ul> <li>W. van Dam, Implausible Consequences of Superstrong Nonlocality, arXiv:quant-ph/0501159.</li> <li>W. van Dam, Nonlocality &amp; Communication Complexity, D.Phil. thesis, University of Oxford, Chapter 9.</li> </ul>"},{"location":"logit_utilities/","title":"Q_Sea_Battle.logit_utilities","text":"<p>Role: Utilities for stable conversions between logits, probabilities and log-probabilities.</p> <p>Location: <code>Q_Sea_Battle.logit_utilities</code></p>"},{"location":"logit_utilities/#overview","title":"Overview","text":"<p>This module provides numerically stable utilities for converting Bernoulli logits to probabilities and computing Bernoulli log-probabilities without explicitly forming probabilities that may underflow or overflow.</p>"},{"location":"logit_utilities/#public-api","title":"Public API","text":""},{"location":"logit_utilities/#functions","title":"Functions","text":""},{"location":"logit_utilities/#logit_to_problogits-arraylike-arraylike","title":"<code>logit_to_prob(logits: ArrayLike) -&gt; ArrayLike</code>","text":"<p>Signature: <code>logit_to_prob(logits: ArrayLike) -&gt; ArrayLike</code> Purpose: Convert Bernoulli logits to probabilities in a numerically stable way. Arguments: - <code>logits</code>: Scalar or array-like of pre-sigmoid activations. Returns: Probabilities with the same shape as <code>logits</code> and values in <code>[0.0, 1.0]</code>; if the input was a scalar, returns a Python <code>float</code>. Errors: Not specified. Example: <pre><code>import numpy as np\nfrom Q_Sea_Battle.logit_utilities import logit_to_prob\n\np0 = logit_to_prob(0.0)              # 0.5\np  = logit_to_prob(np.array([-2, 0, 2], dtype=float))  # array([..., 0.5, ...])\n</code></pre></p>"},{"location":"logit_utilities/#logit_to_logproblogits-arraylike-actions-arraylike-arraylike","title":"<code>logit_to_logprob(logits: ArrayLike, actions: ArrayLike) -&gt; ArrayLike</code>","text":"<p>Signature: <code>logit_to_logprob(logits: ArrayLike, actions: ArrayLike) -&gt; ArrayLike</code> Purpose: Compute log-probability <code>log \u03c0(a | logits)</code> for Bernoulli actions using a stable closed-form expression <code>-softplus((1 - 2a) * z)</code>. Arguments: - <code>logits</code>: Scalar or array-like of logits. - <code>actions</code>: Scalar or array-like of the same shape as <code>logits</code> with values in <code>{0, 1}</code> indicating the chosen Bernoulli outcome. Returns: Log-probabilities with the same shape as the broadcast of <code>logits</code> and <code>actions</code>; if both inputs were scalars, returns a Python <code>float</code>. Errors: - <code>ValueError</code>: If <code>logits</code> and <code>actions</code> cannot be broadcast to a common shape. - <code>ValueError</code>: If <code>actions</code> contains values other than <code>0</code> or <code>1</code>. Example: <pre><code>import numpy as np\nfrom Q_Sea_Battle.logit_utilities import logit_to_logprob\n\nlp = logit_to_logprob(0.0, 1)  # log(0.5)\n\nlogits = np.array([-1.0, 0.0, 1.0])\nactions = np.array([0, 1, 1])\nlps = logit_to_logprob(logits, actions)\n</code></pre></p>"},{"location":"logit_utilities/#constants","title":"Constants","text":"<p>Not specified.</p>"},{"location":"logit_utilities/#types","title":"Types","text":"<ul> <li><code>ArrayLike</code>: <code>typing.Union[float, int, numpy.ndarray]</code></li> </ul>"},{"location":"logit_utilities/#dependencies","title":"Dependencies","text":"<ul> <li>Python: <code>__future__.annotations</code></li> <li>Standard library: <code>typing.Union</code></li> <li>Third-party: <code>numpy</code> (imported as <code>np</code>)</li> </ul>"},{"location":"logit_utilities/#planned-design-spec","title":"Planned (design-spec)","text":"<p>Not specified.</p>"},{"location":"logit_utilities/#deviations","title":"Deviations","text":"<p>Not specified.</p>"},{"location":"logit_utilities/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>This module includes internal helpers (<code>_to_array_and_flag</code>, <code>_from_array</code>, <code>_softplus</code>) that implement scalar/array handling and numerical stability; they are intentionally not part of the documented public API.</li> <li>When changing numerical formulas, preserve stability for large-magnitude logits and avoid creating intermediate probabilities when computing log-probabilities.</li> </ul>"},{"location":"logit_utilities/#related","title":"Related","text":"<ul> <li>NumPy broadcasting rules: used by <code>np.broadcast_arrays</code> in <code>logit_to_logprob</code>.</li> </ul>"},{"location":"logit_utilities/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version (module docstring).</li> </ul>"},{"location":"majority_algorithm/","title":"Majority Algorithm in Classical Random Access Codes (RACs)","text":""},{"location":"majority_algorithm/#overview","title":"Overview","text":"<p>In a classical \\(n \\to 1\\) random access code (RAC), Alice holds an \\(n\\)-bit string and wants Bob to retrieve any one of those \\(n\\) bits with high probability, even though Alice is only allowed to send a single bit (a one-bit message) to Bob. The \u201cmajority\u201d algorithm \u2013 a simple deterministic encoding/decoding strategy \u2013 has been conjectured and proven to be an optimal classical strategy for this task in terms of maximizing the average success probability. Below we summarize how this algorithm works and why it is optimal, along with conditions for its optimality and known limitations. We also indicate how this algorithm is implemented in QSeaBattle.</p>"},{"location":"majority_algorithm/#the-majority-algorithm-encoding-decoding","title":"The Majority Algorithm \u2013 Encoding &amp; Decoding","text":"<p>Algorithm (Majority-encoding, Identity-decoding): Alice counts the number of 0\u2019s and 1\u2019s in her \\(n\\)-bit string and sends a single bit indicating which value is in the majority. (If there is a tie, she can choose either 0 or 1 consistently as a convention.) Bob, upon receiving this one-bit message, simply interprets it as the bit value for any position he is asked to retrieve. In other words, Bob outputs the same bit he received (this is \u201cidentity decoding\u201d) as his guess for the requested index.</p> <ul> <li>Example: If Alice\u2019s string is <code>10110</code> (which has three <code>1</code>s and two <code>0</code>s), she will send \u201c1\u201d because 1 is the majority bit. Regardless of which index \\(j\\) (1 through 5) Bob wants to know, Bob will answer \u201c1\u201d. In this example, Bob will be correct for any bit that was actually 1 (the three positions containing <code>1</code>), and wrong for any position that was 0. If we assume all input strings and query positions are uniformly random, Bob\u2019s guess using the majority scheme is correct with probability \\(\\frac{\\text{count of majority bits in }x}{n}\\). Here that\u2019s \\(3/5 = 60%\\) for this specific string. The average success probability over all 5-bit strings turns out to be about 68.75% for this strategy \u2013 and no other deterministic encoding can do better for 5 bits.</li> </ul>"},{"location":"majority_algorithm/#optimality-and-theoretical-justification","title":"Optimality and Theoretical Justification","text":"<p>For uniform random inputs and query positions (the standard RAC scenario measuring average success), the majority algorithm is provably optimal among all deterministic encodings. Intuitively, by sending the most frequent bit value, Alice maximizes the chance that a randomly chosen bit from her string matches the sent bit. Any other encoding would either send a minority value (reducing success on average) or encode some more complex function of the bits, which ends up being less correlated with a random individual bit than the majority vote is.</p> <p>Historical context: Earlier works had already established the optimal success rate for small cases. In particular, it was known that no classical strategy can exceed 75% average accuracy for the 2-bit case, and the majority-vote strategy achieves this bound. This strongly suggested that using majority encoding is best for larger \\(n\\) as well. Indeed, Tavakoli et al. (2015) conjectured that \u201cAlice sends the most frequent symbol and Bob outputs that symbol\u201d is the optimal classical strategy for general \\(n\\). They calculated formulae for the expected success probability of this majority algorithm in general bases, noting it matched all known optimal values in binary cases.</p> <p>Proof of optimality: The conjecture was later proven rigorously by Ambainis, Kravchenko, and Rai (2015). They showed that for classical RACs with any alphabet size (including binary), no other deterministic or even probabilistic strategy can surpass the majority-encoding scheme\u2019s average success rate. In fact, their theorem establishes that the majority vote (with identity decoding) is one of a family of optimal strategies, meaning if a strategy beats the majority\u2019s performance, it would violate the derived optimality conditions. The proof characterizes all such optimal strategies and confirms that majority encoding is among them, thereby \u201cprovid(ing) a firm basis\u201d for using majority as the benchmark for classical RAC performance. </p> <p>Theorem (Ambainis et al. 2015): Alice sending the most frequent letter in her input \\(x\\) and Bob answering with that same letter for any query \\(j\\) is an optimal deterministic strategy for the \\(n \\to 1\\) RAC (no classical strategy can achieve higher average success). (They further found that this strategy is essentially unique up to symmetry \u2013 any optimal strategy must act like a \u201cmajority vote\u201d possibly after relabeling symbols.) </p>"},{"location":"majority_algorithm/#conditions-and-limitations","title":"Conditions and Limitations","text":"<p>Scope of optimality: The majority algorithm\u2019s optimality holds under the usual assumptions of RACs: Alice\u2019s input bits are uniformly random, and Bob\u2019s target index is uniformly random (so each bit is equally likely to be asked). The optimality is in the average-case sense \u2013 it maximizes the overall probability of correct decoding averaged over all inputs and target choices. Under these conditions, even allowing shared randomness or probabilistic encoding cannot surpass the performance of a deterministic majority vote strategy (by linearity of expectation, any randomized strategy is a mixture of deterministic ones, so at least one deterministic optimum exists). </p> <p>In the context of QSeaBattle the optimality holds holds for uniform inputs (enemy probability equal to 50%), a communication size of 1 bit (\\(m = 1\\)), and noiseless channels. </p>"},{"location":"majority_algorithm/#qseabattle-implementation","title":"QSeaBattle implementation","text":"<p>The majority algorithm is implements as one of the deterministic strategies. In case Alice is allowed to communicate more than one but (\\(m &gt; 1\\)), we implement the majority algorithm block-wise (the total input string is divided in blocks of equal size \\(L\\). The total field size is \\(mL\\)). The expected probability that Bob is correct on a randomly chosen index, given that Alice sends the majority bit of the corresponding block of size \\(L\\).</p> <p>As performance reference we implement <code>expected_win_rate_majority(...)</code> to calculate the expected win rate for the block-wise application of the majority algorithm for a given field size, comms size, enemy probability and channel noise. The performance reference holds for the algorithm as implemented, but the proof that this is the optimal algorithm is subject to more stringent conditions as stated in the previous paragraph.</p>"},{"location":"majority_algorithm/#key-references","title":"Key References","text":"<ul> <li> <p>Tavakoli et al., \"Quantum Random Access Codes using Single d-level Systems,\" Phys. Rev. Lett. 114, 170502 (2015) \u2013 Introduced high-level (d-ary) RACs and hypothesized that \u201cmajority-encoding &amp; identity-decoding\u201d is optimal classically. Provides intuition and examples for small \\(n\\) cases. Quote: \"However, intuition strongly suggests that an optimal strategy is for Alice to use majority encoding i.e. Alice counts the number of times each of the values {0, ..., d \u2212 1} appears in her string x and outputs the d-level that occurs most frequently. In case of a tie, she can output either d-level. Bob does identity decoding and thus outputs whatever he receives from Alice.\"</p> </li> <li> <p>Ambainis, Kravchenko &amp; Rai (2015), \"Quantum Advantages in (n, d)7\u21921 Random Access Codes\" arXiv:1510.03045 \u2013 Proof that the majority algorithm indeed achieves the maximum average success probability for classical RACs. Characterizes all optimal strategies and confirms majority-vote is among them. This work settled the conjecture posed by Tavakoli et al. and solidified the majority algorithm as the benchmark for classical RAC performance. </p> </li> </ul>"},{"location":"majority_player_a/","title":"MajorityPlayerA","text":"<p>Role: Player A that encodes segment-wise majority information from a flattened binary field into a deterministic communication vector.</p> <p>Location: <code>Q_Sea_Battle.majority_player_a.MajorityPlayerA</code></p>"},{"location":"majority_player_a/#derived-constraints","title":"Derived constraints","text":"<ul> <li>Let field_size be <code>self.game_layout.field_size</code> (int, not specified), then \\(n2 = \\text{field\\_size}^2\\) (int).</li> <li>Let comms_size be <code>self.game_layout.comms_size</code> (int, not specified), then \\(m = \\text{comms\\_size}\\) (int).</li> <li>Segment length is \\(\\text{segment\\_len} = n2 // m\\) (int); the implementation assumes \\(m\\) divides \\(n2\\) (comment: enforced by <code>GameLayout</code>).</li> </ul>"},{"location":"majority_player_a/#constructor","title":"Constructor","text":"Parameter Type Description game_layout GameLayout, constraints: not specified, shape: N/A Game configuration for this player; stored/used via the <code>PlayerA</code> base class and accessed as <code>self.game_layout</code>. <p>Preconditions</p> <ul> <li><code>game_layout</code> is a <code>GameLayout</code> instance (not validated here).</li> <li>Additional constraints are not specified in this class.</li> </ul> <p>Postconditions</p> <ul> <li>The instance is initialized by delegating to <code>PlayerA.__init__(game_layout)</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified (any exceptions would come from <code>PlayerA.__init__</code> or invalid <code>game_layout</code> usage elsewhere).</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.game_layout import GameLayout\nfrom Q_Sea_Battle.majority_player_a import MajorityPlayerA\n\nlayout = GameLayout(field_size=4, comms_size=4)  # exact signature not specified here\nplayer = MajorityPlayerA(layout)\n</code></pre>"},{"location":"majority_player_a/#public-methods","title":"Public Methods","text":""},{"location":"majority_player_a/#decidefield-suppnone","title":"decide(field, supp=None)","text":"<p>Encode majority statistics in the communication vector.</p> <p>Parameters</p> <ul> <li>field: np.ndarray, dtype: any (converted to int), constraints: interpreted as 0/1 values by documentation but not validated, shape: any (flattened internally to 1D via <code>.ravel()</code>).</li> <li>supp: Optional[Any], constraints: unused, shape: N/A.</li> </ul> <p>Returns</p> <ul> <li>np.ndarray, dtype int, constraints: values in {0, 1}, shape (m,), where \\(m = \\text{comms\\_size}\\).</li> </ul> <p>Behavior</p> <ul> <li>Converts <code>field</code> to <code>flat_field = np.asarray(field, dtype=int).ravel()</code>.</li> <li>Computes \\(n2 = \\text{field\\_size}^2\\) and \\(m = \\text{comms\\_size}\\) from <code>self.game_layout</code>.</li> <li>Sets <code>segment_len = n2 // m</code>.</li> <li>For each segment \\(i \\in [0, m)\\), slices <code>flat_field[start:end]</code> where <code>start = i * segment_len</code> and <code>end = start + segment_len</code>, then sets <code>comm[i] = 1</code> if <code>ones &gt;= zeros</code> else <code>0</code>, with <code>ones = int(segment.sum())</code> and <code>zeros = segment_len - ones</code>.</li> <li>Returns the resulting <code>comm</code>.</li> </ul> <p>Preconditions</p> <ul> <li><code>self.game_layout.field_size</code> and <code>self.game_layout.comms_size</code> exist and are usable as integers.</li> <li>The implementation assumes \\(m\\) divides \\(n2\\).</li> <li><code>field</code> must contain at least <code>n2</code> elements after flattening; otherwise segment slices may be shorter than <code>segment_len</code>, affecting majority computation (not checked).</li> </ul> <p>Postconditions</p> <ul> <li>Returns a newly allocated vector <code>comm</code> of length <code>m</code>.</li> </ul> <p>Errors</p> <ul> <li>May raise exceptions from <code>np.asarray(..., dtype=int)</code> for non-coercible inputs.</li> <li>May raise <code>ZeroDivisionError</code> if <code>m == 0</code> (not prevented here).</li> <li>May raise attribute errors if <code>self.game_layout</code> is missing required attributes.</li> </ul> <p>Example</p> <pre><code>import numpy as np\nfrom Q_Sea_Battle.majority_player_a import MajorityPlayerA\nfrom Q_Sea_Battle.game_layout import GameLayout\n\nlayout = GameLayout(field_size=4, comms_size=4)  # exact signature not specified here\nplayer = MajorityPlayerA(layout)\n\nfield = np.array([\n    1, 0, 1, 0,\n    0, 0, 1, 1,\n    1, 1, 0, 0,\n    0, 1, 0, 1,\n])\ncomm = player.decide(field)\n</code></pre>"},{"location":"majority_player_a/#data-state","title":"Data &amp; State","text":"<ul> <li>Inherited state from <code>PlayerA</code> (not defined in this module).</li> <li>Uses <code>self.game_layout</code> (type: GameLayout, constraints: not specified, shape: N/A) as established by the base class.</li> <li>No additional attributes are defined by <code>MajorityPlayerA</code>.</li> </ul>"},{"location":"majority_player_a/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified (no design notes provided).</li> </ul>"},{"location":"majority_player_a/#deviations","title":"Deviations","text":"<ul> <li>No deviations identified (no design notes provided to compare against code).</li> </ul>"},{"location":"majority_player_a/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>The method <code>decide</code> computes <code>n2</code> from <code>field_size</code> rather than from the actual length of <code>field</code>; if <code>field</code> does not match the expected size, behavior is not validated and may silently produce incorrect segment statistics.</li> <li>Majority tie-breaking is implemented as <code>1</code> when <code>ones &gt;= zeros</code>.</li> </ul>"},{"location":"majority_player_a/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.players_base.PlayerA</code> (base class; behavior not documented here).</li> <li><code>Q_Sea_Battle.game_layout.GameLayout</code> (provides <code>field_size</code> and <code>comms_size</code>).</li> </ul>"},{"location":"majority_player_a/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial implementation (module docstring version).</li> </ul>"},{"location":"majority_player_b/","title":"MajorityPlayerB","text":"<p>Role: Player B implementation that maps a flattened gun index into a field segment and returns the corresponding communication bit as the shoot decision. Location: <code>Q_Sea_Battle.majority_player_b.MajorityPlayerB</code></p>"},{"location":"majority_player_b/#derived-constraints","title":"Derived constraints","text":"<ul> <li>Let \\(n2\\) be the length of the flattened gun vector and \\(m\\) be the length of the communication vector; the implementation assumes \\(m\\) divides \\(n2\\) and uses \\(segment\\_len = n2 / m\\).</li> </ul>"},{"location":"majority_player_b/#constructor","title":"Constructor","text":"Parameter Type Description game_layout GameLayout, constraints: Not specified, shape: Not applicable Game configuration for this player. <p>Preconditions: <code>game_layout</code> must be a valid <code>GameLayout</code> instance accepted by <code>PlayerB.__init__</code> (validation rules not specified in this module). Postconditions: The instance is initialized via <code>PlayerB</code> with the provided <code>game_layout</code>. Errors: Not specified in this module; any exceptions may originate from <code>PlayerB.__init__</code>. Example:</p> <pre><code>from Q_Sea_Battle.game_layout import GameLayout\nfrom Q_Sea_Battle.majority_player_b import MajorityPlayerB\n\nlayout = GameLayout(...)  # Not specified here\nplayer_b = MajorityPlayerB(layout)\n</code></pre>"},{"location":"majority_player_b/#public-methods","title":"Public Methods","text":""},{"location":"majority_player_b/#decide","title":"decide","text":"<p>Decide whether to shoot based on majority information by segmenting the flattened gun index range into \\(m\\) segments and selecting <code>comm[segment_index]</code>.</p> Parameter Type Description gun np.ndarray, dtype: any (coerced to int), constraints: intended one-hot after flattening, shape (n2,) after <code>ravel()</code> Flattened one-hot gun vector of length \\(n2\\). The implementation coerces to <code>int</code> and flattens. comm np.ndarray, dtype: any (coerced to int), constraints: values intended to be decision bits, shape (m,) after <code>ravel()</code> Communication vector from Player A of length \\(m\\). The implementation coerces to <code>int</code> and flattens. supp Optional[Any], constraints: unused, shape: Not applicable Optional supporting information (unused). <p>Returns: int, constraints: nominally in {0,1} (derived from <code>comm</code> values), shape: scalar; <code>1</code> to shoot or <code>0</code> to not shoot.</p> <p>Preconditions: <code>gun</code> and <code>comm</code> must be array-like and convertible to <code>np.ndarray</code>; \\(m = len(comm) &gt; 0\\); \\(n2 = len(gun) \\ge 1\\); \\(m\\) divides \\(n2\\) (assumed by comment and required for equal-length segmentation); <code>gun</code> is assumed to be a valid one-hot vector (stated in comment). Postconditions: Returns <code>int(comm[segment_index])</code> where <code>segment_index</code> is computed from <code>argmax(gun)</code> and <code>segment_len = n2 // m</code>, with <code>segment_index</code> capped to <code>m - 1</code> if it overflows. Errors: May raise exceptions from <code>np.asarray</code>, <code>np.argmax</code>, division by zero when \\(m = 0\\), or indexing errors if <code>comm</code> is empty; no explicit error handling is implemented. Example:</p> <pre><code>import numpy as np\nfrom Q_Sea_Battle.majority_player_b import MajorityPlayerB\nfrom Q_Sea_Battle.game_layout import GameLayout\n\nlayout = GameLayout(...)  # Not specified here\np = MajorityPlayerB(layout)\n\ngun = np.array([0, 0, 1, 0], dtype=int)     # n2 = 4, gun_index = 2\ncomm = np.array([0, 1], dtype=int)          # m = 2, segment_len = 2, segment_index = 1\ndecision = p.decide(gun=gun, comm=comm)     # returns 1\n</code></pre>"},{"location":"majority_player_b/#data-state","title":"Data &amp; State","text":"<ul> <li>Inherits all state from <code>PlayerB</code>; additional attributes are not defined in this class.</li> <li>Uses local variables within <code>decide</code>: <code>flat_gun</code> (np.ndarray, dtype int, shape (n2,)), <code>comm</code> (np.ndarray, dtype int, shape (m,)), <code>n2</code> (int, scalar), <code>m</code> (int, scalar), <code>segment_len</code> (int, scalar), <code>gun_index</code> (int, scalar), <code>segment_index</code> (int, scalar).</li> </ul>"},{"location":"majority_player_b/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"majority_player_b/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"majority_player_b/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li><code>decide</code> assumes the layout constraint that \\(m\\) divides \\(n2\\) but does not validate it; consider adding validation upstream (e.g., in <code>GameLayout</code> or <code>PlayerB</code>) if required.</li> <li><code>decide</code> assumes <code>gun</code> is one-hot and uses <code>np.argmax</code>; for non-one-hot inputs, behavior is defined by <code>argmax</code> and may not match intended semantics.</li> </ul>"},{"location":"majority_player_b/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.players_base.PlayerB</code> (base class; behavior and required interface not specified in this module)</li> <li><code>Q_Sea_Battle.game_layout.GameLayout</code> (layout/config dependency; constraints not specified in this module)</li> </ul>"},{"location":"majority_player_b/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version (module docstring).</li> </ul>"},{"location":"majority_players/","title":"MajorityPlayers","text":"<p>Role: Factory that produces a <code>(MajorityPlayerA, MajorityPlayerB)</code> pair sharing a common <code>GameLayout</code>. Location: <code>Q_Sea_Battle.majority_players.MajorityPlayers</code></p>"},{"location":"majority_players/#constructor","title":"Constructor","text":"Parameter Type Description game_layout GameLayout | None, optional Shared configuration for both players; if <code>None</code>, a default <code>GameLayout</code> is created by the base class. <p>Preconditions</p> <ul> <li>Not specified.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.game_layout</code> is available for subsequent <code>players()</code> calls (exact initialization behavior is delegated to <code>Players.__init__</code>).</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.majority_players import MajorityPlayers\n\nfactory = MajorityPlayers()\nplayer_a, player_b = factory.players()\n</code></pre>"},{"location":"majority_players/#public-methods","title":"Public Methods","text":""},{"location":"majority_players/#players","title":"players","text":"<p>Create a <code>(MajorityPlayerA, MajorityPlayerB)</code> pair that shares the factory's <code>GameLayout</code>.</p> <p>Signature: <code>players(self) -&gt; Tuple[PlayerA, PlayerB]</code></p> <p>Returns</p> <ul> <li>Tuple[PlayerA, PlayerB], length 2, shape (2,): <code>(player_a, player_b)</code> where <code>player_a</code> is a <code>MajorityPlayerA</code> and <code>player_b</code> is a <code>MajorityPlayerB</code>; both are constructed with the same <code>self.game_layout</code>.</li> </ul> <p>Preconditions</p> <ul> <li><code>self.game_layout</code> is set (provided by the <code>Players</code> base class).</li> </ul> <p>Postconditions</p> <ul> <li>Two new player instances are created and returned.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.majority_players import MajorityPlayers\nfrom Q_Sea_Battle.game_layout import GameLayout\n\nlayout = GameLayout()\nfactory = MajorityPlayers(game_layout=layout)\na, b = factory.players()\n</code></pre>"},{"location":"majority_players/#data-state","title":"Data &amp; State","text":"<ul> <li><code>game_layout</code>: GameLayout, constraints unknown, shape N/A; stored on the instance via <code>Players.__init__</code> and passed into constructed players in <code>players()</code>.</li> </ul>"},{"location":"majority_players/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>None specified.</li> </ul>"},{"location":"majority_players/#deviations","title":"Deviations","text":"<ul> <li>None identified.</li> </ul>"},{"location":"majority_players/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li><code>MajorityPlayers</code> delegates <code>game_layout</code> initialization to <code>Players.__init__</code>; update this documentation if base-class behavior changes (e.g., if defaults or validation are added there).</li> <li><code>players()</code> currently always constructs <code>MajorityPlayerA</code> and <code>MajorityPlayerB</code> with <code>self.game_layout</code>; any future support for alternative player types should be reflected here.</li> </ul>"},{"location":"majority_players/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.players_base.Players</code></li> <li><code>Q_Sea_Battle.majority_player_a.MajorityPlayerA</code></li> <li><code>Q_Sea_Battle.majority_player_b.MajorityPlayerB</code></li> <li><code>Q_Sea_Battle.game_layout.GameLayout</code></li> </ul>"},{"location":"majority_players/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version (module docstring).</li> </ul>"},{"location":"neural_net_imitation_utilities/","title":"Q_Sea_Battle.neural_net_imitation_utilities","text":"<p>Role: Utilities to generate synthetic imitation-learning datasets for NeuralNetPlayers Model A (field -&gt; comm) and Model B (comm + gun -&gt; shoot) using a segment-wise majority teacher strategy.</p> <p>Location: <code>Q_Sea_Battle.neural_net_imitation_utilities</code></p>"},{"location":"neural_net_imitation_utilities/#overview","title":"Overview","text":"<p>This module provides helper functions to (1) partition a flattened game field into contiguous segments, (2) compute majority-based communication bits per segment, and (3) generate pandas DataFrames suitable for imitation training of two neural network models: Model A learns communication from fields, and Model B learns a shoot decision from communication plus a one-hot gun position. Data is synthesized by sampling IID Bernoulli fields and using a deterministic majority teacher policy based on the segment partitioning.</p>"},{"location":"neural_net_imitation_utilities/#public-api","title":"Public API","text":""},{"location":"neural_net_imitation_utilities/#functions","title":"Functions","text":""},{"location":"neural_net_imitation_utilities/#make_segmentslayout-gamelayout-listtupleint-int","title":"<code>make_segments(layout: GameLayout) -&gt; List[Tuple[int, int]]</code>","text":"<p>Purpose: Partition the flattened field <code>[0, n2)</code> into <code>m = layout.comms_size</code> contiguous, near-even segments, returned as Python slice-style <code>(start, end)</code> index pairs.</p> <p>Arguments: - <code>layout</code>: <code>GameLayout</code> instance providing <code>field_size</code> and <code>comms_size</code>.</p> <p>Returns: - <code>List[Tuple[int, int]]</code>: List of <code>(start, end)</code> pairs (end exclusive), length <code>layout.comms_size</code>, covering <code>[0, n2)</code> without gaps or overlaps.</p> <p>Errors: - <code>ValueError</code>: If <code>field_size &lt; 1</code>, or <code>comms_size &lt; 1</code>, or <code>comms_size &gt; n2</code>. - <code>RuntimeError</code>: If the constructed segments do not cover the full field (internal safety check).</p> <p>Example: <pre><code>from Q_Sea_Battle.game_layout import GameLayout\nfrom Q_Sea_Battle.neural_net_imitation_utilities import make_segments\n\nlayout = GameLayout(field_size=4, comms_size=3)\nsegments = make_segments(layout)\n# segments is a list of 3 (start, end) pairs covering indices [0, 16)\n</code></pre></p>"},{"location":"neural_net_imitation_utilities/#compute_majority_commfields-npndarray-layout-gamelayout-npndarray","title":"<code>compute_majority_comm(fields: np.ndarray, layout: GameLayout) -&gt; np.ndarray</code>","text":"<p>Purpose: Compute segment-wise majority communication bits for a batch of flattened binary fields using the segmentation from <code>make_segments</code>.</p> <p>Arguments: - <code>fields</code>: NumPy array of shape <code>(N, n2)</code> containing flattened fields with values in <code>{0, 1}</code>. - <code>layout</code>: <code>GameLayout</code> defining <code>field_size</code> and <code>comms_size</code>.</p> <p>Returns: - <code>np.ndarray</code>: Array of shape <code>(N, m)</code> with values in <code>{0.0, 1.0}</code>, dtype <code>float32</code>, where <code>m = layout.comms_size</code>.</p> <p>Errors: - <code>ValueError</code>: If <code>fields</code> is not 2D, or if <code>fields.shape[1] != layout.field_size**2</code>.</p> <p>Example: <pre><code>import numpy as np\nfrom Q_Sea_Battle.game_layout import GameLayout\nfrom Q_Sea_Battle.neural_net_imitation_utilities import compute_majority_comm\n\nlayout = GameLayout(field_size=3, comms_size=3)\nfields = np.array([[0,1,0, 1,1,0, 0,0,1]], dtype=np.float32)  # shape (1, 9)\ncomm = compute_majority_comm(fields, layout)  # shape (1, 3)\n</code></pre></p>"},{"location":"neural_net_imitation_utilities/#generate_majority_dataset_model_alayout-gamelayout-num_samples-int-p_one-float-05-seed-optionalint-none-pddataframe","title":"<code>generate_majority_dataset_model_a(layout: GameLayout, num_samples: int, p_one: float = 0.5, seed: Optional[int] = None) -&gt; pd.DataFrame</code>","text":"<p>Purpose: Generate an imitation-learning dataset for Model A mapping <code>field -&gt; comm</code>, where fields are IID Bernoulli and <code>comm</code> is the segment-wise majority teacher output.</p> <p>Arguments: - <code>layout</code>: <code>GameLayout</code> defining <code>field_size</code> and <code>comms_size</code>. - <code>num_samples</code>: Number of samples to generate (must be positive). - <code>p_one</code>: Bernoulli probability that a field cell equals <code>1</code>. - <code>seed</code>: Optional RNG seed for reproducible sampling.</p> <p>Returns: - <code>pd.DataFrame</code>: DataFrame with at least the following columns (stored as per-row NumPy arrays): - <code>field</code>: 1D <code>np.ndarray</code> of shape <code>(n2,)</code>, dtype <code>float32</code>. - <code>comm</code>: 1D <code>np.ndarray</code> of shape <code>(m,)</code>, dtype <code>float32</code>. - Additional columns may be added in the future (not specified).</p> <p>Errors: - <code>ValueError</code>: If <code>num_samples &lt;= 0</code>. - Any errors raised by <code>compute_majority_comm</code> due to inconsistent <code>layout</code> and sampled field shape (not explicitly rewrapped).</p> <p>Example: <pre><code>from Q_Sea_Battle.game_layout import GameLayout\nfrom Q_Sea_Battle.neural_net_imitation_utilities import generate_majority_dataset_model_a\n\nlayout = GameLayout(field_size=5, comms_size=5)\ndf_a = generate_majority_dataset_model_a(layout, num_samples=1000, p_one=0.4, seed=123)\nx0 = df_a.loc[0, \"field\"]  # np.ndarray shape (25,)\ny0 = df_a.loc[0, \"comm\"]   # np.ndarray shape (5,)\n</code></pre></p>"},{"location":"neural_net_imitation_utilities/#generate_majority_dataset_model_blayout-gamelayout-num_samples-int-p_one-float-05-seed-optionalint-none-pddataframe","title":"<code>generate_majority_dataset_model_b(layout: GameLayout, num_samples: int, p_one: float = 0.5, seed: Optional[int] = None) -&gt; pd.DataFrame</code>","text":"<p>Purpose: Generate an imitation-learning dataset for Model B mapping <code>(comm + gun) -&gt; shoot</code>, where <code>gun</code> is a one-hot cell index and <code>shoot</code> is the majority comm bit for the segment containing the gun index.</p> <p>Arguments: - <code>layout</code>: <code>GameLayout</code> defining <code>field_size</code> and <code>comms_size</code>. - <code>num_samples</code>: Number of samples to generate (must be positive). - <code>p_one</code>: Bernoulli probability that a field cell equals <code>1</code>. - <code>seed</code>: Optional RNG seed for reproducible sampling.</p> <p>Returns: - <code>pd.DataFrame</code>: DataFrame with at least the following columns: - <code>field</code>: 1D <code>np.ndarray</code> of shape <code>(n2,)</code>, dtype <code>float32</code>. - <code>comm</code>: 1D <code>np.ndarray</code> of shape <code>(m,)</code>, dtype <code>float32</code>. - <code>gun</code>: 1D <code>np.ndarray</code> of shape <code>(n2,)</code>, one-hot, dtype <code>float32</code>. - <code>shoot</code>: scalar <code>np.float32</code> in <code>{0.0, 1.0}</code>. - The docstring states this schema matches what <code>NeuralNetPlayers.train_model_b</code> expects (the referenced symbol is not defined in this module).</p> <p>Errors: - <code>ValueError</code>: If <code>num_samples &lt;= 0</code>. - Any errors raised by <code>make_segments</code> or <code>compute_majority_comm</code> for invalid layout or shape (not explicitly rewrapped).</p> <p>Example: <pre><code>from Q_Sea_Battle.game_layout import GameLayout\nfrom Q_Sea_Battle.neural_net_imitation_utilities import generate_majority_dataset_model_b\n\nlayout = GameLayout(field_size=4, comms_size=4)\ndf_b = generate_majority_dataset_model_b(layout, num_samples=500, p_one=0.5, seed=7)\ngun0 = df_b.loc[0, \"gun\"]      # one-hot np.ndarray shape (16,)\nshoot0 = df_b.loc[0, \"shoot\"]  # np.float32 scalar\n</code></pre></p>"},{"location":"neural_net_imitation_utilities/#generate_majority_imitation_datasetslayout-gamelayout-num_samples_a-int-num_samples_b-int-p_one-float-05-seed-optionalint-none-tuplepddataframe-pddataframe","title":"<code>generate_majority_imitation_datasets(layout: GameLayout, num_samples_a: int, num_samples_b: int, p_one: float = 0.5, seed: Optional[int] = None) -&gt; Tuple[pd.DataFrame, pd.DataFrame]</code>","text":"<p>Purpose: Convenience wrapper to generate paired datasets for Model A and Model B, using derived seeds to make draws reproducible but distinct.</p> <p>Arguments: - <code>layout</code>: <code>GameLayout</code> defining <code>field_size</code> and <code>comms_size</code>. - <code>num_samples_a</code>: Number of samples for the Model A dataset. - <code>num_samples_b</code>: Number of samples for the Model B dataset. - <code>p_one</code>: Bernoulli probability that a field cell equals <code>1</code>. - <code>seed</code>: Optional RNG seed; when provided, dataset A uses <code>seed</code> and dataset B uses <code>seed + 1</code>.</p> <p>Returns: - <code>Tuple[pd.DataFrame, pd.DataFrame]</code>: <code>(dataset_a, dataset_b)</code> where each DataFrame matches the return schema of <code>generate_majority_dataset_model_a</code> and <code>generate_majority_dataset_model_b</code>, respectively.</p> <p>Errors: - Propagates <code>ValueError</code> from underlying generators if <code>num_samples_a &lt;= 0</code> or <code>num_samples_b &lt;= 0</code>. - Propagates any errors from segment construction or majority computation due to invalid layout.</p> <p>Example: <pre><code>from Q_Sea_Battle.game_layout import GameLayout\nfrom Q_Sea_Battle.neural_net_imitation_utilities import generate_majority_imitation_datasets\n\nlayout = GameLayout(field_size=6, comms_size=9)\ndf_a, df_b = generate_majority_imitation_datasets(layout, num_samples_a=2000, num_samples_b=2000, p_one=0.3, seed=42)\n</code></pre></p>"},{"location":"neural_net_imitation_utilities/#constants","title":"Constants","text":"<p>Not specified.</p>"},{"location":"neural_net_imitation_utilities/#types","title":"Types","text":"<p>Not specified.</p>"},{"location":"neural_net_imitation_utilities/#dependencies","title":"Dependencies","text":"<ul> <li><code>numpy</code> (imported as <code>np</code>): used for random sampling, array operations, and float32 conversions.</li> <li><code>pandas</code> (imported as <code>pd</code>): used to construct DataFrame datasets.</li> <li><code>typing</code>: <code>List</code>, <code>Tuple</code>, <code>Optional</code> used for type annotations.</li> <li><code>Q_Sea_Battle.game_layout.GameLayout</code>: required for <code>field_size</code> and <code>comms_size</code>.</li> </ul>"},{"location":"neural_net_imitation_utilities/#planned-design-spec","title":"Planned (design-spec)","text":"<p>Unknown (no design notes provided).</p>"},{"location":"neural_net_imitation_utilities/#deviations","title":"Deviations","text":"<p>Unknown (no external spec provided to compare against).</p>"},{"location":"neural_net_imitation_utilities/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>Keep <code>make_segments()</code> as the single source of truth for segment definitions; any changes must be reflected consistently in <code>compute_majority_comm()</code> and dataset generators.</li> <li>Dataset generators store NumPy arrays per row (object dtype columns in pandas); if changing storage format (e.g., expanding into multiple numeric columns), update downstream training code accordingly (not in this module).</li> <li><code>compute_majority_comm()</code> treats ties as majority (<code>count &gt;= L/2</code> yields <code>1.0</code>); changing this threshold will change teacher behavior and should be considered a breaking change.</li> </ul>"},{"location":"neural_net_imitation_utilities/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.game_layout.GameLayout</code></li> <li>NeuralNetPlayers Model A and Model B training routines (referenced in module docstring and comments; not defined here)</li> </ul>"},{"location":"neural_net_imitation_utilities/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version (as indicated in module docstring).</li> </ul>"},{"location":"neural_net_player_a/","title":"NeuralNetPlayerA","text":"<p>Role: Player A implementation that uses a Keras model to produce a communication bit-vector from a scaled binary field, optionally sampling actions for exploration and tracking the last action log-probability.</p> <p>Location: <code>Q_Sea_Battle.neural_net_player_a.NeuralNetPlayerA</code></p>"},{"location":"neural_net_player_a/#derived-constraints","title":"Derived constraints","text":"<ul> <li>Define \\(n2\\) as the flattened field size and \\(m\\) as the communication vector size.</li> <li><code>decide()</code> reshapes <code>field</code> to <code>(1, n2)</code> and expects <code>model_a</code> to return logits broadcastable to shape <code>(m,)</code> after indexing <code>[0]</code>.</li> <li><code>decide()</code> returns integer bits in <code>{0, 1}</code> with shape <code>(m,)</code> and stores <code>last_logprob</code> as a Python <code>float</code> equal to the sum of per-bit log-probabilities.</li> </ul>"},{"location":"neural_net_player_a/#constructor","title":"Constructor","text":"Parameter Type Description game_layout GameLayout, constraints: not specified, shape: N/A Shared <code>GameLayout</code> describing the environment; passed to <code>PlayerA</code> base constructor. model_a tf.keras.Model, constraints: callable on <code>tf.Tensor</code> or array-like input, shape: input <code>(1, n2)</code> and output convertible to <code>np.ndarray</code> with first dimension size <code>1</code> and remaining shape <code>(m,)</code> Keras model mapping the scaled flattened field to communication logits. explore bool, constraints: {True, False}, shape: scalar If <code>True</code>, sample communication bits from Bernoulli probabilities; if <code>False</code>, act greedily by thresholding at <code>0.5</code>. <p>Preconditions</p> <ul> <li><code>model_a(field_scaled, training=False)</code> must return a tensor/array that supports <code>.numpy()</code> and yields an array with shape <code>(1, m)</code> such that indexing <code>[0]</code> produces shape <code>(m,)</code>.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.model_a</code> is set to <code>model_a</code>.</li> <li><code>self.explore</code> is set to <code>explore</code>.</li> <li><code>self.last_logprob</code> is set to <code>None</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>import numpy as np\nimport tensorflow as tf\n\nfrom Q_Sea_Battle.game_layout import GameLayout\nfrom Q_Sea_Battle.neural_net_player_a import NeuralNetPlayerA\n\ngame_layout = GameLayout(...)  # Not specified\nmodel_a = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(game_layout.field_size * game_layout.field_size,)),\n    tf.keras.layers.Dense(game_layout.comms_size),\n])\n\nplayer = NeuralNetPlayerA(game_layout=game_layout, model_a=model_a, explore=False)\nfield = np.zeros((game_layout.field_size * game_layout.field_size,), dtype=int)\naction = player.decide(field)\nlogp = player.get_log_prob()\n</code></pre>"},{"location":"neural_net_player_a/#public-methods","title":"Public Methods","text":""},{"location":"neural_net_player_a/#decide","title":"decide","text":"<ul> <li>Signature: <code>decide(self, field: np.ndarray, supp: Any | None = None) -&gt; np.ndarray</code></li> </ul> <p>Parameter | Type | Description - <code>field</code> | np.ndarray, dtype float32 (after conversion), constraints: values intended in {0,1}, shape (n2,) | Flattened field array; converted via <code>np.asarray(..., dtype=np.float32).reshape(1, -1)</code> and then scaled by subtracting <code>0.5</code>. - <code>supp</code> | Any | None, constraints: unused, shape: N/A | Optional supporting information; not used.</p> <p>Returns</p> <ul> <li>np.ndarray, dtype int, constraints: elements in {0,1}, shape (m,) | Communication action bits computed either greedily (<code>probs &gt;= 0.5</code>) or by sampling (<code>rnd &lt; probs</code>).</li> </ul> <p>Preconditions</p> <ul> <li><code>field</code> must be reshapeable to <code>(1, n2)</code> via <code>.reshape(1, -1)</code>.</li> <li><code>self.model_a</code> must accept <code>field_scaled</code> and produce logits compatible with <code>logit_to_probs</code>.</li> <li><code>logit_to_prob()</code> and <code>logit_to_logprob()</code> must accept the produced logits and actions and return broadcast-compatible outputs.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.last_logprob</code> is set to <code>float(np.sum(log_probs_bits))</code>, where <code>log_probs_bits = self.logit_to_log_probs(logits, actions)</code>.</li> </ul> <p>Errors</p> <ul> <li>Any exceptions raised by NumPy reshaping/conversion, the Keras model call, <code>.numpy()</code>, or the utility functions are not caught.</li> </ul> <p>Example</p> <pre><code>field = np.random.randint(0, 2, size=(n2,), dtype=int)\naction_bits = player.decide(field)\n</code></pre>"},{"location":"neural_net_player_a/#logit_to_probs","title":"logit_to_probs","text":"<ul> <li>Signature: <code>logit_to_probs(logits: np.ndarray | float) -&gt; np.ndarray | float</code></li> </ul> <p>Parameter | Type | Description - <code>logits</code> | np.ndarray | float, constraints: not specified, shape: any | Logits to convert to probabilities; passed through to <code>Q_Sea_Battle.logit_utilities.logit_to_prob</code>.</p> <p>Returns</p> <ul> <li>np.ndarray | float, constraints: not specified, shape: same as input/broadcast behavior of <code>logit_to_prob</code> | Probabilities corresponding to <code>logits</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified (delegated to <code>logit_to_prob</code>).</li> </ul> <p>Example</p> <pre><code>probs = NeuralNetPlayerA.logit_to_probs(np.array([0.0, 1.0], dtype=np.float32))\n</code></pre>"},{"location":"neural_net_player_a/#logit_to_log_probs","title":"logit_to_log_probs","text":"<ul> <li>Signature: <code>logit_to_log_probs(logits: np.ndarray | float, actions: np.ndarray | float) -&gt; np.ndarray | float</code></li> </ul> <p>Parameter | Type | Description - <code>logits</code> | np.ndarray | float, constraints: not specified, shape: any | Scalar or array of logits. - <code>actions</code> | np.ndarray | float, constraints: intended in {0,1}, shape: any | Scalar or array of actions; broadcast with <code>logits</code>.</p> <p>Returns</p> <ul> <li>np.ndarray | float, constraints: not specified, shape: broadcast(logits, actions) | Log-probabilities corresponding to selecting <code>actions</code> under Bernoulli probabilities induced by <code>logits</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified (delegated to <code>logit_to_logprob</code>).</li> </ul> <p>Example</p> <pre><code>logp_bits = NeuralNetPlayerA.logit_to_log_probs(np.array([0.0, 1.0]), np.array([0.0, 1.0]))\n</code></pre>"},{"location":"neural_net_player_a/#get_log_prob","title":"get_log_prob","text":"<ul> <li>Signature: <code>get_log_prob(self) -&gt; float</code></li> </ul> <p>Parameters</p> <ul> <li>None.</li> </ul> <p>Returns</p> <ul> <li>float, constraints: finite not specified, shape: scalar | The stored log-probability of the last action returned by <code>decide()</code>.</li> </ul> <p>Errors</p> <ul> <li><code>RuntimeError</code>: Raised if <code>self.last_logprob is None</code> (i.e., no decision since construction or last <code>reset()</code>).</li> </ul> <p>Example</p> <pre><code>player.decide(field)\nlogp = player.get_log_prob()\n</code></pre>"},{"location":"neural_net_player_a/#reset","title":"reset","text":"<ul> <li>Signature: <code>reset(self) -&gt; None</code></li> </ul> <p>Parameters</p> <ul> <li>None.</li> </ul> <p>Returns</p> <ul> <li>None.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.last_logprob</code> is set to <code>None</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>player.reset()\n</code></pre>"},{"location":"neural_net_player_a/#data-state","title":"Data &amp; State","text":"<ul> <li><code>model_a</code>: tf.keras.Model, constraints: callable as used by <code>decide()</code>, shape: N/A | The Keras model used to produce logits from scaled fields.</li> <li><code>explore</code>: bool, constraints: {True, False}, shape: scalar | Controls whether actions are sampled or thresholded.</li> <li><code>last_logprob</code>: float | None, constraints: None or scalar float, shape: scalar | Sum of per-bit log-probabilities of the last action chosen by <code>decide()</code>; cleared by <code>reset()</code>.</li> </ul>"},{"location":"neural_net_player_a/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"neural_net_player_a/#deviations","title":"Deviations","text":"<ul> <li>The class docstring states delegation is to <code>Q_Sea_Battle.logit_utils</code>, but the module imports from <code>.logit_utilities</code> and delegates to <code>Q_Sea_Battle.logit_utilities.logit_to_prob</code> and <code>Q_Sea_Battle.logit_utilities.logit_to_logprob</code>.</li> </ul>"},{"location":"neural_net_player_a/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li><code>decide()</code> converts actions to <code>np.float32</code> during sampling/thresholding and returns <code>actions.astype(int)</code>; keep this in mind if downstream code expects specific dtypes.</li> <li><code>_scale_field()</code> is a module-level helper (non-public) that applies the affine transform <code>x_scaled = x - 0.5</code> after casting to <code>np.float32</code>; changes to scaling will affect model input distribution.</li> </ul>"},{"location":"neural_net_player_a/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.players_base.PlayerA</code></li> <li><code>Q_Sea_Battle.game_layout.GameLayout</code></li> <li><code>Q_Sea_Battle.logit_utilities.logit_to_prob</code></li> <li><code>Q_Sea_Battle.logit_utilities.logit_to_logprob</code></li> </ul>"},{"location":"neural_net_player_a/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version in module docstring; provides <code>NeuralNetPlayerA</code> with exploration option and log-prob tracking.</li> </ul>"},{"location":"neural_net_player_b/","title":"NeuralNetPlayerB","text":"<p>Role: Player B implementation driven by a Keras model that maps a compact state representation (normalised gun index + comm bits) to a shoot decision and stores the last action log-probability. Location: <code>Q_Sea_Battle.neural_net_player_b.NeuralNetPlayerB</code></p>"},{"location":"neural_net_player_b/#constructor","title":"Constructor","text":"Parameter Type Description game_layout GameLayout, constraints Not specified, shape Not applicable Shared <code>GameLayout</code> describing the environment; passed to <code>PlayerB</code> base class. model_b tf.keras.Model, constraints callable as <code>model_b(x, training=False)</code> and returns a single logit per sample, shape Not specified Keras model mapping input vectors of shape \\((1 + m,)\\) (normalised gun index + comm bits) to a single shoot logit. explore bool, constraints {True, False}, shape scalar If <code>True</code>, sample shoot actions from the Bernoulli distribution defined by the predicted probability; if <code>False</code>, act greedily using a <code>0.5</code> probability threshold. <p>Preconditions: <code>game_layout</code> is a valid <code>GameLayout</code> instance; <code>model_b</code> accepts NumPy inputs compatible with the concatenated feature vector <code>x</code> and returns a scalar logit for the single sample. Postconditions: <code>self.model_b</code> is set; <code>self.explore</code> is set; <code>self.last_logprob</code> is initialised to <code>None</code>. Errors: Not specified. Example:</p> <pre><code>import numpy as np\nimport tensorflow as tf\n\nplayer_b = NeuralNetPlayerB(game_layout=game_layout, model_b=model_b, explore=True)\ngun = np.zeros((n2,), dtype=np.float32)\ngun[0] = 1.0\ncomm = np.zeros((m,), dtype=np.float32)\naction = player_b.decide(gun=gun, comm=comm)\nlogp = player_b.get_log_prob()\nplayer_b.reset()\n</code></pre>"},{"location":"neural_net_player_b/#public-methods","title":"Public Methods","text":""},{"location":"neural_net_player_b/#decidegun-comm-suppnone","title":"decide(gun, comm, supp=None)","text":"<p>Decide whether to shoot based on the gun vector and communication bits.</p> Parameter Type Description gun np.ndarray, dtype float32 (after conversion), constraints flattened one-hot intended but argmax fallback if not strictly one-hot, shape (n2,) Flattened gun vector of length <code>n2</code>; internally converted to a normalised scalar index in \\([0, 1]\\). comm np.ndarray, dtype float32 (after conversion), constraints Not specified, shape (m,) Communication vector from Player A of length <code>m</code>. supp Any | None, constraints unused, shape Not applicable Optional supporting information; not used by this implementation. <p>Returns: int, constraints {0,1}, shape scalar; <code>1</code> to shoot, <code>0</code> to not shoot.  </p> <p>Preconditions: <code>gun</code> and <code>comm</code> are array-like and can be reshaped to <code>(1, -1)</code>; <code>model_b(x, training=False)</code> is valid for <code>x</code> of shape <code>(1, 1 + m)</code> and yields a value convertible to a scalar logit. Postconditions: <code>self.last_logprob</code> is updated to the log-probability of the chosen action under the model\u2019s Bernoulli distribution; returns the chosen action as <code>int</code>. Errors: Not specified.</p>"},{"location":"neural_net_player_b/#logit_to_probslogits","title":"logit_to_probs(logits)","text":"<p>Backward-compatible wrapper around <code>logit_to_prob</code>.</p> Parameter Type Description logits np.ndarray | float, constraints Not specified, shape scalar or broadcastable array Logit(s) to convert to probability/probabilities. <p>Returns: np.ndarray | float, constraints range \\([0, 1]\\), shape matches input; the probability/probabilities derived from <code>logits</code>.  </p> <p>Preconditions: Not specified. Postconditions: Not specified. Errors: Not specified.</p>"},{"location":"neural_net_player_b/#logit_to_log_probslogits-actions","title":"logit_to_log_probs(logits, actions)","text":"<p>Backward-compatible wrapper around <code>logit_to_logprob</code>.</p> Parameter Type Description logits np.ndarray | float, constraints Not specified, shape scalar or broadcastable array Logit(s) used to compute log-probabilities. actions np.ndarray | float, constraints Not specified, shape scalar or broadcastable array Action(s) associated with the log-probability computation. <p>Returns: np.ndarray | float, constraints Not specified, shape broadcasted from inputs; log-probability/log-probabilities for <code>actions</code> under the Bernoulli distribution parameterised by <code>logits</code>.  </p> <p>Preconditions: Not specified. Postconditions: Not specified. Errors: Not specified.</p>"},{"location":"neural_net_player_b/#get_log_prob","title":"get_log_prob()","text":"<p>Return the log-probability of the last chosen action.</p> <p>Returns: float, constraints finite scalar expected, shape scalar; log-probability of the last action.  </p> <p>Preconditions: <code>decide()</code> has been called since the last <code>reset()</code> such that <code>self.last_logprob</code> is not <code>None</code>. Postconditions: Does not modify state. Errors: Raises <code>RuntimeError</code> if <code>self.last_logprob</code> is <code>None</code>.</p>"},{"location":"neural_net_player_b/#reset","title":"reset()","text":"<p>Reset internal state (stored log-probability).</p> <p>Returns: None, constraints Not applicable, shape Not applicable.  </p> <p>Preconditions: Not specified. Postconditions: <code>self.last_logprob</code> is set to <code>None</code>. Errors: Not specified.</p>"},{"location":"neural_net_player_b/#data-state","title":"Data &amp; State","text":"<ul> <li><code>model_b</code>: tf.keras.Model, constraints callable as <code>model_b(x, training=False)</code>, shape Not specified; the Keras model used to produce a shoot logit from the concatenated feature vector.</li> <li><code>explore</code>: bool, constraints {True, False}, shape scalar; controls stochastic sampling (<code>True</code>) vs greedy thresholding (<code>False</code>).</li> <li><code>last_logprob</code>: float | None, constraints <code>None</code> before first decision or after reset, shape scalar; log-probability of the most recent action chosen by <code>decide()</code>.</li> </ul>"},{"location":"neural_net_player_b/#planned-design-spec","title":"Planned (design-spec)","text":"<p>Not specified.</p>"},{"location":"neural_net_player_b/#deviations","title":"Deviations","text":"<p>Not specified.</p>"},{"location":"neural_net_player_b/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>The public <code>gun</code> interface is a flattened vector of length <code>n2</code>, but internally it is compressed via argmax into a single normalised index in \\([0, 1]\\) using \\(idx / \\max(1, n2 - 1)\\); non-strict one-hot inputs are handled by argmax fallback.</li> <li><code>decide()</code> stores the log-probability of the selected action in <code>last_logprob</code>; callers relying on <code>get_log_prob()</code> must call <code>decide()</code> first and must handle the <code>RuntimeError</code> case after <code>reset()</code>.</li> </ul>"},{"location":"neural_net_player_b/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.players_base.PlayerB</code></li> <li><code>Q_Sea_Battle.game_layout.GameLayout</code></li> <li><code>Q_Sea_Battle.logit_utilities.logit_to_prob</code></li> <li><code>Q_Sea_Battle.logit_utilities.logit_to_logprob</code></li> </ul>"},{"location":"neural_net_player_b/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial implementation of <code>NeuralNetPlayerB</code> with Keras-based decision-making and stored last-action log-probability.</li> </ul>"},{"location":"neural_net_players/","title":"NeuralNetPlayers","text":"<p>Role: Factory and manager for a shared pair of neural-network-based Sea Battle players (Player A for communication, Player B for shooting), including model persistence and separate training entry points.</p> <p>Location: <code>Q_Sea_Battle.neural_net_players.NeuralNetPlayers</code></p>"},{"location":"neural_net_players/#constructor","title":"Constructor","text":"Parameter Type Description game_layout GameLayout or None, default None, constraint: if None then a default <code>GameLayout()</code> is constructed Game configuration used to derive dimensions such as field_size and comms_size. model_a tf.keras.Model or None, default None Optional pre-constructed communication model for Player A; if None, built lazily on first use. model_b tf.keras.Model or None, default None Optional pre-constructed shoot model for Player B; if None, built lazily on first use. explore bool, default False Exploration flag propagated to created child players; True enables stochastic behavior, False deterministic thresholding. <p>Preconditions</p> <ul> <li><code>game_layout</code> is either None or a <code>GameLayout</code> instance.</li> <li>If provided, <code>model_a</code> and <code>model_b</code> are compatible with the input/output shapes implied by the current <code>game_layout</code> (compatibility is not validated here).</li> </ul> <p>Postconditions</p> <ul> <li><code>self.game_layout: GameLayout</code> is set (constructed if <code>game_layout is None</code>).</li> <li><code>self.model_a: tf.keras.Model or None</code> and <code>self.model_b: tf.keras.Model or None</code> are stored without modification.</li> <li>Internal cached players <code>self._playerA</code> and <code>self._playerB</code> are initialized to None.</li> <li><code>self.explore: bool</code> is set.</li> </ul> <p>Errors</p> <ul> <li>Not specified; downstream errors may be raised by <code>GameLayout()</code> construction or by incompatible model usage later.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.neural_net_players import NeuralNetPlayers\n\nfactory = NeuralNetPlayers(explore=True)\nplayer_a, player_b = factory.players()\n</code></pre>"},{"location":"neural_net_players/#public-methods","title":"Public Methods","text":""},{"location":"neural_net_players/#players","title":"players()","text":"<p>Create or return a neural Player A/B pair, lazily constructing default models and caching created players.</p> <p>Returns</p> <ul> <li>Tuple[PlayerA, PlayerB], shape (2,), constraints: <code>(player_a, player_b)</code> where instances are cached and reused across calls.</li> </ul> <p>Side effects</p> <ul> <li>If <code>self.model_a is None</code>, assigns <code>self.model_a = self._build_model_a()</code>.</li> <li>If <code>self.model_b is None</code>, assigns <code>self.model_b = self._build_model_b()</code>.</li> <li>If cached players are missing, constructs <code>NeuralNetPlayerA</code> and/or <code>NeuralNetPlayerB</code> with <code>game_layout=self.game_layout</code>, the relevant model, and <code>explore=self.explore</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified; may raise errors from TensorFlow/Keras model construction or player constructors.</li> </ul> <p>Example</p> <pre><code>player_a, player_b = factory.players()\n</code></pre>"},{"location":"neural_net_players/#reset","title":"reset()","text":"<p>Reset internal state of the neural players (clears per-game log-probabilities) without modifying the underlying Keras models.</p> <p>Returns</p> <ul> <li>None</li> </ul> <p>Side effects</p> <ul> <li>Calls <code>self._playerA.reset()</code> if <code>self._playerA is not None</code>.</li> <li>Calls <code>self._playerB.reset()</code> if <code>self._playerB is not None</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>factory.reset()\n</code></pre>"},{"location":"neural_net_players/#set_exploreflag","title":"set_explore(flag)","text":"<p>Set exploration behavior for both cached players (if they exist) and update the factory flag used for newly created players.</p> <p>Parameters</p> <ul> <li>flag: bool, constraints: {True, False}</li> </ul> <p>Returns</p> <ul> <li>None</li> </ul> <p>Side effects</p> <ul> <li>Sets <code>self.explore = flag</code>.</li> <li>If cached players exist, sets <code>self._playerA.explore = flag</code> and <code>self._playerB.explore = flag</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>factory.set_explore(False)\n</code></pre>"},{"location":"neural_net_players/#store_modelsfilenamea-filenameb","title":"store_models(filenameA, filenameB)","text":"<p>Store the underlying Keras models to disk, lazily building them first if needed.</p> <p>Parameters</p> <ul> <li>filenameA: str, constraints: path-like string accepted by <code>tf.keras.Model.save</code></li> <li>filenameB: str, constraints: path-like string accepted by <code>tf.keras.Model.save</code></li> </ul> <p>Returns</p> <ul> <li>None</li> </ul> <p>Side effects</p> <ul> <li>Ensures <code>self.model_a</code> and <code>self.model_b</code> are non-None by building defaults if needed.</li> <li>Calls <code>self.model_a.save(filenameA)</code> and <code>self.model_b.save(filenameB)</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified; may raise I/O errors or TensorFlow/Keras serialization errors.</li> </ul> <p>Example</p> <pre><code>factory.store_models(\"model_a.keras\", \"model_b.keras\")\n</code></pre>"},{"location":"neural_net_players/#load_modelsfilenamea-filenameb","title":"load_models(filenameA, filenameB)","text":"<p>Load Keras models from disk, attach them to this factory, and update existing cached players to reference the loaded models.</p> <p>Parameters</p> <ul> <li>filenameA: str, constraints: path-like string accepted by <code>tf.keras.models.load_model</code></li> <li>filenameB: str, constraints: path-like string accepted by <code>tf.keras.models.load_model</code></li> </ul> <p>Returns</p> <ul> <li>None</li> </ul> <p>Side effects</p> <ul> <li>Sets <code>self.model_a = tf.keras.models.load_model(filenameA)</code>.</li> <li>Sets <code>self.model_b = tf.keras.models.load_model(filenameB)</code>.</li> <li>If cached players exist, updates <code>self._playerA.model_a</code> and/or <code>self._playerB.model_b</code> to point to the newly loaded models.</li> </ul> <p>Errors</p> <ul> <li>Not specified; may raise I/O errors or TensorFlow/Keras deserialization errors.</li> </ul> <p>Example</p> <pre><code>factory.load_models(\"model_a.keras\", \"model_b.keras\")\n</code></pre>"},{"location":"neural_net_players/#traindataset-training_settings","title":"train(dataset, training_settings)","text":"<p>Legacy training API; currently a no-op that emits a deprecation warning.</p> <p>Parameters</p> <ul> <li>dataset: Unknown, constraints: Not specified</li> <li>training_settings: Unknown, constraints: Not specified</li> </ul> <p>Returns</p> <ul> <li>None, constraints: always returns without training</li> </ul> <p>Side effects</p> <ul> <li>Emits <code>UserWarning</code> via <code>warnings.warn(...)</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>factory.train(dataset, training_settings)\n</code></pre>"},{"location":"neural_net_players/#train_model_adataset-training_settings","title":"train_model_a(dataset, training_settings)","text":"<p>Train the communication model (model_a) on a dataset.</p> <p>Parameters</p> <ul> <li>dataset: pandas.DataFrame, constraints: must contain column <code>\"field\"</code> with array-like per-row entries and column <code>\"comm\"</code> with array-like per-row entries; optional column <code>\"sample_weight\"</code> if enabled; shapes: <code>\"field\"</code> entries reshape to (n2,) and <code>\"comm\"</code> entries reshape to (m,)</li> <li>training_settings: dict-like, constraints: supports keys <code>\"use_sample_weight\"</code> (bool-like), <code>\"epochs\"</code> (int-like), <code>\"batch_size\"</code> (int-like), <code>\"learning_rate\"</code> (float-like), <code>\"verbose\"</code> (int-like)</li> </ul> <p>Returns</p> <ul> <li>None</li> </ul> <p>Behavior</p> <ul> <li>Defines \\(n2 = \\mathrm{field\\_size}^2\\) and \\(m = \\mathrm{comms\\_size}\\) from <code>self.game_layout</code>.</li> <li>Builds <code>self.model_a</code> via <code>_build_model_a()</code> if it is None.</li> <li>Constructs training inputs: <code>fields_scaled</code> is produced by stacking <code>dataset[\"field\"]</code>, reshaping to <code>(-1, n2)</code>, converting to float32, and applying <code>_scale_field(...)</code>.</li> <li>Constructs targets: <code>comms_teacher</code> by stacking <code>dataset[\"comm\"]</code>, reshaping to <code>(-1, m)</code>, converting to float32.</li> <li>Optionally uses <code>sample_weight</code> if <code>training_settings[\"use_sample_weight\"]</code> is truthy and <code>\"sample_weight\"</code> exists in <code>dataset.columns</code>.</li> <li>Compiles <code>self.model_a</code> with <code>Adam(learning_rate)</code>, <code>BinaryCrossentropy(from_logits=True)</code>, and metric <code>\"accuracy\"</code>.</li> <li>Calls <code>.fit(...)</code> with <code>epochs</code>, <code>batch_size</code>, and <code>verbose</code> from <code>training_settings</code> (defaults: 3, 32, 1e-3, 0).</li> </ul> <p>Errors</p> <ul> <li>Not specified; may raise KeyError for missing required columns, NumPy reshape/stack errors for inconsistent shapes, and TensorFlow/Keras training errors.</li> </ul> <p>Example</p> <pre><code>training_settings = {\"epochs\": 5, \"batch_size\": 64, \"learning_rate\": 1e-3, \"verbose\": 1}\nfactory.train_model_a(dataset=df, training_settings=training_settings)\n</code></pre>"},{"location":"neural_net_players/#train_model_bdataset-training_settings","title":"train_model_b(dataset, training_settings)","text":"<p>Train the shoot model (model_b) on a dataset.</p> <p>Parameters</p> <ul> <li>dataset: pandas.DataFrame, constraints: must contain column <code>\"gun\"</code> with array-like per-row entries, column <code>\"comm\"</code> with array-like per-row entries, and column <code>\"shoot\"</code> with scalar/array-like per-row entries; optional column <code>\"sample_weight\"</code> if enabled; shapes: <code>\"gun\"</code> entries reshape to (n2,), <code>\"comm\"</code> entries reshape to (m,), <code>\"shoot\"</code> reshapes to (1,)</li> <li>training_settings: dict-like, constraints: supports keys <code>\"use_sample_weight\"</code> (bool-like), <code>\"epochs\"</code> (int-like), <code>\"batch_size\"</code> (int-like), <code>\"learning_rate\"</code> (float-like), <code>\"verbose\"</code> (int-like)</li> </ul> <p>Returns</p> <ul> <li>None</li> </ul> <p>Behavior</p> <ul> <li>Defines \\(n2 = \\mathrm{field\\_size}^2\\) and \\(m = \\mathrm{comms\\_size}\\) from <code>self.game_layout</code>.</li> <li>Builds <code>self.model_b</code> via <code>_build_model_b()</code> if it is None.</li> <li>Constructs gun index feature: stacks <code>dataset[\"gun\"]</code>, reshapes to <code>(-1, n2)</code>, converts to float32, then calls <code>_gun_one_hot_to_index(guns)</code> to obtain <code>gun_idx_norm</code>, documented in code as shape <code>(N, 1)</code>.</li> <li>Constructs communication feature: stacks <code>dataset[\"comm\"]</code>, reshapes to <code>(-1, m)</code>, converts to float32.</li> <li>Concatenates features: <code>x = np.concatenate([gun_idx_norm, comms], axis=1)</code>, yielding shape <code>(N, 1 + m)</code>.</li> <li>Constructs targets: <code>shoots = dataset[\"shoot\"].to_numpy().astype(\"float32\").reshape((-1, 1))</code>.</li> <li>Optionally uses <code>sample_weight</code> if <code>training_settings[\"use_sample_weight\"]</code> is truthy and <code>\"sample_weight\"</code> exists in <code>dataset.columns</code>.</li> <li>Compiles <code>self.model_b</code> with <code>Adam(learning_rate)</code>, <code>BinaryCrossentropy(from_logits=True)</code>, and metric <code>\"accuracy\"</code>.</li> <li>Calls <code>.fit(...)</code> with <code>epochs</code>, <code>batch_size</code>, and <code>verbose</code> from <code>training_settings</code> (defaults: 3, 32, 1e-3, 0).</li> </ul> <p>Errors</p> <ul> <li>Not specified; may raise KeyError for missing required columns, NumPy shape/concat errors, and TensorFlow/Keras training errors.</li> </ul> <p>Example</p> <pre><code>training_settings = {\"epochs\": 3, \"batch_size\": 32, \"learning_rate\": 1e-3, \"verbose\": 0}\nfactory.train_model_b(dataset=df, training_settings=training_settings)\n</code></pre>"},{"location":"neural_net_players/#data-state","title":"Data &amp; State","text":"<ul> <li>has_log_probs: bool, constraints: always True in this implementation; indicates Tournament should attempt to read log-probabilities via <code>get_log_prob</code> from underlying players.</li> <li>game_layout: GameLayout, constraints: non-None after construction; used to derive <code>field_size</code> and <code>comms_size</code>.</li> <li>explore: bool, constraints: {True, False}; propagated to created/cached players and used when creating new ones.</li> <li>model_a: tf.keras.Model or None, constraints: if non-None should map input shape (n2,) to output shape (m,) with logits semantics (trained with <code>BinaryCrossentropy(from_logits=True)</code>).</li> <li>model_b: tf.keras.Model or None, constraints: if non-None should map input shape (1 + m,) to output shape (1,) with logits semantics (trained with <code>BinaryCrossentropy(from_logits=True)</code>).</li> <li>_playerA: NeuralNetPlayerA or None, constraints: cached instance; created lazily by <code>players()</code>.</li> <li>_playerB: NeuralNetPlayerB or None, constraints: cached instance; created lazily by <code>players()</code>.</li> </ul>"},{"location":"neural_net_players/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"neural_net_players/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"neural_net_players/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>Dimension symbols used throughout training are derived from <code>self.game_layout</code>: \\(n2 = \\mathrm{field\\_size}^2\\) and \\(m = \\mathrm{comms\\_size}\\).</li> <li>Default model architectures are constructed lazily; changes to <code>game_layout</code> after instantiation may desynchronize expected shapes from previously built models (no validation is performed here).</li> <li><code>train()</code> is retained only for backward compatibility and intentionally performs no training; callers should be migrated to <code>train_model_a()</code> and <code>train_model_b()</code>.</li> </ul>"},{"location":"neural_net_players/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.players_base.Players</code> (base factory interface)</li> <li><code>Q_Sea_Battle.neural_net_player_a.NeuralNetPlayerA</code> and <code>_scale_field</code></li> <li><code>Q_Sea_Battle.neural_net_player_b.NeuralNetPlayerB</code> and <code>_gun_one_hot_to_index</code></li> <li><code>Q_Sea_Battle.game_layout.GameLayout</code></li> </ul>"},{"location":"neural_net_players/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version in module header; <code>train_model_a</code> and <code>train_model_b</code> present alongside deprecated no-op <code>train()</code>.</li> </ul>"},{"location":"player_base_a/","title":"PlayerA","text":"<p>Role: Baseline A-side player that emits a random binary communication vector of length \\(m=\\mathrm{comms\\_size}\\), independent of inputs. Location: <code>Q_Sea_Battle.player_base_a.PlayerA</code></p>"},{"location":"player_base_a/#constructor","title":"Constructor","text":"Parameter Type Description game_layout GameLayout, constraints: not specified, shape: not applicable Game configuration for this player; stored as <code>self.game_layout</code>. <p>Preconditions</p> <ul> <li><code>game_layout</code> must provide attribute <code>comms_size</code> (type and constraints not specified in this module).</li> </ul> <p>Postconditions</p> <ul> <li><code>self.game_layout</code> is set to the provided <code>game_layout</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.player_base_a import PlayerA\nfrom Q_Sea_Battle.game_layout import GameLayout\n\nlayout = GameLayout(...)  # Not specified in this module\nplayer = PlayerA(game_layout=layout)\n</code></pre>"},{"location":"player_base_a/#public-methods","title":"Public Methods","text":""},{"location":"player_base_a/#decidefield-suppnone","title":"decide(field, supp=None)","text":"<p>Decide on a communication vector given the field; the base implementation ignores both inputs and returns a random binary vector of length \\(m=\\mathrm{comms\\_size}\\).</p> <p>Parameters</p> <ul> <li><code>field</code>: np.ndarray, dtype int {0,1}, shape (n2,); flattened field array containing 0/1 values; content is unused in the base implementation.</li> <li><code>supp</code>: Optional[Any], constraints: may be None, shape: not applicable; optional supporting information (unused in base class).</li> </ul> <p>Returns</p> <ul> <li>np.ndarray, dtype int {0,1}, shape (m,); a one-dimensional communication array with entries in {0, 1}, where \\(m=\\mathrm{game\\_layout.comms\\_size}\\).</li> </ul> <p>Preconditions</p> <ul> <li><code>self.game_layout.comms_size</code> is defined and is usable as the <code>size</code> argument to <code>np.random.randint</code> (exact type constraints not specified).</li> </ul> <p>Postconditions</p> <ul> <li>No state changes are specified.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>import numpy as np\nfrom Q_Sea_Battle.player_base_a import PlayerA\nfrom Q_Sea_Battle.game_layout import GameLayout\n\nlayout = GameLayout(...)  # Not specified in this module\nplayer = PlayerA(layout)\n\nn2 = 100\nfield = np.zeros((n2,), dtype=int)\ncomms = player.decide(field)\n</code></pre>"},{"location":"player_base_a/#data-state","title":"Data &amp; State","text":"<ul> <li><code>game_layout</code>: GameLayout, constraints: not specified, shape: not applicable; shared configuration from the Players factory; used for <code>comms_size</code>.</li> </ul>"},{"location":"player_base_a/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>None specified.</li> </ul>"},{"location":"player_base_a/#deviations","title":"Deviations","text":"<ul> <li>None identified.</li> </ul>"},{"location":"player_base_a/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>This baseline uses <code>np.random.randint(0, 2, size=m, dtype=int)</code>; any changes that affect reproducibility (e.g., RNG seeding) are not specified in this module and should be documented explicitly if introduced.</li> <li>Ensure <code>field</code> remains a flattened 0/1 vector of shape <code>(n2,)</code> if downstream implementations start depending on it; this base class currently ignores <code>field</code> and <code>supp</code>.</li> </ul>"},{"location":"player_base_a/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.game_layout.GameLayout</code> (provides <code>comms_size</code>).</li> </ul>"},{"location":"player_base_a/#changelog","title":"Changelog","text":"<ul> <li>Version in module docstring: 0.2.</li> </ul>"},{"location":"player_base_b/","title":"PlayerB","text":"<p>Role: Baseline B-side player that decides whether to shoot; default strategy is random.</p> <p>Location: <code>Q_Sea_Battle.player_base_b.PlayerB</code></p>"},{"location":"player_base_b/#constructor","title":"Constructor","text":"Parameter Type Description game_layout <code>GameLayout</code>, not specified, shape (not applicable) Game configuration for this player; stored on the instance as <code>game_layout</code>. <p>Preconditions</p> <ul> <li><code>game_layout</code> is provided (not <code>None</code>); further constraints are not specified.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.game_layout</code> is set to the provided <code>game_layout</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.player_base_b import PlayerB\nfrom Q_Sea_Battle.game_layout import GameLayout\n\ngame_layout = GameLayout()  # arguments not specified in this module\nplayer_b = PlayerB(game_layout=game_layout)\n</code></pre>"},{"location":"player_base_b/#public-methods","title":"Public Methods","text":""},{"location":"player_base_b/#decidegun-comm-suppnone","title":"decide(gun, comm, supp=None)","text":"<p>Decide whether to shoot based on gun position and message; the base implementation ignores inputs and returns a random decision in {0, 1}.</p> <p>Parameters</p> <ul> <li><code>gun</code>: <code>np.ndarray</code>, dtype not specified, flattened one-hot encoding, shape (n2,).</li> <li><code>comm</code>: <code>np.ndarray</code>, dtype not specified, communication vector, shape (comms_size,).</li> <li><code>supp</code>: <code>Optional[Any]</code>, constraint: may be <code>None</code>, shape (not applicable).</li> </ul> <p>Returns</p> <ul> <li><code>int</code>, constraint: value in {0, 1}, shape (not applicable).</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>import numpy as np\nfrom Q_Sea_Battle.player_base_b import PlayerB\nfrom Q_Sea_Battle.game_layout import GameLayout\n\nplayer_b = PlayerB(game_layout=GameLayout())\ngun = np.zeros((10,), dtype=int)   # n2 is not specified in this module\ncomm = np.zeros((4,), dtype=float) # comms_size is not specified in this module\naction = player_b.decide(gun=gun, comm=comm)\nassert action in (0, 1)\n</code></pre>"},{"location":"player_base_b/#data-state","title":"Data &amp; State","text":"<ul> <li><code>game_layout</code>: <code>GameLayout</code>, not specified, shape (not applicable); shared configuration from the Players factory.</li> </ul>"},{"location":"player_base_b/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"player_base_b/#deviations","title":"Deviations","text":"<ul> <li>No design notes were provided; no deviations can be derived.</li> </ul>"},{"location":"player_base_b/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>The current implementation of <code>decide</code> is intentionally input-agnostic and uses <code>np.random.randint(0, 2)</code>; subclasses may override <code>decide</code> to implement non-random strategies.</li> </ul>"},{"location":"player_base_b/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.game_layout.GameLayout</code></li> </ul>"},{"location":"player_base_b/#changelog","title":"Changelog","text":"<ul> <li>Not specified in module text.</li> </ul>"},{"location":"players_base/","title":"Players","text":"<p>Role: Factory and container for a pair of QSeaBattle players. Location: <code>Q_Sea_Battle.players_base.Players</code></p>"},{"location":"players_base/#constructor","title":"Constructor","text":"Parameter Type Description game_layout Optional[GameLayout], constraint: None or instance of <code>Q_Sea_Battle.game_layout.GameLayout</code>, shape: scalar Optional shared configuration; if None, a default <code>GameLayout</code> is created. <p>Preconditions</p> <ul> <li>Not specified.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.game_layout</code> is set to the provided <code>game_layout</code> if not None, otherwise to a newly created <code>GameLayout</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.players_base import Players\n\nplayers_facade = Players()\nplayer_a, player_b = players_facade.players()\n</code></pre>"},{"location":"players_base/#public-methods","title":"Public Methods","text":""},{"location":"players_base/#players_1","title":"players","text":"<p>Create the concrete Player A and Player B instances using the shared <code>GameLayout</code>.</p> <p>Parameters</p> <ul> <li>None.</li> </ul> <p>Returns</p> <ul> <li>Tuple[\"PlayerA\", \"PlayerB\"], constraint: 2-tuple of player instances, shape: (2,).</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.players_base import Players\n\np = Players()\nplayer_a, player_b = p.players()\n</code></pre>"},{"location":"players_base/#reset","title":"reset","text":"<p>Reset any internal state across both players.</p> <p>Parameters</p> <ul> <li>None.</li> </ul> <p>Returns</p> <ul> <li>None, constraint: always <code>None</code>, shape: scalar.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.players_base import Players\n\np = Players()\np.reset()\n</code></pre>"},{"location":"players_base/#data-state","title":"Data &amp; State","text":"<ul> <li>game_layout: GameLayout, constraint: instance of <code>Q_Sea_Battle.game_layout.GameLayout</code>, shape: scalar; shared configuration used by both players.</li> </ul>"},{"location":"players_base/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"players_base/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"players_base/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>This module also provides deprecated attribute access for <code>PlayerA</code> and <code>PlayerB</code> via module-level <code>__getattr__</code>, emitting <code>DeprecationWarning</code> and caching the resolved symbols in <code>globals()</code>.</li> </ul>"},{"location":"players_base/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.player_base_a.PlayerA</code> (concrete baseline implementation imported as <code>_PlayerA</code> internally)</li> <li><code>Q_Sea_Battle.player_base_b.PlayerB</code> (concrete baseline implementation imported as <code>_PlayerB</code> internally)</li> <li><code>Q_Sea_Battle.game_layout.GameLayout</code></li> </ul>"},{"location":"players_base/#changelog","title":"Changelog","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pr_assisted/","title":"PRAssisted","text":"<p>Role: Stateful two-party PR-assisted resource that returns a uniformly random first outcome string and a second outcome string biased-correlated to the first based on both parties\u2019 measurement settings and <code>p_high</code>.</p> <p>Location: <code>Q_Sea_Battle.pr_assisted.PRAssisted</code></p>"},{"location":"pr_assisted/#constructor","title":"Constructor","text":"Parameter Type Description length int, constraint \\(length \\ge 1\\) Number of bits in each measurement/outcome string. p_high float, constraint \\(0.0 \\le p\\_high \\le 1.0\\) Correlation parameter controlling how likely the second outcome matches (or complements) the first outcome per index. <p>Preconditions</p> <ul> <li><code>length</code> is <code>int</code>, constraint \\(length \\ge 1\\).</li> <li><code>p_high</code> is <code>int</code> or <code>float</code>, convertible to <code>float</code>, constraint \\(0.0 \\le p\\_high \\le 1.0\\).</li> </ul> <p>Postconditions</p> <ul> <li><code>self.length</code> is set to <code>length</code>.</li> <li><code>self.p_high</code> is set to <code>float(p_high)</code>.</li> <li>Measurement state is reset: <code>a_measured == False</code>, <code>b_measured == False</code>, and previous-measurement/outcome fields are <code>None</code>.</li> <li>A NumPy RNG is created and stored in <code>self._rng</code>.</li> </ul> <p>Errors</p> <ul> <li>Raises <code>TypeError</code> if <code>length</code> is not an <code>int</code>.</li> <li>Raises <code>ValueError</code> if <code>length &lt; 1</code>.</li> <li>Raises <code>TypeError</code> if <code>p_high</code> is not an <code>int</code> or <code>float</code>.</li> <li>Raises <code>ValueError</code> if <code>p_high</code> is outside \\([0.0, 1.0]\\).</li> </ul> <p>Example</p> <p>Construct and use in a single round</p> <pre><code>import numpy as np\nfrom Q_Sea_Battle.pr_assisted import PRAssisted\n\npr = PRAssisted(length=4, p_high=0.8)\n\nx_a = np.array([0, 1, 0, 1], dtype=int)\nx_b = np.array([1, 1, 0, 0], dtype=int)\n\nout_a = pr.measurement_a(x_a)\nout_b = pr.measurement_b(x_b)\n\npr.reset()\n</code></pre>"},{"location":"pr_assisted/#public-methods","title":"Public Methods","text":""},{"location":"pr_assisted/#measurement_ameasurement","title":"measurement_a(measurement)","text":"<p>Perform a measurement by party A.</p> <p>Arguments</p> <ul> <li>measurement: np.ndarray, dtype int, values in {0,1}, shape (length,), 1D measurement setting vector.</li> </ul> <p>Returns</p> <ul> <li>np.ndarray, dtype int, values in {0,1}, shape (length,), 1D outcome vector for party A.</li> </ul> <p>Preconditions</p> <ul> <li><code>self.a_measured</code> is <code>False</code> for the current round.</li> <li><code>measurement</code> is valid per <code>_validate_measurement</code>: coercible to 1D <code>int</code> array, shape <code>(length,)</code>, values in <code>{0,1}</code>.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.a_measured</code> is set to <code>True</code>.</li> <li>If this is the first measurement of the round: <code>prev_party</code>, <code>prev_measurement</code>, and <code>prev_outcome</code> are stored and the returned outcome is uniformly random in <code>{0,1}^{length}</code>.</li> <li>If this is the second measurement of the round: the returned outcome is generated by <code>_second_measurement(...)</code> using stored previous measurement/outcome.</li> </ul> <p>Errors</p> <ul> <li>Raises <code>ValueError</code> if A already measured in the current round.</li> <li>Raises <code>ValueError</code> if <code>measurement</code> is not 1D, not shape <code>(length,)</code>, or contains values outside <code>{0,1}</code>.</li> </ul> <p>Example</p> <p>A measures first</p> <pre><code>import numpy as np\nfrom Q_Sea_Battle.pr_assisted import PRAssisted\n\npr = PRAssisted(length=3, p_high=0.9)\nout_a = pr.measurement_a(np.array([0, 0, 1], dtype=int))\n</code></pre>"},{"location":"pr_assisted/#measurement_bmeasurement","title":"measurement_b(measurement)","text":"<p>Perform a measurement by party B.</p> <p>Arguments</p> <ul> <li>measurement: np.ndarray, dtype int, values in {0,1}, shape (length,), 1D measurement setting vector.</li> </ul> <p>Returns</p> <ul> <li>np.ndarray, dtype int, values in {0,1}, shape (length,), 1D outcome vector for party B.</li> </ul> <p>Preconditions</p> <ul> <li><code>self.b_measured</code> is <code>False</code> for the current round.</li> <li><code>measurement</code> is valid per <code>_validate_measurement</code>: coercible to 1D <code>int</code> array, shape <code>(length,)</code>, values in <code>{0,1}</code>.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.b_measured</code> is set to <code>True</code>.</li> <li>If this is the first measurement of the round: <code>prev_party</code>, <code>prev_measurement</code>, and <code>prev_outcome</code> are stored and the returned outcome is uniformly random in <code>{0,1}^{length}</code>.</li> <li>If this is the second measurement of the round: the returned outcome is generated by <code>_second_measurement(...)</code> using stored previous measurement/outcome.</li> </ul> <p>Errors</p> <ul> <li>Raises <code>ValueError</code> if B already measured in the current round.</li> <li>Raises <code>ValueError</code> if <code>measurement</code> is not 1D, not shape <code>(length,)</code>, or contains values outside <code>{0,1}</code>.</li> </ul> <p>Example</p> <p>B measures second</p> <pre><code>import numpy as np\nfrom Q_Sea_Battle.pr_assisted import PRAssisted\n\npr = PRAssisted(length=2, p_high=0.7)\nout_a = pr.measurement_a(np.array([1, 0], dtype=int))\nout_b = pr.measurement_b(np.array([1, 1], dtype=int))\n</code></pre>"},{"location":"pr_assisted/#reset","title":"reset()","text":"<p>Reset internal measurement state for reuse of the resource.</p> <p>Arguments</p> <ul> <li>None.</li> </ul> <p>Returns</p> <ul> <li>None.</li> </ul> <p>Postconditions</p> <ul> <li><code>a_measured == False</code></li> <li><code>b_measured == False</code></li> <li><code>prev_party is None</code></li> <li><code>prev_measurement is None</code></li> <li><code>prev_outcome is None</code></li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <p>Reset between rounds</p> <pre><code>from Q_Sea_Battle.pr_assisted import PRAssisted\n\npr = PRAssisted(length=1, p_high=0.5)\npr.reset()\n</code></pre>"},{"location":"pr_assisted/#data-state","title":"Data &amp; State","text":"<p>Public attributes</p> <ul> <li>length: int, constraint \\(length \\ge 1\\), scalar, number of bits in each measurement/outcome string.</li> <li>p_high: float, constraint \\(0.0 \\le p\\_high \\le 1.0\\), scalar, correlation parameter.</li> <li>a_measured: bool, scalar, whether party A has measured in the current round.</li> <li>b_measured: bool, scalar, whether party B has measured in the current round.</li> <li>prev_party: Optional[str], constraint in {\"a\",\"b\"} when not None, scalar, party label of the first measurer for the current round.</li> <li>prev_measurement: Optional[np.ndarray], dtype int, values in {0,1}, shape (length,), first party\u2019s measurement vector for the current round when set.</li> <li>prev_outcome: Optional[np.ndarray], dtype int, values in {0,1}, shape (length,), first party\u2019s outcome vector for the current round when set.</li> </ul> <p>Private/internal attributes</p> <ul> <li>_rng: numpy.random.Generator, RNG used for uniform bit generation and per-index Bernoulli draws.</li> </ul> <p>State model (round semantics)</p> <ul> <li>A \u201cround\u201d consists of up to two successful measurements, at most one by A and at most one by B, in either order; <code>reset()</code> clears the round state.</li> </ul>"},{"location":"pr_assisted/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pr_assisted/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pr_assisted/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>The first successful measurement in a round stores <code>prev_measurement</code> and <code>prev_outcome</code>; the second successful measurement uses those stored arrays to generate correlated outcomes; <code>reset()</code> must be called to reuse the instance for a new round.</li> <li>Input validation is centralized in <code>_validate_measurement</code>, which coerces inputs using <code>np.asarray(..., dtype=int)</code>; callers can pass array-like inputs as long as they coerce to a valid 1D 0/1 vector of length <code>length</code>.</li> </ul>"},{"location":"pr_assisted/#related","title":"Related","text":"<ul> <li><code>SharedRandomness</code> in <code>shared_randomness.py</code> (mentioned in the module docstring as functionally identical; not present in this module text).</li> </ul>"},{"location":"pr_assisted/#changelog","title":"Changelog","text":"<ul> <li>Version 0.1: Initial implementation of <code>PRAssisted</code> (as indicated by the module docstring).</li> </ul>"},{"location":"pr_assisted_layer/","title":"PRAssistedLayer","text":"<p>Role: Stateless Keras layer that produces correlated two-party outcomes from current/previous measurements and previous outcomes, in either expected-value or deterministic stateless-sampling mode.</p> <p>Location: <code>Q_Sea_Battle.pr_assisted_layer.PRAssistedLayer</code></p>"},{"location":"pr_assisted_layer/#constructor","title":"Constructor","text":"Parameter Type Description <code>length</code> <code>int</code>, constraint: <code>&gt; 0</code>, shape: scalar Number of bits in each measurement/outcome vector (the required size of the last dimension of input tensors). <code>p_high</code> <code>float</code>, constraint: \\(0 \\le p\\_high \\le 1\\), shape: scalar Correlation parameter used to compute the probability of matching the previous outcome on the second measurement. <code>mode</code> <code>str</code>, constraint: one of <code>{\"expected\",\"sample\"}</code>, shape: scalar Operation mode: <code>\"expected\"</code> returns deterministic expected outcomes in $[0,1]<code>;</code>\"sample\"<code>returns sampled binary outcomes in</code>`. <code>resource_index</code> <code>Optional[int]</code>, constraint: <code>None</code> or any <code>int</code>, shape: scalar Optional identifier mixed into the stateless RNG seed derivation. <code>seed</code> <code>Optional[int]</code>, constraint: <code>None</code> or any <code>int</code>, shape: scalar Optional base seed for deterministic stateless sampling; if <code>None</code>, sampling is process-local and may be non-deterministic across processes. <code>name</code> <code>Optional[str]</code>, constraint: <code>None</code> or any <code>str</code>, shape: scalar Optional layer name passed to the Keras <code>Layer</code> constructor. <code>**kwargs</code> <code>Any</code>, constraint: Keras <code>Layer</code> kwargs, shape: N/A Forwarded to <code>tf.keras.layers.Layer</code>. <p>Preconditions</p> <ul> <li><code>length &gt; 0</code>.</li> <li><code>p_high</code> is within <code>[0, 1]</code> (after <code>float(p_high)</code> conversion).</li> <li><code>mode</code> is either <code>\"expected\"</code> or <code>\"sample\"</code>.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.length: int</code> is set to <code>int(length)</code>.</li> <li><code>self.p_high: float</code> is set to <code>float(p_high)</code>.</li> <li><code>self.mode: str</code> is set to <code>mode</code>.</li> <li><code>self.resource_index: Optional[int]</code> is set to <code>None</code> or <code>int(resource_index)</code>.</li> <li><code>self.seed: Optional[int]</code> is set to <code>None</code> or <code>int(seed)</code>.</li> <li>No trainable variables are created by this constructor (stateless behavior is by design, but variable absence is not explicitly asserted in code).</li> </ul> <p>Errors</p> <ul> <li>Raises <code>ValueError</code> if <code>length &lt;= 0</code>.</li> <li>Raises <code>ValueError</code> if <code>p_high</code> is not in <code>[0, 1]</code>.</li> <li>Raises <code>ValueError</code> if <code>mode</code> is not one of <code>{\"expected\", \"sample\"}</code>.</li> </ul> <p>Example</p> <p>Construct a layer</p> <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.pr_assisted_layer import PRAssistedLayer\n\nlayer = PRAssistedLayer(length=8, p_high=0.9, mode=\"expected\", resource_index=0, seed=123)\n\ninputs = {\n    \"current_measurement\": tf.zeros((2, 8), dtype=tf.float32),\n    \"previous_measurement\": tf.zeros((2, 8), dtype=tf.float32),\n    \"previous_outcome\": tf.zeros((2, 8), dtype=tf.float32),\n    \"first_measurement\": tf.ones((2, 1), dtype=tf.float32),\n}\ny = layer(inputs)  # tf.Tensor, dtype float32, shape (2, 8)\n</code></pre>"},{"location":"pr_assisted_layer/#public-methods","title":"Public Methods","text":""},{"location":"pr_assisted_layer/#get_config-dictstr-any","title":"<code>get_config() -&gt; Dict[str, Any]</code>","text":"<p>Return layer config for Keras serialization.</p> <p>Parameters</p> <ul> <li>None.</li> </ul> <p>Returns</p> <ul> <li><code>Dict[str, Any]</code>, constraints: JSON-serializable-by-Keras values for this layer\u2019s constructor fields; shape: mapping with scalar values under keys <code>{\"length\",\"p_high\",\"mode\",\"resource_index\",\"seed\"}</code> plus base Keras config fields.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul>"},{"location":"pr_assisted_layer/#callinputs-dictstr-tftensor-tftensor","title":"<code>call(inputs: Dict[str, tf.Tensor]) -&gt; tf.Tensor</code>","text":"<p>Compute correlated outcomes.</p> <p>Parameters</p> <ul> <li><code>inputs</code>: <code>Dict[str, tf.Tensor]</code>, constraints: keys must match exactly <code>{\"current_measurement\",\"previous_measurement\",\"previous_outcome\",\"first_measurement\"}</code>; shapes and dtypes are normalized internally as follows: <code>current_measurement</code>, <code>previous_measurement</code>, <code>previous_outcome</code> are converted to <code>tf.Tensor, dtype float32, shape (..., length)</code> with soft range assertions \\([0,1]\\); <code>first_measurement</code> is converted to <code>tf.Tensor, dtype float32, shape broadcastable to (..., length)</code> and interpreted as boolean via <code>first_measurement &gt;= 0.5</code>.</li> </ul> <p>Returns</p> <ul> <li><code>tf.Tensor</code>, dtype <code>float32</code>, shape <code>(..., length)</code>, constraints: in <code>mode=\"expected\"</code> values are in \\([0,1]\\); in <code>mode=\"sample\"</code> values are in <code>{0,1}</code> after rounding.</li> </ul> <p>Errors</p> <ul> <li>Raises <code>ValueError</code> if <code>inputs</code> keys do not match the required set exactly (missing or extra keys).</li> <li>May raise <code>tf.errors.InvalidArgumentError</code> (via <code>tf.debugging.assert_equal</code>) if the last dimension of any of <code>current_measurement</code>, <code>previous_measurement</code>, or <code>previous_outcome</code> is not equal to <code>length</code>.</li> <li>May raise <code>tf.errors.InvalidArgumentError</code> (via <code>tf.debugging.assert_greater_equal</code> / <code>assert_less_equal</code>) if any of <code>current_measurement</code>, <code>previous_measurement</code>, or <code>previous_outcome</code> contain values outside <code>[0, 1]</code>.</li> <li>In <code>mode=\"sample\"</code>, may raise <code>tf.errors.InvalidArgumentError</code> (via <code>tf.debugging.assert_near</code>) if <code>previous_outcome</code> is not binary (not near its rounded value).</li> </ul> <p>Input semantics</p> <p>The layer does not maintain internal state about measurement ordering; the caller must provide <code>first_measurement</code> (first vs second) and pass the corresponding <code>previous_measurement</code> and <code>previous_outcome</code> tensors.</p>"},{"location":"pr_assisted_layer/#data-state","title":"Data &amp; State","text":"<ul> <li><code>length</code>: <code>int</code>, constraint: <code>&gt; 0</code>, shape: scalar; the expected size of the last dimension of measurement/outcome vectors.</li> <li><code>p_high</code>: <code>float</code>, constraint: \\(0 \\le p\\_high \\le 1\\), shape: scalar; correlation parameter used in the second-measurement rule.</li> <li><code>mode</code>: <code>str</code>, constraint: one of <code>{\"expected\",\"sample\"}</code>, shape: scalar.</li> <li><code>resource_index</code>: <code>Optional[int]</code>, constraint: <code>None</code> or any <code>int</code>, shape: scalar; mixed into stateless RNG seed derivation.</li> <li><code>seed</code>: <code>Optional[int]</code>, constraint: <code>None</code> or any <code>int</code>, shape: scalar; base seed for stateless RNG, with a process-local fallback when <code>None</code>.</li> </ul>"},{"location":"pr_assisted_layer/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pr_assisted_layer/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pr_assisted_layer/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>Input validation is strict on keys: <code>_validate_inputs</code> requires the input dictionary keys to match exactly; adding optional keys will currently error.</li> <li>Shape checks enforce only the last dimension equality to <code>length</code>; leading dimensions are unconstrained as long as broadcasting of <code>first_measurement</code> to <code>(..., length)</code> is possible.</li> <li><code>mode=\"sample\"</code> uses <code>tf.random.stateless_uniform</code> with a seed derived from <code>seed</code>, <code>resource_index</code>, and a <code>stream_id</code>; when <code>seed is None</code> the base seed is drawn from <code>tf.random.uniform((), ...)</code>, which is process-local and may vary across processes.</li> </ul>"},{"location":"pr_assisted_layer/#related","title":"Related","text":"<ul> <li>TensorFlow Keras Layer base class: <code>tf.keras.layers.Layer</code></li> <li>Internal helper dataclass (private): <code>_PRInputs</code></li> </ul>"},{"location":"pr_assisted_layer/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial implementation of <code>PRAssistedLayer</code> with <code>\"expected\"</code> and <code>\"sample\"</code> modes, strict input validation, and stateless seed derivation.</li> </ul>"},{"location":"pr_assisted_player_a/","title":"PRAssistedPlayerA","text":"<p>Role: Player A implementation using PR-assisted resources to iteratively compress a binary field into a single communication bit. Location: <code>Q_Sea_Battle.pr_assisted_player_a.PRAssistedPlayerA</code></p>"},{"location":"pr_assisted_player_a/#constructor","title":"Constructor","text":"Parameter Type Description game_layout GameLayout, constraints: instance of <code>Q_Sea_Battle.game_layout.GameLayout</code>, shape: N/A Game configuration, used to derive \\(n2 = \\text{field\\_size}^2\\) via <code>game_layout.field_size</code>. parent PRAssistedPlayers, constraints: instance of <code>Q_Sea_Battle.pr_assisted_players.PRAssistedPlayers</code>, shape: N/A Owning factory providing access to PR-assisted boxes via <code>parent.pr_assisted(level)</code>. <p>Preconditions</p> <ul> <li><code>parent</code> is an instance of <code>Q_Sea_Battle.pr_assisted_players.PRAssistedPlayers</code>.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.game_layout</code> is initialized by <code>PlayerA.__init__(game_layout)</code> (exact state not specified in this module).</li> <li><code>self.parent</code> is set to the provided <code>parent</code>.</li> </ul> <p>Errors</p> <ul> <li><code>TypeError</code>: raised if <code>parent</code> is not a <code>PRAssistedPlayers</code> instance.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.game_layout import GameLayout\nfrom Q_Sea_Battle.pr_assisted_players import PRAssistedPlayers\nfrom Q_Sea_Battle.pr_assisted_player_a import PRAssistedPlayerA\n\ngame_layout = GameLayout(...)  # not specified in this module\nparent = PRAssistedPlayers(...)  # not specified in this module\nplayer_a = PRAssistedPlayerA(game_layout=game_layout, parent=parent)\n</code></pre>"},{"location":"pr_assisted_player_a/#public-methods","title":"Public Methods","text":""},{"location":"pr_assisted_player_a/#decidefield-suppnone","title":"decide(field, supp=None)","text":"<p>Compute the communication bit from the field by repeatedly compressing the 1D binary field using PR-assisted resources at increasing <code>level</code> until a single bit remains.</p> <p>Parameters</p> <ul> <li>field: np.ndarray, dtype int {0,1}, shape (n2,) where \\(n2 = \\text{field\\_size}^2\\); constraints: <code>field.ndim == 1</code>, <code>field.shape[0] == n2</code>, and values are only 0/1 (checked after <code>np.asarray(field, dtype=int)</code> coercion).</li> <li>supp: Any | None, constraints: unused and ignored, shape: N/A.</li> </ul> <p>Returns</p> <ul> <li>np.ndarray, dtype int {0,1}, shape (1,); constraints: a 1D NumPy array of length 1 containing the computed communication bit.</li> </ul> <p>Preconditions</p> <ul> <li><code>self.game_layout.field_size</code> is defined and supports exponentiation and integer multiplication such that \\(n2 = \\text{field\\_size}^2\\) is a valid integer.</li> <li>The PR-assisted resources returned by <code>self.parent.pr_assisted(level)</code> provide a method <code>measurement_a(measurement)</code> that accepts <code>measurement: np.ndarray, dtype int {0,1}, shape (m,)</code> and returns an array indexable as <code>outcome_a[k]</code> for <code>k in [0, m)</code>; exact type/shape constraints are not specified in this module but must be compatible with the algorithm.</li> </ul> <p>Postconditions</p> <ul> <li>The returned array contains <code>comm_bit = int(intermediate_field[0])</code> where <code>intermediate_field</code> is the final compressed field of length 1.</li> <li><code>supp</code> has no effect on the output.</li> </ul> <p>Errors</p> <ul> <li><code>ValueError</code>: raised if <code>field</code> is not 1D or its length is not \\(n2\\).</li> <li><code>ValueError</code>: raised if <code>field</code> contains values other than 0/1 (after conversion to <code>dtype=int</code>).</li> <li><code>ValueError</code>: raised if an intermediate compression level produces an odd-length <code>intermediate_field</code> (i.e., <code>intermediate_field.size % 2 != 0</code>).</li> <li><code>RuntimeError</code>: raised if the final <code>intermediate_field</code> does not have length 1 (defensive check).</li> </ul> <p>Example</p> <pre><code>import numpy as np\nfrom Q_Sea_Battle.pr_assisted_player_a import PRAssistedPlayerA\n\n# player_a constructed elsewhere\nfield_size = player_a.game_layout.field_size\nn2 = field_size ** 2\nfield = np.zeros((n2,), dtype=int)\ncomm = player_a.decide(field)\nassert comm.shape == (1,)\nassert comm.dtype == int\n</code></pre>"},{"location":"pr_assisted_player_a/#data-state","title":"Data &amp; State","text":"<ul> <li>parent: PRAssistedPlayers, constraints: instance of <code>Q_Sea_Battle.pr_assisted_players.PRAssistedPlayers</code>, shape: N/A; set in the constructor and used by <code>decide()</code> to access PR-assisted boxes via <code>self.parent.pr_assisted(level)</code>.</li> <li>game_layout: GameLayout, constraints: instance of <code>Q_Sea_Battle.game_layout.GameLayout</code>, shape: N/A; inherited from <code>PlayerA</code> and used by <code>decide()</code> to compute \\(n2 = \\text{field\\_size}^2\\) via <code>self.game_layout.field_size</code>.</li> </ul>"},{"location":"pr_assisted_player_a/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified (no design notes provided).</li> </ul>"},{"location":"pr_assisted_player_a/#deviations","title":"Deviations","text":"<ul> <li>Not specified (no design notes provided).</li> </ul>"},{"location":"pr_assisted_player_a/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li><code>decide()</code> coerces <code>field</code> using <code>np.asarray(field, dtype=int)</code> before validating values; callers passing non-integer numeric types may have their inputs truncated before validation.</li> <li>The compression loop requires that the current length be even at every level; since the initial length is \\(n2 = \\text{field\\_size}^2\\), <code>field_size</code> must be such that repeated halving eventually reaches 1 without producing an odd length; this property is not validated up front.</li> <li>The PR-assisted box interface is used implicitly (<code>measurement_a</code>); changes to <code>PRAssistedPlayers.pr_assisted()</code> or the returned box object must preserve compatibility with this call pattern.</li> </ul>"},{"location":"pr_assisted_player_a/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.players_base.PlayerA</code></li> <li><code>Q_Sea_Battle.game_layout.GameLayout</code></li> <li><code>Q_Sea_Battle.pr_assisted_players.PRAssistedPlayers</code></li> </ul>"},{"location":"pr_assisted_player_a/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version (module docstring indicates PR-assisted naming update and internal rename from \"shared randomness\" to \"PR-assisted\").</li> </ul>"},{"location":"pr_assisted_player_b/","title":"PRAssistedPlayerB","text":"<p>Role: Player B implementation using PR-assisted resources.</p> <p>Location: <code>Q_Sea_Battle.pr_assisted_player_b.PRAssistedPlayerB</code></p>"},{"location":"pr_assisted_player_b/#constructor","title":"Constructor","text":"Parameter Type Description game_layout GameLayout, constraints: instance of <code>Q_Sea_Battle.game_layout.GameLayout</code>, shape: N/A Game configuration passed to <code>PlayerB</code>. parent PRAssistedPlayers, constraints: instance of <code>Q_Sea_Battle.pr_assisted_players.PRAssistedPlayers</code>, shape: N/A Owning factory providing access to PR-assisted boxes via <code>parent.pr_assisted(level)</code>. <p>Preconditions: <code>parent</code> is an instance of <code>PRAssistedPlayers</code>. Postconditions: <code>self.game_layout</code> is initialized by <code>PlayerB.__init__(game_layout)</code>; <code>self.parent</code> is set to <code>parent</code>. Errors: Raises <code>TypeError</code> if <code>parent</code> is not a <code>PRAssistedPlayers</code> instance. Example:</p> <pre><code>from Q_Sea_Battle.game_layout import GameLayout\nfrom Q_Sea_Battle.pr_assisted_players import PRAssistedPlayers\nfrom Q_Sea_Battle.pr_assisted_player_b import PRAssistedPlayerB\n\ngame_layout = GameLayout(...)\nparent = PRAssistedPlayers(...)\nplayer_b = PRAssistedPlayerB(game_layout=game_layout, parent=parent)\n</code></pre>"},{"location":"pr_assisted_player_b/#public-methods","title":"Public Methods","text":""},{"location":"pr_assisted_player_b/#decidegun-comm-suppnone","title":"decide(gun, comm, supp=None)","text":"<p>Decide whether to shoot using PR-assisted resources and communication.</p> Parameter Type Description gun np.ndarray, dtype int {0,1}, shape (n2,) One-hot gun vector. Here \\(n2 = \\mathrm{field\\_size}^2\\) where <code>field_size = self.game_layout.field_size</code>. Must contain exactly one <code>1</code>. comm np.ndarray, dtype int {0,1}, shape (1,) Communication array with a single bit at index <code>0</code>. supp Any | None, constraints: unused, shape: N/A Optional supporting information; ignored. <p>Returns: int, constraints: in {0,1}, shape: scalar; <code>1</code> to shoot or <code>0</code> to not shoot.</p> <p>Preconditions: <code>gun</code> can be converted to <code>np.ndarray</code> with <code>dtype=int</code> and satisfies one-hot and shape constraints; <code>comm</code> can be converted to <code>np.ndarray</code> with <code>dtype=int</code> and satisfies shape/value constraints; <code>self.parent.pr_assisted(level)</code> returns an object supporting <code>measurement_b(measurement)</code> for successive <code>level</code> values. Postconditions: Does not mutate input arrays; computes <code>shoot</code> as the parity (mod 2) of a list consisting of PR-assisted outcomes gathered per reduction level and <code>comm[0]</code>. Errors: Raises <code>ValueError</code> if any of the following checks fail: <code>gun</code> is not 1D of length <code>n2</code>; <code>gun</code> contains values outside <code>{0,1}</code>; <code>gun.sum() != 1</code>; <code>comm</code> is not 1D of length <code>1</code>; <code>comm</code> contains values outside <code>{0,1}</code>; at any reduction level the intermediate gun length is odd; the intermediate gun is not one-hot; there is not exactly one active pair <code>(0, 1)</code> or <code>(1, 0)</code> per level; the constructed <code>measurement</code> has sum not in <code>{0,1}</code>. Example:</p> <pre><code>import numpy as np\n\nfield_size = player_b.game_layout.field_size\nn2 = field_size ** 2\n\ngun = np.zeros(n2, dtype=int)\ngun[3] = 1\ncomm = np.array([1], dtype=int)\n\nshoot = player_b.decide(gun=gun, comm=comm)\nassert shoot in (0, 1)\n</code></pre>"},{"location":"pr_assisted_player_b/#data-state","title":"Data &amp; State","text":"<ul> <li><code>parent</code>: PRAssistedPlayers, constraints: instance of <code>Q_Sea_Battle.pr_assisted_players.PRAssistedPlayers</code>, shape: N/A; reference to the owning factory that provides PR-assisted boxes via <code>pr_assisted(level)</code>.</li> <li>Inherited state: Not specified in this module; see <code>Q_Sea_Battle.players_base.PlayerB</code>.</li> </ul>"},{"location":"pr_assisted_player_b/#planned-design-spec","title":"Planned (design-spec)","text":"<p>Not specified.</p>"},{"location":"pr_assisted_player_b/#deviations","title":"Deviations","text":"<p>Not specified.</p>"},{"location":"pr_assisted_player_b/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>This class enforces strict input validation (shape, dtype conversion to <code>int</code>, and {0,1} constraints) before interacting with PR-assisted resources; update validation consistently if upstream interfaces change.</li> <li>The constructor uses a local import of <code>PRAssistedPlayers</code> to avoid an import cycle.</li> </ul>"},{"location":"pr_assisted_player_b/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.players_base.PlayerB</code></li> <li><code>Q_Sea_Battle.pr_assisted_players.PRAssistedPlayers</code></li> <li><code>Q_Sea_Battle.game_layout.GameLayout</code></li> </ul>"},{"location":"pr_assisted_player_b/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version in package metadata; PR-assisted naming update described in module docstring.</li> </ul>"},{"location":"pr_assisted_players/","title":"PRAssistedPlayers","text":"<p>Role: Factory that owns a hierarchy of PR-assisted resources and hands out paired <code>PRAssistedPlayerA</code> / <code>PRAssistedPlayerB</code> instances.</p> <p>Location: <code>Q_Sea_Battle.pr_assisted_players.PRAssistedPlayers</code></p>"},{"location":"pr_assisted_players/#derived-constraints","title":"Derived constraints","text":"<ul> <li>Let <code>field_size</code> be <code>game_layout.field_size</code> (int, constraints not specified in this module) and <code>comms_size</code> be <code>game_layout.comms_size</code> (int, constraints not specified in this module).</li> <li><code>comms_size == 1</code> is required.</li> <li>Let <code>n2 = field_size ** 2</code> (int). <code>n2 &gt; 0</code> is required.</li> <li><code>n2</code> must be a power of two (equivalently: <code>n2 &amp; (n2 - 1) == 0</code> for <code>n2 &gt; 0</code>), and additionally <code>_create_pr_assisted_array()</code> enforces exact power-of-two via <code>2**n == n2</code> where \\(n = \\lfloor \\log_2(n2) \\rfloor\\).</li> <li>The internal PR-assisted resource list has length \\(n\\), and contains resources with lengths \\(2^{n-1}, 2^{n-2}, \\dots, 2^0\\).</li> </ul>"},{"location":"pr_assisted_players/#constructor","title":"Constructor","text":"Parameter Type Description <code>game_layout</code> <code>GameLayout</code>, constraints Unknown, shape N/A Game configuration used to derive <code>field_size</code> and <code>comms_size</code>. <code>p_high</code> <code>float</code>, constraints Unknown, shape N/A Correlation parameter used for all PR-assisted resources. Coerced via <code>float(p_high)</code>. <p>Preconditions</p> <ul> <li><code>game_layout.comms_size == 1</code>.</li> <li><code>n2 = game_layout.field_size ** 2</code> satisfies <code>n2 &gt; 0</code>.</li> <li><code>n2</code> is a power of two.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.p_high</code> is set to <code>float(p_high)</code>.</li> <li><code>self._pr_assisted_array</code> is created via <code>_create_pr_assisted_array()</code>.</li> <li><code>self._playerA is None</code> and <code>self._playerB is None</code> (players are created lazily by <code>players()</code>).</li> </ul> <p>Errors</p> <ul> <li>Raises <code>ValueError</code> if <code>game_layout.comms_size != 1</code>.</li> <li>Raises <code>ValueError</code> if <code>game_layout.field_size ** 2 &lt;= 0</code>.</li> <li>Raises <code>ValueError</code> if <code>game_layout.field_size ** 2</code> is not a power of two.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.game_layout import GameLayout\nfrom Q_Sea_Battle.pr_assisted_players import PRAssistedPlayers\n\nlayout = GameLayout(field_size=4, comms_size=1)  # other args, if any, are not specified here\nfactory = PRAssistedPlayers(game_layout=layout, p_high=0.9)\nplayer_a, player_b = factory.players()\n</code></pre>"},{"location":"pr_assisted_players/#public-methods","title":"Public Methods","text":""},{"location":"pr_assisted_players/#players","title":"players","text":"<ul> <li>Signature: <code>players(self) -&gt; Tuple[PlayerA, PlayerB]</code></li> <li>Returns: <code>Tuple[PlayerA, PlayerB]</code>, constraints Unknown, shape <code>(2,)</code> as a 2-tuple <code>(player_a, player_b)</code>.</li> <li>Behavior: Creates <code>PRAssistedPlayerA</code> and <code>PRAssistedPlayerB</code> on first call (lazy initialization) and caches them; later calls return the cached instances.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>player_a, player_b = factory.players()\nplayer_a2, player_b2 = factory.players()\nassert player_a is player_a2 and player_b is player_b2\n</code></pre>"},{"location":"pr_assisted_players/#reset","title":"reset","text":"<ul> <li>Signature: <code>reset(self) -&gt; None</code></li> <li>Returns: <code>None</code>, constraints N/A, shape N/A.</li> <li>Behavior: Recreates <code>self._pr_assisted_array</code> via <code>_create_pr_assisted_array()</code>; does not modify cached <code>_playerA</code> / <code>_playerB</code> in this module.</li> </ul> <p>Errors</p> <ul> <li>May raise <code>ValueError</code> propagated from <code>_create_pr_assisted_array()</code> if the current <code>game_layout.field_size ** 2</code> is not an exact power of two.</li> </ul> <p>Example</p> <pre><code>factory.reset()\n</code></pre>"},{"location":"pr_assisted_players/#pr_assisted","title":"pr_assisted","text":"<ul> <li>Signature: <code>pr_assisted(self, index: int) -&gt; PRAssisted</code></li> <li>Parameters:</li> <li><code>index</code>: <code>int</code>, constraints: must be a valid list index for <code>self._pr_assisted_array</code>, shape N/A.</li> <li>Returns: <code>PRAssisted</code>, constraints Unknown, shape N/A.</li> <li>Behavior: Returns the PR-assisted resource at <code>self._pr_assisted_array[index]</code>.</li> </ul> <p>Errors</p> <ul> <li>Raises <code>IndexError</code> if <code>index</code> is out of bounds (propagated from list indexing).</li> </ul> <p>Example</p> <pre><code>box0 = factory.pr_assisted(0)\n</code></pre>"},{"location":"pr_assisted_players/#shared_randomness","title":"shared_randomness","text":"<ul> <li>Signature: <code>shared_randomness(self, index: int) -&gt; PRAssisted</code></li> <li>Parameters:</li> <li><code>index</code>: <code>int</code>, constraints: must be a valid list index for <code>self._pr_assisted_array</code>, shape N/A.</li> <li>Returns: <code>PRAssisted</code>, constraints Unknown, shape N/A.</li> <li>Behavior: Prints a deprecation warning to stdout and delegates to <code>pr_assisted(index)</code>.</li> </ul> <p>Errors</p> <ul> <li>Raises <code>IndexError</code> if <code>index</code> is out of bounds (via <code>pr_assisted</code>).</li> </ul> <p>Example</p> <pre><code>box0 = factory.shared_randomness(0)  # prints a deprecation warning\n</code></pre>"},{"location":"pr_assisted_players/#data-state","title":"Data &amp; State","text":"<ul> <li><code>game_layout</code>: <code>GameLayout</code>, constraints Unknown, shape N/A; inherited from <code>Players</code> (assignment performed by <code>Players.__init__</code> as invoked by <code>super().__init__(game_layout)</code>).</li> <li><code>p_high</code>: <code>float</code>, constraints Unknown, shape N/A; correlation parameter used when constructing each <code>PRAssisted</code>.</li> <li><code>_pr_assisted_array</code>: <code>list[PRAssisted]</code>, constraints: length <code>n</code> where \\(n = \\log_2(n2)\\) with <code>n2 = field_size ** 2</code> an exact power of two, shape N/A.</li> <li><code>_playerA</code>: <code>PRAssistedPlayerA | None</code>, constraints Unknown, shape N/A; cached instance created by <code>players()</code>.</li> <li><code>_playerB</code>: <code>PRAssistedPlayerB | None</code>, constraints Unknown, shape N/A; cached instance created by <code>players()</code>.</li> </ul>"},{"location":"pr_assisted_players/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pr_assisted_players/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pr_assisted_players/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li><code>_create_pr_assisted_array()</code> uses <code>np.log2(n2)</code> and casts to <code>int</code>, then verifies exactness via <code>2**n == n2</code>; keep this check if refactoring to avoid float rounding issues.</li> <li><code>shared_randomness()</code> prints directly; if changing deprecation behavior, ensure compatibility considerations are addressed across the package.</li> </ul>"},{"location":"pr_assisted_players/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.pr_assisted.PRAssisted</code></li> <li><code>Q_Sea_Battle.pr_assisted_player_a.PRAssistedPlayerA</code></li> <li><code>Q_Sea_Battle.pr_assisted_player_b.PRAssistedPlayerB</code></li> <li><code>Q_Sea_Battle.players_base.Players</code></li> <li><code>Q_Sea_Battle.game_layout.GameLayout</code></li> </ul>"},{"location":"pr_assisted_players/#changelog","title":"Changelog","text":"<ul> <li>Version 0.1: Introduced <code>PRAssistedPlayers</code> with <code>pr_assisted()</code> and deprecated alias <code>shared_randomness()</code>.</li> </ul>"},{"location":"pyr_combine_layer_a/","title":"PyrCombineLayerA","text":"<p>Role: Trainable Keras layer mapping <code>(field_batch, sr_outcome_batch)</code> to <code>next_field</code> logits for the next field.</p> <p>Location: <code>Q_Sea_Battle.pyr_combine_layer_a.PyrCombineLayerA</code></p>"},{"location":"pyr_combine_layer_a/#derived-constraints","title":"Derived constraints","text":"<ul> <li>Let \\(L\\) be the last dimension of <code>field_batch</code>; \\(L\\) must be statically known at build time and must be even, so that \\(L/2\\) is an integer.</li> <li>Let \\(B\\) be the batch dimension.</li> <li>At runtime, the last dimension of <code>sr_outcome_batch</code> must equal \\(L/2\\).</li> </ul>"},{"location":"pyr_combine_layer_a/#constructor","title":"Constructor","text":"Parameter Type Description hidden_units int, constraint \\(\\ge 1\\), scalar Number of hidden units in the internal Dense(ReLU) layer. name Optional[str], scalar Keras layer name; may be <code>None</code>. dtype Optional[tf.dtypes.DType], scalar Layer dtype; may be <code>None</code> (defaults are applied by Keras and <code>call()</code> conversions). **kwargs Any, variadic Forwarded to <code>tf.keras.layers.Layer</code> constructor. <p>Preconditions</p> <ul> <li><code>hidden_units</code> is an <code>int</code> with value \\(\\ge 1\\).</li> </ul> <p>Postconditions</p> <ul> <li><code>self.hidden_units</code> is set to <code>int(hidden_units)</code>.</li> <li>Sub-layers (<code>_dense_hidden</code>, <code>_dense_out</code>) are not created until <code>build()</code>.</li> </ul> <p>Errors</p> <ul> <li>Raises <code>ValueError</code> if <code>hidden_units &lt; 1</code>.</li> </ul> <p>Example</p> <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.pyr_combine_layer_a import PyrCombineLayerA\n\nlayer = PyrCombineLayerA(hidden_units=64, dtype=tf.float32)\n</code></pre>"},{"location":"pyr_combine_layer_a/#public-methods","title":"Public Methods","text":""},{"location":"pyr_combine_layer_a/#build","title":"build","text":"<p>Signature: <code>build(input_shape: Any) -&gt; None</code></p> <p>Parameters</p> <ul> <li><code>input_shape</code>: Any, constraint \"shape-like accepted by Keras\", scalar or nested; expected to describe <code>field_batch</code> as <code>(B, L)</code> and may also include a second shape for <code>sr_outcome_batch</code> that is ignored for building.</li> </ul> <p>Returns</p> <ul> <li><code>None</code>.</li> </ul> <p>Behavior</p> <ul> <li>Infers \\(L\\) from the last dimension of the field shape and computes <code>out_dim = L // 2</code>.</li> <li>Creates two Dense sublayers: a hidden Dense with <code>hidden_units</code> and ReLU activation, and an output Dense with <code>out_dim</code> and linear activation.</li> <li>Records <code>self._built_for_L = L</code>.</li> </ul> <p>Preconditions</p> <ul> <li>The last dimension \\(L\\) of the field shape is statically known (not <code>None</code>).</li> <li>\\(L\\) is even.</li> </ul> <p>Postconditions</p> <ul> <li><code>self._dense_hidden</code> and <code>self._dense_out</code> are non-<code>None</code> instances of <code>tf.keras.layers.Dense</code>.</li> <li><code>self._built_for_L</code> is set to <code>int(L)</code>.</li> </ul> <p>Errors</p> <ul> <li>Raises <code>ValueError</code> if the field shape last dimension is not statically known.</li> <li>Raises <code>ValueError</code> if \\(L\\) is not even.</li> </ul>"},{"location":"pyr_combine_layer_a/#call","title":"call","text":"<p>Signature: <code>call(field_batch: tf.Tensor, sr_outcome_batch: tf.Tensor, training: bool = False, **kwargs: Any) -&gt; tf.Tensor</code></p> <p>Parameters</p> <ul> <li><code>field_batch</code>: tf.Tensor, dtype convertible to <code>self.dtype</code> (or <code>tf.float32</code> if <code>self.dtype is None</code>), shape \\((B, L)\\), rank 2.</li> <li><code>sr_outcome_batch</code>: tf.Tensor, dtype convertible to <code>self.dtype</code> (or <code>tf.float32</code> if <code>self.dtype is None</code>), shape \\((B, L/2)\\), rank 2.</li> <li><code>training</code>: bool, scalar; forwarded to Dense sublayers.</li> <li><code>**kwargs</code>: Any, variadic; accepted but not used.</li> </ul> <p>Returns</p> <ul> <li><code>next_field</code>: tf.Tensor, dtype <code>self.dtype</code> if set else <code>tf.float32</code>, shape \\((B, L/2)\\).</li> </ul> <p>Behavior</p> <ul> <li>Converts inputs to tensors with dtype <code>self.dtype</code> (or <code>tf.float32</code> if unset).</li> <li>Validates both inputs are rank-2.</li> <li>Asserts at runtime that <code>sr_outcome_batch.shape[-1] == field_batch.shape[-1] // 2</code>.</li> <li>Concatenates inputs along the last axis to form shape \\((B, 3L/2)\\), applies hidden Dense(ReLU), then output Dense(linear) to produce logits.</li> </ul> <p>Preconditions</p> <ul> <li>Layer has been built so that <code>_dense_hidden</code> and <code>_dense_out</code> exist.</li> </ul> <p>Postconditions</p> <ul> <li>No state is created during the call.</li> </ul> <p>Errors</p> <ul> <li>Raises <code>ValueError</code> if either input is not rank-2 (when rank is statically known and not equal to 2).</li> <li>Raises a TensorFlow assertion error at runtime if <code>sr_outcome_batch</code> last dimension does not equal <code>field_batch</code> last dimension divided by 2.</li> <li>Raises <code>RuntimeError</code> if sublayers are missing (layer not built correctly).</li> </ul> <p>Example</p> <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.pyr_combine_layer_a import PyrCombineLayerA\n\nB, L = 4, 10\nfield = tf.random.uniform((B, L), dtype=tf.float32)\nsr = tf.random.uniform((B, L // 2), dtype=tf.float32)\n\nlayer = PyrCombineLayerA(hidden_units=32, dtype=tf.float32)\nlogits = layer(field, sr, training=True)\nassert logits.shape == (B, L // 2)\n</code></pre>"},{"location":"pyr_combine_layer_a/#get_config","title":"get_config","text":"<p>Signature: <code>get_config() -&gt; Dict[str, Any]</code></p> <p>Parameters</p> <ul> <li>None.</li> </ul> <p>Returns</p> <ul> <li><code>config</code>: Dict[str, Any], mapping; includes base layer config plus key <code>\"hidden_units\"</code> with value <code>int</code>.</li> </ul> <p>Behavior</p> <ul> <li>Extends <code>tf.keras.layers.Layer.get_config()</code> with <code>{\"hidden_units\": self.hidden_units}</code>.</li> </ul>"},{"location":"pyr_combine_layer_a/#data-state","title":"Data &amp; State","text":"<ul> <li><code>hidden_units</code>: int, constraint \\(\\ge 1\\), scalar; persisted in config.</li> <li><code>_dense_hidden</code>: Optional[tf.keras.layers.Dense], scalar; created in <code>build()</code>, otherwise <code>None</code>.</li> <li><code>_dense_out</code>: Optional[tf.keras.layers.Dense], scalar; created in <code>build()</code>, otherwise <code>None</code>.</li> <li><code>_built_for_L</code>: Optional[int], scalar; set to \\(L\\) in <code>build()</code>, otherwise <code>None</code>.</li> </ul>"},{"location":"pyr_combine_layer_a/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pyr_combine_layer_a/#deviations","title":"Deviations","text":"<ul> <li>The module docstring describes a contract that <code>sr_outcome_batch</code> has shape <code>(B, L/2)</code> and that <code>build()</code> can use only the field shape; the implementation matches this, but <code>build()</code> accepts multiple possible <code>input_shape</code> encodings and ignores SR shape even when provided.</li> </ul>"},{"location":"pyr_combine_layer_a/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>Keras may call <code>build()</code> with only the first input shape for multi-input layers; this implementation intentionally creates all state in <code>build()</code> from the field shape alone and validates SR shape at runtime in <code>call()</code>.</li> <li>Avoid creating new variables or sublayers in <code>call()</code>; sublayers are expected to be created in <code>build()</code>.</li> </ul>"},{"location":"pyr_combine_layer_a/#related","title":"Related","text":"<ul> <li>TensorFlow: <code>tf.keras.layers.Layer</code>, <code>tf.keras.layers.Dense</code></li> <li>Tensor ops: <code>tf.concat</code>, <code>tf.convert_to_tensor</code>, <code>tf.debugging.assert_equal</code></li> </ul>"},{"location":"pyr_combine_layer_a/#changelog","title":"Changelog","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pyr_combine_layer_b/","title":"PyrCombineLayerB","text":"<p>Role: Trainable Keras layer mapping (gun, sr_outcome, comm) -&gt; (next_gun logits, next_comm logits) for the next pyramid level.</p> <p>Location: <code>Q_Sea_Battle.pyr_combine_layer_b.PyrCombineLayerB</code></p>"},{"location":"pyr_combine_layer_b/#derived-constraints","title":"Derived constraints","text":"<ul> <li>Let \\(L\\) be the last dimension of <code>gun_batch</code> (the gun feature size). Constraint: \\(L\\) must be statically known at build time and even; the layer outputs gun logits with last dimension \\(L/2\\).</li> <li>All inputs to <code>call(...)</code> must be rank-2 tensors: shape \\((B, D)\\) for batch size \\(B\\) and feature dimension \\(D\\).</li> </ul>"},{"location":"pyr_combine_layer_b/#constructor","title":"Constructor","text":"Parameter Type Description hidden_units int, constraint \\(&gt;= 1\\) Width of the intermediate dense layer. name str or None, optional Keras layer name. dtype tf.dtypes.DType or None, optional Layer dtype; if None, <code>call()</code> converts inputs to <code>tf.float32</code> unless overridden elsewhere. **kwargs dict[str, Any] Forwarded to <code>tf.keras.layers.Layer</code> base constructor. <p>Preconditions</p> <ul> <li><code>hidden_units</code> must be an integer value \\(&gt;= 1\\).</li> </ul> <p>Postconditions</p> <ul> <li><code>self.hidden_units</code> is set to <code>int(hidden_units)</code>.</li> <li>Internal sublayers (<code>_dense_hidden</code>, <code>_dense_gun</code>, <code>_dense_comm</code>) are initialized to <code>None</code> and are created later in <code>build(...)</code>.</li> </ul> <p>Errors</p> <ul> <li>Raises <code>ValueError</code> if <code>hidden_units &lt; 1</code>.</li> </ul> <p>Example</p> <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.pyr_combine_layer_b import PyrCombineLayerB\n\nlayer = PyrCombineLayerB(hidden_units=64)\n</code></pre>"},{"location":"pyr_combine_layer_b/#public-methods","title":"Public Methods","text":""},{"location":"pyr_combine_layer_b/#buildinput_shape","title":"build(input_shape)","text":"<p>Creates sublayers based on the statically known gun dimension \\(L\\).</p> <p>Parameters</p> <ul> <li>input_shape: Any, constraints: must be convertible to <code>tf.TensorShape</code> and have a statically known last dimension \\(L\\).</li> </ul> <p>Returns</p> <ul> <li>None</li> </ul> <p>Preconditions</p> <ul> <li><code>input_shape</code> must have a statically known last dimension $L`.</li> <li>\\(L\\) must be even.</li> </ul> <p>Postconditions</p> <ul> <li>Creates the following sublayers (all with <code>dtype=self.dtype</code>): <code>dense_hidden</code> (Dense, units=<code>hidden_units</code>, activation=\"relu\"), <code>dense_gun</code> (Dense, units=\\(L/2\\), activation=None), <code>dense_comm</code> (Dense, units=1, activation=None).</li> <li>Stores <code>self._built_for_L = L</code>.</li> </ul> <p>Errors</p> <ul> <li>Raises <code>ValueError</code> if <code>gun_batch</code> last dimension \\(L\\) is not statically known.</li> <li>Raises <code>ValueError</code> if \\(L\\) is not even.</li> </ul> <p>Example</p> <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.pyr_combine_layer_b import PyrCombineLayerB\n\nlayer = PyrCombineLayerB(hidden_units=32)\nlayer.build((None, 8))  # L=8 -&gt; next_gun dim is 4\n</code></pre>"},{"location":"pyr_combine_layer_b/#callgun_batch-sr_outcome_batch-comm_batch-trainingfalse-kwargs","title":"call(gun_batch, sr_outcome_batch, comm_batch, training=False, **kwargs)","text":"<p>Forward pass combining gun, SR outcome, and communication bit into next-level logits.</p> <p>Parameters</p> <ul> <li>gun_batch: tf.Tensor, dtype float32 or <code>self.dtype</code>, shape (B, L), constraints: rank-2; last dim \\(L\\); \\(L\\) even (enforced at build time).</li> <li>sr_outcome_batch: tf.Tensor, dtype float32 or <code>self.dtype</code>, shape (B, L/2), constraints: rank-2; last dim must equal <code>tf.shape(gun_batch)[-1] // 2</code>.</li> <li>comm_batch: tf.Tensor, dtype float32 or <code>self.dtype</code>, shape (B, 1), constraints: rank-2; last dim must equal 1.</li> <li>training: bool, constraints: no additional constraints | Passed to sublayers.</li> <li>**kwargs: dict[str, Any] | Accepted but not otherwise specified by this implementation.</li> </ul> <p>Returns</p> <ul> <li>next_gun: tf.Tensor, dtype float32 or <code>self.dtype</code>, shape (B, L/2), constraints: logits (activation=None).</li> <li>next_comm: tf.Tensor, dtype float32 or <code>self.dtype</code>, shape (B, 1), constraints: logits (activation=None).</li> </ul> <p>Preconditions</p> <ul> <li>The layer must have been built such that internal sublayers exist (typically via first call with known input shapes or explicit <code>build(...)</code>).</li> <li>All three inputs must be rank-2 tensors.</li> </ul> <p>Postconditions</p> <ul> <li>Produces <code>next_gun</code> and <code>next_comm</code> by concatenating inputs along the last axis, applying a hidden Dense layer with ReLU, then projecting to two separate linear heads.</li> </ul> <p>Errors</p> <ul> <li>Raises <code>ValueError</code> if any input has known static rank not equal to 2.</li> <li>Raises <code>tf.errors.InvalidArgumentError</code> (via <code>tf.debugging.assert_equal</code>) if <code>sr_outcome_batch</code> last dim is not <code>L/2</code> at runtime.</li> <li>Raises <code>tf.errors.InvalidArgumentError</code> (via <code>tf.debugging.assert_equal</code>) if <code>comm_batch</code> last dim is not 1 at runtime.</li> <li>Raises <code>RuntimeError</code> if sublayers are missing (layer not built correctly).</li> </ul> <p>Example</p> <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.pyr_combine_layer_b import PyrCombineLayerB\n\nB, L = 4, 8\ngun = tf.zeros((B, L), dtype=tf.float32)\nsr = tf.zeros((B, L // 2), dtype=tf.float32)\ncomm = tf.zeros((B, 1), dtype=tf.float32)\n\nlayer = PyrCombineLayerB(hidden_units=64)\nnext_gun, next_comm = layer(gun, sr, comm, training=False)\nprint(next_gun.shape, next_comm.shape)  # (4, 4) (4, 1)\n</code></pre>"},{"location":"pyr_combine_layer_b/#get_config","title":"get_config()","text":"<p>Returns the Keras-serializable configuration.</p> <p>Parameters</p> <ul> <li>None</li> </ul> <p>Returns</p> <ul> <li>cfg: dict[str, Any], constraints: includes base layer config and key <code>\"hidden_units\"</code> with value <code>int</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.pyr_combine_layer_b import PyrCombineLayerB\n\nlayer = PyrCombineLayerB(hidden_units=16)\ncfg = layer.get_config()\nassert cfg[\"hidden_units\"] == 16\n</code></pre>"},{"location":"pyr_combine_layer_b/#data-state","title":"Data &amp; State","text":"<ul> <li>hidden_units: int, constraint \\(&gt;= 1\\) | Hyperparameter controlling hidden Dense width.</li> <li>_dense_hidden: tf.keras.layers.Dense or None | Created in <code>build(...)</code>; units=<code>hidden_units</code>, activation=\"relu\".</li> <li>_dense_gun: tf.keras.layers.Dense or None | Created in <code>build(...)</code>; units=\\(L/2\\), activation=None.</li> <li>_dense_comm: tf.keras.layers.Dense or None | Created in <code>build(...)</code>; units=1, activation=None.</li> <li>_built_for_L: int or None | Stores the gun dimension \\(L\\) used during <code>build(...)</code>.</li> </ul>"},{"location":"pyr_combine_layer_b/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pyr_combine_layer_b/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pyr_combine_layer_b/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>Do not create trainable state in <code>call(...)</code>; sublayers are expected to be created in <code>build(...)</code> based on the statically known gun dimension \\(L\\).</li> <li><code>_ensure_rank2(...)</code> validates rank only when the rank is statically known; dynamic-rank inputs may bypass this check and rely on downstream TensorFlow errors.</li> </ul>"},{"location":"pyr_combine_layer_b/#related","title":"Related","text":"<ul> <li>TensorFlow Keras <code>tf.keras.layers.Layer</code></li> <li>TensorFlow Keras <code>tf.keras.layers.Dense</code></li> </ul>"},{"location":"pyr_combine_layer_b/#changelog","title":"Changelog","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pyr_measurement_layer_a/","title":"PyrMeasurementLayerA","text":"<p>Role: Trainable Keras layer that maps a rank-2 field batch to rank-2 measurement probabilities with output dimension \\(L/2\\).</p> <p>Location: <code>Q_Sea_Battle.pyr_measurement_layer_a.PyrMeasurementLayerA</code></p>"},{"location":"pyr_measurement_layer_a/#derived-constraints","title":"Derived constraints","text":"<ul> <li>Let \\(L\\) be the last dimension of <code>field_batch</code>; \\(L\\) must be statically known at build time and must be even.</li> <li>Output last dimension is \\(L/2\\); output values are in \\([0, 1]\\) due to a sigmoid activation.</li> </ul>"},{"location":"pyr_measurement_layer_a/#constructor","title":"Constructor","text":"Parameter Type Description hidden_units int, constraint \\(\\ge 1\\), scalar Number of units in the hidden Dense layer. name str | None, scalar Layer name passed to <code>tf.keras.layers.Layer</code>. dtype tf.dtypes.DType | None, scalar Dtype for the layer and its sublayers; if <code>None</code>, call-time conversion defaults to <code>tf.float32</code>. **kwargs Any, variadic mapping Forwarded to <code>tf.keras.layers.Layer</code> constructor."},{"location":"pyr_measurement_layer_a/#preconditions","title":"Preconditions","text":"<ul> <li><code>hidden_units &gt;= 1</code>.</li> </ul>"},{"location":"pyr_measurement_layer_a/#postconditions","title":"Postconditions","text":"<ul> <li>The instance is created with <code>trainable=True</code>.</li> <li>The sublayers are not created until <code>build()</code> is called; internal sublayer references are initialized to <code>None</code>.</li> </ul>"},{"location":"pyr_measurement_layer_a/#errors","title":"Errors","text":"<ul> <li><code>ValueError</code>: if <code>hidden_units &lt; 1</code>.</li> </ul>"},{"location":"pyr_measurement_layer_a/#example","title":"Example","text":"<p>Constructing the layer</p> <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.pyr_measurement_layer_a import PyrMeasurementLayerA\n\nlayer = PyrMeasurementLayerA(hidden_units=64, dtype=tf.float32)\n\nx = tf.zeros([8, 10], dtype=tf.float32)  # B=8, L=10 (even)\ny = layer(x, training=False)\nprint(y.shape)  # (8, 5)\n</code></pre>"},{"location":"pyr_measurement_layer_a/#public-methods","title":"Public Methods","text":""},{"location":"pyr_measurement_layer_a/#build","title":"build","text":"<ul> <li>Signature: <code>build(input_shape: Any) -&gt; None</code></li> </ul> <p>Parameters:</p> <ul> <li><code>input_shape</code>: Any, constraint: convertible to <code>tf.TensorShape</code>, shape: not specified; expected to describe an input with statically known last dimension \\(L\\).</li> </ul> <p>Returns:</p> <ul> <li><code>None</code>.</li> </ul> <p>Preconditions:</p> <ul> <li>The last dimension \\(L\\) of <code>input_shape</code> is statically known (<code>shape[-1] is not None</code>).</li> <li>\\(L\\) is even.</li> </ul> <p>Postconditions:</p> <ul> <li>Creates two sublayers:</li> <li><code>Dense(hidden_units, activation=\"relu\", name=\"dense_hidden\")</code></li> <li><code>Dense(L/2, activation=\"sigmoid\", name=\"dense_out\")</code></li> <li>Records <code>_built_for_L = L</code>.</li> </ul> <p>Errors:</p> <ul> <li><code>ValueError</code>: if the last dimension is unknown at build time.</li> <li><code>ValueError</code>: if \\(L\\) is not even.</li> </ul>"},{"location":"pyr_measurement_layer_a/#call","title":"call","text":"<ul> <li>Signature: <code>call(field_batch: tf.Tensor, training: bool = False, **kwargs: Any) -&gt; tf.Tensor</code></li> </ul> <p>Parameters:</p> <ul> <li><code>field_batch</code>: tf.Tensor, dtype float32 (or <code>self.dtype</code> if set), shape (B, L); constraint: rank must be 2 and \\(L\\) must be even.</li> <li><code>training</code>: bool, scalar; forwarded to the Dense sublayers.</li> <li><code>**kwargs</code>: Any, variadic mapping; accepted but not used by this implementation.</li> </ul> <p>Returns:</p> <ul> <li><code>meas_a</code>: tf.Tensor, dtype float32 (or <code>self.dtype</code> if set), shape (B, L/2); constraint: values in \\([0, 1]\\) (sigmoid output).</li> </ul> <p>Preconditions:</p> <ul> <li><code>field_batch</code> must be rank-2.</li> <li>The last dimension \\(L\\) must be even.</li> <li>The layer must have been built such that internal sublayers exist.</li> </ul> <p>Postconditions:</p> <ul> <li>Produces measurement probabilities via an MLP: hidden ReLU Dense followed by sigmoid Dense.</li> </ul> <p>Errors:</p> <ul> <li><code>ValueError</code>: if <code>field_batch</code> has a statically known rank that is not 2.</li> <li><code>tf.errors.InvalidArgumentError</code> (or framework equivalent): if <code>tf.shape(field_batch)[-1] % 2 != 0</code> at runtime due to <code>tf.debugging.assert_equal</code>.</li> <li><code>RuntimeError</code>: if internal sublayers are missing (layer not built correctly).</li> </ul>"},{"location":"pyr_measurement_layer_a/#get_config","title":"get_config","text":"<ul> <li>Signature: <code>get_config() -&gt; Dict[str, Any]</code></li> </ul> <p>Parameters:</p> <ul> <li>None.</li> </ul> <p>Returns:</p> <ul> <li><code>Dict[str, Any]</code>, mapping containing the base Keras layer config plus <code>{\"hidden_units\": int}</code>.</li> </ul>"},{"location":"pyr_measurement_layer_a/#data-state","title":"Data &amp; State","text":"<ul> <li><code>hidden_units</code>: int, constraint \\(\\ge 1\\), scalar; stored constructor hyperparameter.</li> <li><code>_dense_hidden</code>: tf.keras.layers.Dense | None; created in <code>build()</code>, otherwise <code>None</code>.</li> <li><code>_dense_out</code>: tf.keras.layers.Dense | None; created in <code>build()</code>, otherwise <code>None</code>.</li> <li><code>_built_for_L</code>: int | None; the input last dimension \\(L\\) the layer was built for, set in <code>build()</code>.</li> </ul>"},{"location":"pyr_measurement_layer_a/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pyr_measurement_layer_a/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pyr_measurement_layer_a/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>Sublayers must be created in <code>build()</code> (not in <code>call()</code>), and <code>call()</code> must not create state.</li> <li>The output is probabilities (sigmoid), not logits, to satisfy downstream validation of outcomes in \\([0, 1]\\).</li> <li>The contract requires <code>field_batch</code> shape (B, L) and output shape (B, L/2); ensure any future changes preserve this public API.</li> </ul>"},{"location":"pyr_measurement_layer_a/#related","title":"Related","text":"<ul> <li><code>tf.keras.layers.Layer</code></li> <li><code>tf.keras.layers.Dense</code></li> <li><code>tf.debugging.assert_equal</code></li> </ul>"},{"location":"pyr_measurement_layer_a/#changelog","title":"Changelog","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pyr_measurement_layer_b/","title":"PyrMeasurementLayerB","text":"<p>Role: Trainable Keras layer mapping a gun state tensor to measurement probabilities constrained to \\([0,1]\\) via a sigmoid output head.</p> <p>Location: <code>Q_Sea_Battle.pyr_measurement_layer_b.PyrMeasurementLayerB</code></p>"},{"location":"pyr_measurement_layer_b/#derived-constraints","title":"Derived constraints","text":"<ul> <li>Let \\(L\\) be the last dimension of <code>gun_batch</code>; \\(L\\) must be statically known at build time and must be even.</li> <li>Let \\(B\\) be the batch size (dynamic).</li> <li>Output last dimension is \\(L/2\\).</li> </ul>"},{"location":"pyr_measurement_layer_b/#constructor","title":"Constructor","text":"Parameter Type Description hidden_units int, constraint \\(\\ge 1\\) Number of units in the hidden Dense layer. name Optional[str], default None Layer name passed to the Keras base class. dtype Optional[tf.dtypes.DType], default None Layer dtype; used for internal tensor conversion and Dense sublayers. **kwargs Any Additional keyword arguments forwarded to <code>tf.keras.layers.Layer</code>. <p>Preconditions</p> <ul> <li><code>hidden_units &gt;= 1</code>.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.hidden_units</code> is set to <code>int(hidden_units)</code>.</li> <li>Sublayers <code>_dense_hidden</code> and <code>_dense_out</code> are initialized to <code>None</code> and are created later in <code>build()</code>.</li> </ul> <p>Errors</p> <ul> <li><code>ValueError</code>: if <code>hidden_units &lt; 1</code>.</li> </ul> <p>Example</p> <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.pyr_measurement_layer_b import PyrMeasurementLayerB\n\nlayer = PyrMeasurementLayerB(hidden_units=64, dtype=tf.float32)\nx = tf.random.uniform(shape=(8, 10), dtype=tf.float32)  # L=10 is even\ny = layer(x, training=True)\nprint(y.shape)  # (8, 5)\n</code></pre>"},{"location":"pyr_measurement_layer_b/#public-methods","title":"Public Methods","text":""},{"location":"pyr_measurement_layer_b/#build","title":"build","text":"<ul> <li>Signature: <code>build(input_shape: Any) -&gt; None</code></li> </ul> <p>Parameters</p> <ul> <li>input_shape: Any, constraints: convertible to <code>tf.TensorShape</code>; shape must have a statically known last dimension \\(L\\) and \\(L\\) must be even.</li> </ul> <p>Returns</p> <ul> <li>None: NoneType, no value returned.</li> </ul> <p>Behavior</p> <ul> <li>Creates two Dense sublayers based on the inferred gun dimension \\(L\\) from <code>input_shape</code>:</li> <li>Hidden layer: <code>Dense(self.hidden_units, activation=\"relu\")</code></li> <li>Output layer: <code>Dense(out_dim, activation=\"sigmoid\")</code> where <code>out_dim = L // 2</code></li> <li>Records <code>self._built_for_L = L</code>.</li> </ul> <p>Errors</p> <ul> <li><code>ValueError</code>: if the last dimension of <code>input_shape</code> is <code>None</code> (not statically known).</li> <li><code>ValueError</code>: if \\(L\\) is odd (so \\(L/2\\) is not an integer).</li> </ul>"},{"location":"pyr_measurement_layer_b/#call","title":"call","text":"<ul> <li>Signature: <code>call(gun_batch: tf.Tensor, training: bool = False, **kwargs: Any) -&gt; tf.Tensor</code></li> </ul> <p>Parameters</p> <ul> <li>gun_batch: tf.Tensor, dtype float32 or <code>self.dtype</code> (via conversion), shape \\((B, L)\\); constraints: rank must be 2; last dimension \\(L\\) must be even (checked at runtime).</li> <li>training: bool, default False; forwarded to internal Dense sublayers.</li> <li>**kwargs: Any; accepted but not specified/used directly in the implementation.</li> </ul> <p>Returns</p> <ul> <li>meas_b: tf.Tensor, dtype float32 or <code>self.dtype</code>, shape \\((B, L/2)\\); constraints: values in \\([0, 1]\\) due to sigmoid activation.</li> </ul> <p>Errors</p> <ul> <li><code>ValueError</code>: if <code>gun_batch</code> has statically known rank not equal to 2.</li> <li><code>tf.errors.InvalidArgumentError</code>: if runtime check finds \\(L\\) is not even (from <code>tf.debugging.assert_equal</code> on <code>tf.shape(x)[-1] % 2</code>).</li> <li><code>RuntimeError</code>: if the layer is not built correctly (either <code>_dense_hidden</code> or <code>_dense_out</code> is <code>None</code>).</li> </ul>"},{"location":"pyr_measurement_layer_b/#get_config","title":"get_config","text":"<ul> <li>Signature: <code>get_config() -&gt; Dict[str, Any]</code></li> </ul> <p>Parameters</p> <ul> <li>None.</li> </ul> <p>Returns</p> <ul> <li>cfg: Dict[str, Any], constraints: includes base layer config plus key <code>\"hidden_units\"</code> with an int value.</li> </ul>"},{"location":"pyr_measurement_layer_b/#data-state","title":"Data &amp; State","text":"<ul> <li>hidden_units: int, constraint \\(\\ge 1\\); persisted in config via <code>get_config()</code>.</li> <li>_dense_hidden: Optional[tf.keras.layers.Dense], initialized to None; created in <code>build()</code> with <code>activation=\"relu\"</code>.</li> <li>_dense_out: Optional[tf.keras.layers.Dense], initialized to None; created in <code>build()</code> with <code>activation=\"sigmoid\"</code> and units \\(L/2\\).</li> <li>_built_for_L: Optional[int], initialized to None; set to \\(L\\) in <code>build()</code>.</li> </ul>"},{"location":"pyr_measurement_layer_b/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pyr_measurement_layer_b/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pyr_measurement_layer_b/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>This layer enforces probability outputs via a sigmoid head; avoid changing the public contract <code>call(gun_batch) -&gt; meas_b</code> and the \\([0,1]\\) output constraint.</li> <li>Do not create state in <code>call()</code>; sublayers are created in <code>build()</code> based on the statically known last dimension \\(L\\).</li> </ul>"},{"location":"pyr_measurement_layer_b/#related","title":"Related","text":"<ul> <li><code>tf.keras.layers.Layer</code></li> <li><code>tf.keras.layers.Dense</code></li> </ul>"},{"location":"pyr_measurement_layer_b/#changelog","title":"Changelog","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pyr_teacher_layers/","title":"Q_Sea_Battle.pyr_teacher_layers","text":"<p>Role: Re-export module for pyramid (Pyr) teacher layer primitives used as non-trainable teacher rules for pyramid imitation-learning dataset generation.</p> <p>Location: <code>Q_Sea_Battle.pyr_teacher_layers</code></p>"},{"location":"pyr_teacher_layers/#overview","title":"Overview","text":"<p>This module provides a single import surface for the pyramid teacher layer primitives used to define teacher rules for pyramid imitation-learning dataset generation. It re-exports the following symbols from sibling modules: <code>PyrMeasurementLayerA</code>, <code>PyrMeasurementLayerB</code>, <code>PyrCombineLayerA</code>, and <code>PyrCombineLayerB</code>.</p> <p>Terminology (as defined by this module docstring): SR refers to a shared resource available to both players without communication; \"Shared randomness\" is not used in this project's terminology.</p>"},{"location":"pyr_teacher_layers/#public-api","title":"Public API","text":""},{"location":"pyr_teacher_layers/#functions","title":"Functions","text":"<p>Not specified.</p>"},{"location":"pyr_teacher_layers/#constants","title":"Constants","text":"<ul> <li><code>__all__</code>: List of public re-exports: <code>[\"PyrMeasurementLayerA\", \"PyrMeasurementLayerB\", \"PyrCombineLayerA\", \"PyrCombineLayerB\"]</code>.</li> </ul>"},{"location":"pyr_teacher_layers/#types","title":"Types","text":"<p>Not specified.</p>"},{"location":"pyr_teacher_layers/#dependencies","title":"Dependencies","text":"<ul> <li>Imports (relative): <code>.pyr_measurement_layer_a.PyrMeasurementLayerA</code>, <code>.pyr_measurement_layer_b.PyrMeasurementLayerB</code>, <code>.pyr_combine_layer_a.PyrCombineLayerA</code>, <code>.pyr_combine_layer_b.PyrCombineLayerB</code>.</li> </ul>"},{"location":"pyr_teacher_layers/#planned-design-spec","title":"Planned (design-spec)","text":"<p>Not specified.</p>"},{"location":"pyr_teacher_layers/#deviations","title":"Deviations","text":"<p>Not specified.</p>"},{"location":"pyr_teacher_layers/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>This module is a re-export layer; changes here should generally be limited to updating imports and <code>__all__</code> to reflect the intended public surface.  </li> <li>The implementation details, signatures, and behavior of the exported classes are defined in their respective modules (not shown here).</li> </ul>"},{"location":"pyr_teacher_layers/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.pyr_measurement_layer_a</code></li> <li><code>Q_Sea_Battle.pyr_measurement_layer_b</code></li> <li><code>Q_Sea_Battle.pyr_combine_layer_a</code></li> <li><code>Q_Sea_Battle.pyr_combine_layer_b</code></li> </ul>"},{"location":"pyr_teacher_layers/#changelog","title":"Changelog","text":"<ul> <li>Version: 0.1 (as stated in module docstring).</li> </ul>"},{"location":"pyr_trainable_assisted_imitation_utilities/","title":"Q_Sea_Battle.pyr_trainable_assisted_imitation_utilities","text":"<p>Role: Per-level (level-aware) dataset generation and training/weight-transfer utilities for pyramid trainable-assisted imitation models; produces dict-of-arrays datasets and optional TensorFlow <code>tf.data.Dataset</code> wrappers.</p> <p>Location: <code>Q_Sea_Battle.pyr_trainable_assisted_imitation_utilities</code></p>"},{"location":"pyr_trainable_assisted_imitation_utilities/#overview","title":"Overview","text":"<p>This module provides utilities to generate supervised training datasets for pyramid-style trainable-assisted models, where each pyramid level maps an input vector of length <code>L</code> to outputs of length <code>L/2</code> (with <code>L</code> a power of two and <code>L &gt;= 2</code>). It includes dataset generators for \u201cmeasurement\u201d and \u201ccombine\u201d tasks for model variants A and B, helper conversion to <code>tf.data.Dataset</code>, a wrapper for training a Keras layer standalone, and utilities to transfer per-level trained weights back into a multi-level model.</p> <p>TensorFlow is optional at import time; functions requiring TensorFlow will raise <code>ModuleNotFoundError</code> if TensorFlow is not available.</p>"},{"location":"pyr_trainable_assisted_imitation_utilities/#public-api","title":"Public API","text":""},{"location":"pyr_trainable_assisted_imitation_utilities/#functions","title":"Functions","text":""},{"location":"pyr_trainable_assisted_imitation_utilities/#pyramid_levels","title":"<code>pyramid_levels</code>","text":"<p>Signature: <code>pyramid_levels(field_size: int) -&gt; List[int]</code></p> <p>Purpose: Compute the input sizes per pyramid level for a power-of-two field size.</p> <p>Arguments: - <code>field_size</code> (<code>int</code>): Initial size <code>N</code> (must be power of two, and <code>&gt;= 2</code>).</p> <p>Returns: - <code>List[int]</code>: Input sizes per level, e.g. <code>16 -&gt; [16, 8, 4, 2]</code>.</p> <p>Errors: - <code>ValueError</code>: If <code>field_size &lt; 2</code>. - <code>ValueError</code>: If <code>field_size</code> is not a power of two.</p> <p>Example: <pre><code>from Q_Sea_Battle.pyr_trainable_assisted_imitation_utilities import pyramid_levels\n\nlevels = pyramid_levels(16)  # [16, 8, 4, 2]\n</code></pre></p>"},{"location":"pyr_trainable_assisted_imitation_utilities/#generate_measurement_dataset_a","title":"<code>generate_measurement_dataset_a</code>","text":"<p>Signature: <code>generate_measurement_dataset_a(L: int, num_samples: int, seed: int = 0) -&gt; Dict[str, np.ndarray]</code></p> <p>Purpose: Generate a per-level dataset for \u201cmeasurement A\u201d targets.</p> <p>Arguments: - <code>L</code> (<code>int</code>): Level input length (must be power of two, and <code>&gt;= 2</code>). - <code>num_samples</code> (<code>int</code>): Number of samples to generate. - <code>seed</code> (<code>int</code>, default: <code>0</code>): Random seed.</p> <p>Returns: - <code>Dict[str, np.ndarray]</code>: Dict-of-arrays with keys:   - <code>field</code>: shape <code>(num_samples, L)</code>, dtype <code>np.float32</code>, values in <code>{0.0, 1.0}</code>.   - <code>meas_target</code>: shape <code>(num_samples, L//2)</code>, dtype <code>np.float32</code>, values in <code>{0.0, 1.0}</code>.</p> <p>Errors: - <code>ValueError</code>: If <code>L &lt; 2</code>. - <code>ValueError</code>: If <code>L</code> is not a power of two.</p> <p>Example: <pre><code>from Q_Sea_Battle.pyr_trainable_assisted_imitation_utilities import generate_measurement_dataset_a\n\nds = generate_measurement_dataset_a(L=8, num_samples=1024, seed=1)\n# ds[\"field\"].shape == (1024, 8)\n# ds[\"meas_target\"].shape == (1024, 4)\n</code></pre></p>"},{"location":"pyr_trainable_assisted_imitation_utilities/#generate_combine_dataset_a","title":"<code>generate_combine_dataset_a</code>","text":"<p>Signature: <code>generate_combine_dataset_a(L: int, num_samples: int, seed: int = 0) -&gt; Dict[str, np.ndarray]</code></p> <p>Purpose: Generate a per-level dataset for \u201ccombine A\u201d targets.</p> <p>Arguments: - <code>L</code> (<code>int</code>): Level input length (must be power of two, and <code>&gt;= 2</code>). - <code>num_samples</code> (<code>int</code>): Number of samples to generate. - <code>seed</code> (<code>int</code>, default: <code>0</code>): Random seed (also influences SR sampling via <code>seed + 12345</code>).</p> <p>Returns: - <code>Dict[str, np.ndarray]</code>: Dict-of-arrays with keys:   - <code>field</code>: shape <code>(num_samples, L)</code>, dtype <code>np.float32</code>, values in <code>{0.0, 1.0}</code>.   - <code>sr_outcome</code>: shape <code>(num_samples, L//2)</code>, dtype <code>np.float32</code>, values in <code>{0.0, 1.0}</code>.   - <code>next_field_target</code>: shape <code>(num_samples, L//2)</code>, dtype <code>np.float32</code>, values in <code>{0.0, 1.0}</code>.</p> <p>Errors: - <code>ValueError</code>: If <code>L &lt; 2</code>. - <code>ValueError</code>: If <code>L</code> is not a power of two.</p> <p>Example: <pre><code>from Q_Sea_Battle.pyr_trainable_assisted_imitation_utilities import generate_combine_dataset_a\n\nds = generate_combine_dataset_a(L=8, num_samples=512, seed=0)\n# ds[\"next_field_target\"].shape == (512, 4)\n</code></pre></p>"},{"location":"pyr_trainable_assisted_imitation_utilities/#generate_measurement_dataset_b","title":"<code>generate_measurement_dataset_b</code>","text":"<p>Signature: <code>generate_measurement_dataset_b(L: int, num_samples: int, seed: int = 0)</code></p> <p>Purpose: Generate a per-level dataset for \u201cmeasurement B\u201d targets, using one-hot <code>gun</code> vectors to match the game/inference setting.</p> <p>Arguments: - <code>L</code> (<code>int</code>): Level input length. Constraints are not validated in this function (not specified), but other utilities assume <code>L</code> is a power of two and <code>&gt;= 2</code>. - <code>num_samples</code> (<code>int</code>): Number of samples to generate. - <code>seed</code> (<code>int</code>, default: <code>0</code>): Random seed.</p> <p>Returns: - <code>dict</code>: Dict-of-arrays with keys:   - <code>gun</code>: shape <code>(num_samples, L)</code>, one-hot, dtype <code>np.float32</code>.   - <code>meas_target</code>: shape <code>(num_samples, L//2)</code>, dtype <code>np.float32</code>, values in <code>{0.0, 1.0}</code> computed per-sample.</p> <p>Errors: - Not specified. (No explicit validation; NumPy may raise on invalid shapes.)</p> <p>Example: <pre><code>from Q_Sea_Battle.pyr_trainable_assisted_imitation_utilities import generate_measurement_dataset_b\n\nds = generate_measurement_dataset_b(L=8, num_samples=128, seed=42)\n# ds[\"gun\"].shape == (128, 8)\n# ds[\"meas_target\"].shape == (128, 4)\n</code></pre></p>"},{"location":"pyr_trainable_assisted_imitation_utilities/#generate_combine_dataset_b","title":"<code>generate_combine_dataset_b</code>","text":"<p>Signature: <code>generate_combine_dataset_b(L: int, num_samples: int, seed: int = 0)</code></p> <p>Purpose: Generate a per-level dataset for \u201ccombine B\u201d targets, using one-hot <code>gun</code>, binary SR outcomes, and a binary <code>comm</code> bit.</p> <p>Arguments: - <code>L</code> (<code>int</code>): Level input length. Constraints are not validated in this function (not specified), but target shapes assume <code>L//2</code> exists. - <code>num_samples</code> (<code>int</code>): Number of samples to generate. - <code>seed</code> (<code>int</code>, default: <code>0</code>): Random seed.</p> <p>Returns: - <code>dict</code>: Dict-of-arrays with keys:   - <code>gun</code>: shape <code>(num_samples, L)</code>, one-hot, dtype <code>np.float32</code>.   - <code>sr_outcome</code>: shape <code>(num_samples, L//2)</code>, dtype <code>np.float32</code>, values in <code>{0.0, 1.0}</code>.   - <code>comm</code>: shape <code>(num_samples, 1)</code>, dtype <code>np.float32</code>, values in <code>{0.0, 1.0}</code>.   - <code>next_gun_target</code>: shape <code>(num_samples, L//2)</code>, dtype <code>np.float32</code>.   - <code>next_comm_target</code>: shape <code>(num_samples, 1)</code>, dtype <code>np.float32</code>.</p> <p>Errors: - Not specified. (No explicit validation; NumPy may raise on invalid shapes.)</p> <p>Example: <pre><code>from Q_Sea_Battle.pyr_trainable_assisted_imitation_utilities import generate_combine_dataset_b\n\nds = generate_combine_dataset_b(L=8, num_samples=256, seed=7)\n# ds[\"next_gun_target\"].shape == (256, 4)\n# ds[\"next_comm_target\"].shape == (256, 1)\n</code></pre></p>"},{"location":"pyr_trainable_assisted_imitation_utilities/#to_tf_dataset","title":"<code>to_tf_dataset</code>","text":"<p>Signature: <code>to_tf_dataset(ds: Mapping[str, np.ndarray], x_keys: Sequence[str], y_key: str, batch_size: int, shuffle: bool = True, seed: int = 0) -&gt; \"tf.data.Dataset\"</code></p> <p>Purpose: Convert a dict-of-NumPy-arrays dataset into a <code>tf.data.Dataset</code> yielding <code>(x, y)</code>, where <code>x</code> is either a single tensor or a tuple of tensors.</p> <p>Arguments: - <code>ds</code> (<code>Mapping[str, np.ndarray]</code>): Dict-like dataset of arrays. - <code>x_keys</code> (<code>Sequence[str]</code>): Keys from <code>ds</code> to use as model inputs. - <code>y_key</code> (<code>str</code>): Key from <code>ds</code> to use as the target. - <code>batch_size</code> (<code>int</code>): Batch size for <code>.batch()</code>. - <code>shuffle</code> (<code>bool</code>, default: <code>True</code>): Whether to shuffle. - <code>seed</code> (<code>int</code>, default: <code>0</code>): Shuffle seed.</p> <p>Returns: - <code>tf.data.Dataset</code>: A batched, prefetched dataset. Prefetch uses <code>tf.data.AUTOTUNE</code>.</p> <p>Errors: - <code>ModuleNotFoundError</code>: If TensorFlow is required but not available. - <code>KeyError</code>: If keys are missing in <code>ds</code> (raised by dict access). - Other errors: Not specified (TensorFlow/NumPy may raise for incompatible shapes).</p> <p>Example: <pre><code>from Q_Sea_Battle.pyr_trainable_assisted_imitation_utilities import generate_measurement_dataset_a, to_tf_dataset\n\nnp_ds = generate_measurement_dataset_a(L=8, num_samples=1024, seed=0)\ntf_ds = to_tf_dataset(np_ds, x_keys=[\"field\"], y_key=\"meas_target\", batch_size=32, shuffle=True, seed=0)\n</code></pre></p>"},{"location":"pyr_trainable_assisted_imitation_utilities/#train_layer","title":"<code>train_layer</code>","text":"<p>Signature: <code>train_layer(layer: Any, ds: \"tf.data.Dataset\", loss: Any, epochs: int, metrics: Optional[Sequence[Any]] = None, verbose: int = 1) -&gt; \"tf.keras.Model\"</code></p> <p>Purpose: Train a Keras layer as a standalone model by wrapping it in a <code>tf.keras.Model</code> with inferred input signatures from the dataset.</p> <p>Arguments: - <code>layer</code> (<code>Any</code>): A callable Keras layer (or similar) supporting <code>layer(inp)</code> or <code>layer(*inputs)</code>. - <code>ds</code> (<code>tf.data.Dataset</code>): Dataset yielding <code>(x, y)</code>, where <code>x</code> is a tensor or a tuple/list of tensors. - <code>loss</code> (<code>Any</code>): Loss passed to <code>model.compile(loss=...)</code>. - <code>epochs</code> (<code>int</code>): Number of epochs. - <code>metrics</code> (<code>Optional[Sequence[Any]]</code>, default: <code>None</code>): Metrics passed to <code>model.compile(metrics=...)</code>. - <code>verbose</code> (<code>int</code>, default: <code>1</code>): Verbosity passed to <code>model.fit(...)</code>.</p> <p>Returns: - <code>tf.keras.Model</code>: The compiled and fitted wrapper model.</p> <p>Errors: - <code>ModuleNotFoundError</code>: If TensorFlow is required but not available. - <code>StopIteration</code>: If <code>ds</code> is empty (when sampling <code>next(iter(ds.take(1)))</code>). - Other errors: Not specified (TensorFlow/Keras may raise for incompatible shapes/types).</p> <p>Example: <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.pyr_trainable_assisted_imitation_utilities import generate_measurement_dataset_a, to_tf_dataset, train_layer\n\nnp_ds = generate_measurement_dataset_a(L=8, num_samples=1024, seed=0)\nds = to_tf_dataset(np_ds, x_keys=[\"field\"], y_key=\"meas_target\", batch_size=32)\n\nlayer = tf.keras.layers.Dense(4, activation=\"sigmoid\")\nmodel = train_layer(layer, ds=ds, loss=\"binary_crossentropy\", epochs=3, metrics=[tf.keras.metrics.BinaryAccuracy()])\n</code></pre></p>"},{"location":"pyr_trainable_assisted_imitation_utilities/#transfer_pyr_model_a_layer_weights","title":"<code>transfer_pyr_model_a_layer_weights</code>","text":"<p>Signature: <code>transfer_pyr_model_a_layer_weights(model_a: Any, measure_layers_a: Sequence[Any], combine_layers_a: Sequence[Any]) -&gt; None</code></p> <p>Purpose: Copy per-level trained weights into a Model A instance that exposes <code>measure_layers</code> and <code>combine_layers</code> sequences.</p> <p>Arguments: - <code>model_a</code> (<code>Any</code>): Destination model; must have attributes <code>measure_layers</code> and <code>combine_layers</code> (both sequences). - <code>measure_layers_a</code> (<code>Sequence[Any]</code>): Source per-level measurement layers to copy from. - <code>combine_layers_a</code> (<code>Sequence[Any]</code>): Source per-level combine layers to copy from.</p> <p>Returns: - <code>None</code></p> <p>Errors: - <code>ValueError</code>: If <code>model_a</code> does not have required attributes. - <code>ValueError</code>: If <code>measure_layers_a</code> length does not match <code>len(model_a.measure_layers)</code>. - <code>ValueError</code>: If <code>combine_layers_a</code> length does not match <code>len(model_a.combine_layers)</code>. - <code>ValueError</code>: If <code>len(model_a.measure_layers) != len(model_a.combine_layers)</code>.</p> <p>Example: <pre><code>from Q_Sea_Battle.pyr_trainable_assisted_imitation_utilities import transfer_pyr_model_a_layer_weights\n\n# model_a must have .measure_layers and .combine_layers, and each element must support get_weights()/set_weights()\ntransfer_pyr_model_a_layer_weights(model_a, measure_layers_a=trained_meas_layers, combine_layers_a=trained_comb_layers)\n</code></pre></p>"},{"location":"pyr_trainable_assisted_imitation_utilities/#transfer_pyr_model_b_layer_weights","title":"<code>transfer_pyr_model_b_layer_weights</code>","text":"<p>Signature: <code>transfer_pyr_model_b_layer_weights(model_b: Any, measure_layers_b: Sequence[Any], combine_layers_b: Sequence[Any]) -&gt; None</code></p> <p>Purpose: Copy per-level trained weights into a Model B instance that exposes <code>measure_layers</code> and <code>combine_layers</code> sequences.</p> <p>Arguments: - <code>model_b</code> (<code>Any</code>): Destination model; must have attributes <code>measure_layers</code> and <code>combine_layers</code> (both sequences). - <code>measure_layers_b</code> (<code>Sequence[Any]</code>): Source per-level measurement layers to copy from. - <code>combine_layers_b</code> (<code>Sequence[Any]</code>): Source per-level combine layers to copy from.</p> <p>Returns: - <code>None</code></p> <p>Errors: - <code>ValueError</code>: If <code>model_b</code> does not have required attributes. - <code>ValueError</code>: If <code>measure_layers_b</code> length does not match <code>len(model_b.measure_layers)</code>. - <code>ValueError</code>: If <code>combine_layers_b</code> length does not match <code>len(model_b.combine_layers)</code>. - <code>ValueError</code>: If <code>len(model_b.measure_layers) != len(model_b.combine_layers)</code>.</p> <p>Example: <pre><code>from Q_Sea_Battle.pyr_trainable_assisted_imitation_utilities import transfer_pyr_model_b_layer_weights\n\ntransfer_pyr_model_b_layer_weights(model_b, measure_layers_b=trained_meas_layers, combine_layers_b=trained_comb_layers)\n</code></pre></p>"},{"location":"pyr_trainable_assisted_imitation_utilities/#constants","title":"Constants","text":"<p>Not specified.</p>"},{"location":"pyr_trainable_assisted_imitation_utilities/#types","title":"Types","text":""},{"location":"pyr_trainable_assisted_imitation_utilities/#arraylike","title":"<code>ArrayLike</code>","text":"<p>Definition: <code>ArrayLike = Union[np.ndarray, \"tf.Tensor\"]</code></p> <p>Description: Convenience union type representing either a NumPy array or a TensorFlow tensor (as a forward reference).</p>"},{"location":"pyr_trainable_assisted_imitation_utilities/#dependencies","title":"Dependencies","text":"<ul> <li><code>numpy</code> (imported as <code>np</code>)</li> <li><code>tensorflow</code> (optional; imported as <code>tf</code> if available; required for <code>to_tf_dataset</code> and <code>train_layer</code>)</li> <li>Standard library: <code>typing</code> (<code>Any</code>, <code>Dict</code>, <code>List</code>, <code>Mapping</code>, <code>Optional</code>, <code>Sequence</code>, <code>Tuple</code>, <code>Union</code>)</li> </ul>"},{"location":"pyr_trainable_assisted_imitation_utilities/#planned-design-spec","title":"Planned (design-spec)","text":"<p>Not specified.</p>"},{"location":"pyr_trainable_assisted_imitation_utilities/#deviations","title":"Deviations","text":"<ul> <li>Several dataset generators validate that <code>L</code> is a power of two (<code>generate_measurement_dataset_a</code>, <code>generate_combine_dataset_a</code>) via a shared checker, while the B-variant generators do not perform the same validation (constraints are implied by shape math but not enforced).</li> <li>TensorFlow is treated as an optional dependency at import time; TensorFlow-required functions raise at call time rather than import time.</li> </ul>"},{"location":"pyr_trainable_assisted_imitation_utilities/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>Keep outputs as <code>np.float32</code> with binary values <code>{0.0, 1.0}</code> to match existing generators and teachers.</li> <li>If adding new TensorFlow-dependent utilities, follow the existing pattern of <code>_require_tf()</code> to provide a consistent error when TensorFlow is unavailable.</li> <li>Maintain the dict-of-arrays convention so callers can consistently convert to <code>tf.data.Dataset</code> using <code>to_tf_dataset</code>.</li> </ul>"},{"location":"pyr_trainable_assisted_imitation_utilities/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle</code> package (project context)</li> <li>TensorFlow <code>tf.data.Dataset</code> and <code>tf.keras</code> APIs (used by <code>to_tf_dataset</code> and <code>train_layer</code>)</li> </ul>"},{"location":"pyr_trainable_assisted_imitation_utilities/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version (as stated in module docstring).</li> </ul>"},{"location":"pyr_trainable_assisted_model_a/","title":"PyrTrainableAssistedModelA","text":"<p>Role: Keras model implementing the Player-A pyramid assisted architecture with per-level measurement/combine layers and per-level shared-resource assisted layers, producing a single communication logit per batch element.</p> <p>Location: <code>Q_Sea_Battle.pyr_trainable_assisted_model_a.PyrTrainableAssistedModelA</code></p>"},{"location":"pyr_trainable_assisted_model_a/#derived-constraints","title":"Derived constraints","text":"<ul> <li>\\(n2\\) (the flattened field length) is inferred from <code>game_layout</code> as either <code>int(game_layout.n2)</code> or <code>int(game_layout.field_size) * int(game_layout.field_size)</code>, and must be positive and a power of two.</li> <li>\\(m\\) (communication size) is inferred as <code>int(game_layout.comms_size)</code> or <code>int(game_layout.M)</code> or defaults to <code>1</code>, and must satisfy \\(m = 1\\).</li> <li>Model depth \\(K = \\log_2(n2)\\) (an integer), and the number of per-level measurement layers, combine layers, and shared-resource layers is exactly \\(K\\).</li> <li>Input batch <code>field_batch</code> must be rank-2 with shape \\((B, n2)\\); \\(B\\) is batch size.</li> </ul>"},{"location":"pyr_trainable_assisted_model_a/#constructor","title":"Constructor","text":"Parameter Type Description <code>game_layout</code> <code>Any</code>, GameLayout-like object Must expose either <code>n2: int</code> or <code>field_size: int</code> (to derive \\(n2\\)), and may expose <code>comms_size: int</code> or <code>M: int</code> (to derive \\(m\\)). Constraints: derived \\(n2\\) must be positive and a power of two; derived \\(m\\) must equal 1. Shape: not applicable. <code>p_high</code> <code>float</code>, unconstrained Correlation parameter forwarded to each <code>PRAssistedLayer</code>. <code>sr_mode</code> <code>str</code>, unconstrained Mode forwarded to each <code>PRAssistedLayer</code> as <code>mode</code>. <code>measure_layers</code> <code>Optional[Sequence[tf.keras.layers.Layer]]</code>, length \\(K\\) if provided Optional per-level measurement layers. Constraint: if not <code>None</code>, <code>len(measure_layers) == K</code>. <code>combine_layers</code> <code>Optional[Sequence[tf.keras.layers.Layer]]</code>, length \\(K\\) if provided Optional per-level combine layers. Constraint: if not <code>None</code>, <code>len(combine_layers) == K</code>. <code>name</code> <code>Optional[str]</code>, unconstrained Optional Keras model name. <p>Preconditions</p> <ul> <li><code>game_layout</code> provides enough attributes to infer \\(n2\\) and \\(m\\) as described under Derived constraints.</li> <li>Derived \\(n2\\) is a power of two and greater than 0.</li> <li>Derived \\(m == 1\\).</li> <li>If <code>measure_layers</code> is provided, its length equals \\(K\\).</li> <li>If <code>combine_layers</code> is provided, its length equals \\(K\\).</li> </ul> <p>Postconditions</p> <ul> <li><code>self.n2: int</code> is set to inferred \\(n2\\).</li> <li><code>self.M: int</code> is set to inferred \\(m\\).</li> <li><code>self.depth: int</code> is set to \\(K = \\log_2(n2)\\).</li> <li><code>self.measure_layers: List[tf.keras.layers.Layer]</code> has length \\(K\\); defaults to <code>PyrMeasurementLayerA()</code> repeated \\(K\\) times if not provided.</li> <li><code>self.combine_layers: List[tf.keras.layers.Layer]</code> has length \\(K\\); defaults to <code>PyrCombineLayerA()</code> repeated \\(K\\) times if not provided.</li> <li><code>self.measure_layer</code> aliases <code>self.measure_layers[0]</code> and <code>self.combine_layer</code> aliases <code>self.combine_layers[0]</code> (legacy compatibility).</li> <li><code>self.sr_layers: List[PRAssistedLayer]</code> has length \\(K\\) with <code>resource_index</code> equal to the level index, and <code>length</code> equal to \\(n2 / 2^{(level+1)}\\).</li> </ul> <p>Errors</p> <ul> <li>Raises <code>ValueError</code> if derived $m \\ne 1`.</li> <li>Raises <code>ValueError</code> if derived $n2 \\le 0`.</li> <li>Raises <code>ValueError</code> if derived \\(n2\\) is not a power of two.</li> <li>Raises <code>ValueError</code> if <code>measure_layers</code> is provided and <code>len(measure_layers) != K</code>.</li> <li>Raises <code>ValueError</code> if <code>combine_layers</code> is provided and <code>len(combine_layers) != K</code>.</li> </ul> <p>Example</p> <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.pyr_trainable_assisted_model_a import PyrTrainableAssistedModelA\n\nclass Layout:\n    field_size = 4\n    comms_size = 1\n\nmodel = PyrTrainableAssistedModelA(game_layout=Layout(), p_high=0.9, sr_mode=\"sample\")\nx = tf.zeros((2, model.n2), dtype=tf.float32)\nlogits = model(x)\n</code></pre>"},{"location":"pyr_trainable_assisted_model_a/#public-methods","title":"Public Methods","text":""},{"location":"pyr_trainable_assisted_model_a/#call","title":"call","text":"<ul> <li>Signature: <code>call(self, field_batch: tf.Tensor) -&gt; tf.Tensor</code></li> </ul> <p>Parameters</p> <ul> <li><code>field_batch</code>: <code>tf.Tensor</code>, dtype not specified, shape \\((B, n2)\\), rank must be 2.</li> </ul> <p>Returns</p> <ul> <li><code>comm_logits</code>: <code>tf.Tensor</code>, dtype float32, shape \\((B, 1)\\).</li> </ul> <p>Errors</p> <ul> <li>Propagates <code>ValueError</code> from <code>compute_with_internal</code> if <code>field_batch</code> is not rank-2.</li> </ul> <p>Notes</p> <ul> <li>Delegates computation to <code>compute_with_internal</code> and returns only the first element (communication logits).</li> </ul>"},{"location":"pyr_trainable_assisted_model_a/#compute_with_internal","title":"compute_with_internal","text":"<ul> <li>Signature: <code>compute_with_internal(self, field_batch: tf.Tensor) -&gt; Tuple[tf.Tensor, List[tf.Tensor], List[tf.Tensor]]</code></li> </ul> <p>Parameters</p> <ul> <li><code>field_batch</code>: <code>tf.Tensor</code>, dtype not specified, shape \\((B, n2)\\), rank must be 2.</li> </ul> <p>Returns</p> <ul> <li><code>comm_logits</code>: <code>tf.Tensor</code>, dtype float32, shape \\((B, 1)\\); computed as <code>((clip(state,0,1) * 2 - 1) * 10)</code> after the final level, where final <code>state</code> is expected to be shape \\((B, 1)\\).</li> <li><code>measurements</code>: <code>List[tf.Tensor]</code>, length \\(K\\), each element <code>tf.Tensor</code> dtype float32, shape \\((B, L/2)\\) for that level (exact \\(L\\) per level is not validated in code).</li> <li><code>outcomes</code>: <code>List[tf.Tensor]</code>, length \\(K\\), each element <code>tf.Tensor</code> dtype float32, shape not specified (depends on <code>PRAssistedLayer</code> output).</li> </ul> <p>Errors</p> <ul> <li>Raises <code>ValueError</code> if <code>field_batch</code> is not rank-2, i.e., <code>x.shape.rank != 2</code>.</li> </ul>"},{"location":"pyr_trainable_assisted_model_a/#data-state","title":"Data &amp; State","text":"<ul> <li><code>n2</code>: <code>int</code>, inferred field length; constraint: positive power of two.</li> <li><code>M</code>: <code>int</code>, inferred communication size; constraint: equals 1.</li> <li><code>depth</code>: <code>int</code>, \\(K = \\log_2(n2)\\).</li> <li><code>measure_layers</code>: <code>List[tf.keras.layers.Layer]</code>, length \\(K\\); per-level measurement layers.</li> <li><code>combine_layers</code>: <code>List[tf.keras.layers.Layer]</code>, length \\(K\\); per-level combine layers.</li> <li><code>measure_layer</code>: <code>tf.keras.layers.Layer</code>, alias to <code>measure_layers[0]</code> (legacy compatibility).</li> <li><code>combine_layer</code>: <code>tf.keras.layers.Layer</code>, alias to <code>combine_layers[0]</code> (legacy compatibility).</li> <li><code>sr_layers</code>: <code>List[PRAssistedLayer]</code>, length \\(K\\); per-level shared-resource assisted layers with <code>resource_index=level</code>.</li> </ul>"},{"location":"pyr_trainable_assisted_model_a/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pyr_trainable_assisted_model_a/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pyr_trainable_assisted_model_a/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>Input validation in <code>compute_with_internal</code> checks only tensor rank, not the second dimension size; mismatched \\((B, n2)\\) sizes may fail later inside measurement/combine layers.</li> <li>The final conversion from the last-level <code>state</code> to <code>comm_logits</code> is a hard mapping to logits \\(\\{-10, +10\\}\\) after clipping to \\([0,1]\\); this is intentionally non-trainable per module docstring.</li> <li>The shared-resource layer invocation always passes <code>first_measurement</code> as ones and passes zero tensors for previous measurement/outcome, regardless of level.</li> </ul>"},{"location":"pyr_trainable_assisted_model_a/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.pyr_measurement_layer_a.PyrMeasurementLayerA</code></li> <li><code>Q_Sea_Battle.pyr_combine_layer_a.PyrCombineLayerA</code></li> <li><code>Q_Sea_Battle.pr_assisted_layer.PRAssistedLayer</code></li> </ul>"},{"location":"pyr_trainable_assisted_model_a/#changelog","title":"Changelog","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pyr_trainable_assisted_model_b/","title":"PyrTrainableAssistedModelB","text":"<p>Role: Player-B pyramid assisted Keras model that applies per-level measurement, assisted processing, and combination layers to produce a shooting logit.</p> <p>Location: <code>Q_Sea_Battle.pyr_trainable_assisted_model_b.PyrTrainableAssistedModelB</code></p>"},{"location":"pyr_trainable_assisted_model_b/#derived-constraints","title":"Derived constraints","text":"<ul> <li>Symbols: n2 = field_size, m = comms_size.</li> <li>n2 and m are inferred from <code>game_layout</code> via <code>_infer_n2_and_m(game_layout)</code>.</li> <li>Constraint: m must equal 1; otherwise construction raises <code>ValueError</code>.</li> <li>Constraint: n2 must be a power of two; enforced via <code>_validate_power_of_two(n2)</code> which returns <code>depth</code> (number of pyramid levels).</li> </ul>"},{"location":"pyr_trainable_assisted_model_b/#constructor","title":"Constructor","text":"Parameter Type Description game_layout Any, constraints: must be accepted by <code>_infer_n2_and_m(game_layout)</code>, shape: not specified Game layout object used to infer n2 and m. p_high float, constraints: not specified, shape: scalar Passed to each <code>PRAssistedLayer</code> as <code>p_high</code>. sr_mode str, constraints: not specified, shape: scalar Passed to each <code>PRAssistedLayer</code> as <code>mode</code>. Default <code>\"sample\"</code>. measure_layers Optional[Sequence[tf.keras.layers.Layer]], constraints: if provided then <code>len(measure_layers) == depth</code>, shape: sequence length <code>depth</code> Per-level measurement layers; if <code>None</code>, defaults to <code>depth</code> instances of <code>PyrMeasurementLayerB()</code>. combine_layers Optional[Sequence[tf.keras.layers.Layer]], constraints: if provided then <code>len(combine_layers) == depth</code>, shape: sequence length <code>depth</code> Per-level combine layers; if <code>None</code>, defaults to <code>depth</code> instances of <code>PyrCombineLayerB()</code>. name Optional[str], constraints: Keras model name semantics, shape: scalar Passed to <code>tf.keras.Model</code> constructor. <p>Preconditions</p> <ul> <li><code>_infer_n2_and_m(game_layout)</code> must succeed and return <code>(n2, m)</code>.</li> <li>m must be 1.</li> <li>n2 must be a power of two (as validated by <code>_validate_power_of_two</code>).</li> <li>If <code>measure_layers</code> is provided, it must be a sequence of length <code>depth</code>.</li> <li>If <code>combine_layers</code> is provided, it must be a sequence of length <code>depth</code>.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.n2: int</code> and <code>self.M: int</code> are set from <code>_infer_n2_and_m(game_layout)</code>.</li> <li><code>self.depth: int</code> is set from <code>_validate_power_of_two(self.n2)</code>.</li> <li><code>self.measure_layers: List[tf.keras.layers.Layer]</code> and <code>self.combine_layers: List[tf.keras.layers.Layer]</code> are populated with length <code>depth</code>.</li> <li>Backward-compat aliases <code>self.measure_layer</code> and <code>self.combine_layer</code> reference the first element of the corresponding per-level lists.</li> <li><code>self.sr_layers: List[PRAssistedLayer]</code> is populated with length <code>depth</code>, with decreasing <code>length</code> values per level: <code>n2/2, n2/4, ..., 1</code>.</li> </ul> <p>Errors</p> <ul> <li><code>ValueError</code>: if <code>m != 1</code> (message indicates <code>comms_size==1</code> is required).</li> <li><code>ValueError</code>: if <code>measure_layers</code> is provided and its length is not <code>depth</code>.</li> <li><code>ValueError</code>: if <code>combine_layers</code> is provided and its length is not <code>depth</code>.</li> </ul> <p>Example</p> <p>Instantiate with default per-level layers</p> <pre><code>import tensorflow as tf\nfrom Q_Sea_Battle.pyr_trainable_assisted_model_b import PyrTrainableAssistedModelB\n\ngame_layout = ...  # must be accepted by _infer_n2_and_m\nmodel = PyrTrainableAssistedModelB(game_layout=game_layout, p_high=0.9, sr_mode=\"sample\", name=\"player_b\")\n\n# Example call signature (see call() for tensor shapes)\nB = 4\nn2 = model.n2\ndepth = model.depth\ngun = tf.zeros((B, n2), dtype=tf.float32)\ncomm = tf.zeros((B, 1), dtype=tf.float32)\nprev_measurements = [tf.zeros((B, n2 // (2 ** (level + 1))), dtype=tf.float32) for level in range(depth)]\nprev_outcomes = [tf.zeros((B, n2 // (2 ** (level + 1))), dtype=tf.float32) for level in range(depth)]\ny = model([gun, comm, prev_measurements, prev_outcomes], training=False)\n</code></pre>"},{"location":"pyr_trainable_assisted_model_b/#public-methods","title":"Public Methods","text":""},{"location":"pyr_trainable_assisted_model_b/#call","title":"call","text":"<p>Signature: <code>call(self, inputs: list, training: bool = False, **kwargs: Any) -&gt; tf.Tensor</code></p> <p>Parameters</p> <ul> <li>inputs: list, constraints: must be list/tuple of length 4 in the order <code>[gun_batch, comm_batch, prev_measurements, prev_outcomes]</code>, shape: outer length 4.</li> <li>training: bool, constraints: not specified, shape: scalar; forwarded to per-level <code>measure_layer</code> and <code>combine_layer</code> if they accept a <code>training</code> keyword argument.</li> <li>**kwargs: Any, constraints: accepted but not used in implementation, shape: not applicable.</li> </ul> <p>Returns</p> <ul> <li>shoot_logit: tf.Tensor, dtype float32, constraints: derived from clipped comm in \\([0,1]\\) then mapped to logits, shape (B, 1).</li> </ul> <p>Behavior</p> <ul> <li>Validates <code>inputs</code> is a list/tuple of length 4 and unpacks it into <code>gun_batch</code>, <code>comm_batch</code>, <code>prev_measurements</code>, <code>prev_outcomes</code>.</li> <li>Converts <code>gun_batch</code> and <code>comm_batch</code> to float32 tensors and validates shapes: <code>gun</code> rank 2 with shape (B, n2) and <code>comm</code> rank 2 with shape (B, 1).</li> <li>Validates <code>prev_measurements</code> and <code>prev_outcomes</code> are Python lists/tuples with length <code>depth</code>.</li> <li>Iterates levels <code>0..depth-1</code>: computes a per-level measurement <code>meas_b</code> from current <code>state</code> using <code>measure_layers[level]</code>, then runs <code>PRAssistedLayer</code> to produce <code>out_b</code>, then applies <code>combine_layers[level]</code> to update <code>(state, c)</code>.</li> <li>Clips <code>c</code> to \\([0,1]\\) and maps to logits: <code>shoot_logit = (c * 2.0 - 1.0) * 10.0</code>.</li> </ul> <p>Errors</p> <ul> <li><code>ValueError</code>: if <code>inputs</code> is not list/tuple length 4.</li> <li><code>ValueError</code>: if <code>gun_batch</code> does not convert to a rank-2 tensor.</li> <li><code>ValueError</code>: if <code>comm_batch</code> does not convert to a rank-2 tensor of trailing dimension 1.</li> <li><code>TypeError</code>: if <code>prev_measurements</code> or <code>prev_outcomes</code> is not a Python list/tuple.</li> <li><code>ValueError</code>: if <code>prev_measurements</code> or <code>prev_outcomes</code> does not have length <code>depth</code>.</li> <li><code>ValueError</code>: if per-level <code>prev_out</code> or <code>meas_b</code> is not rank-2.</li> <li><code>ValueError</code>: if per-level last-dimension lengths of <code>meas_b</code>, <code>prev_meas</code>, and <code>prev_out</code> do not match.</li> </ul>"},{"location":"pyr_trainable_assisted_model_b/#data-state","title":"Data &amp; State","text":"<ul> <li>n2: int, constraints: inferred from <code>game_layout</code>, expected power of two, shape: scalar; field_size.</li> <li>M: int, constraints: must equal 1, shape: scalar; comms_size (m).</li> <li>depth: int, constraints: returned by <code>_validate_power_of_two(n2)</code>, shape: scalar; number of pyramid levels.</li> <li>measure_layers: List[tf.keras.layers.Layer], constraints: length <code>depth</code>, shape: list length <code>depth</code>; per-level measurement layers.</li> <li>combine_layers: List[tf.keras.layers.Layer], constraints: length <code>depth</code>, shape: list length <code>depth</code>; per-level combine layers.</li> <li>measure_layer: tf.keras.layers.Layer, constraints: alias of <code>measure_layers[0]</code>, shape: scalar reference.</li> <li>combine_layer: tf.keras.layers.Layer, constraints: alias of <code>combine_layers[0]</code>, shape: scalar reference.</li> <li>sr_layers: List[PRAssistedLayer], constraints: length <code>depth</code>, shape: list length <code>depth</code>; per-level assisted processing layers with decreasing <code>length</code>.</li> </ul>"},{"location":"pyr_trainable_assisted_model_b/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pyr_trainable_assisted_model_b/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"pyr_trainable_assisted_model_b/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>The forward pass attempts to call measurement/combine sublayers with <code>training=training</code> and falls back to calling without <code>training</code> on <code>TypeError</code>; changes to sublayer call signatures should preserve this robustness behavior.</li> <li><code>**kwargs</code> is accepted by <code>call</code> but unused; if adding behavior, ensure compatibility with Keras calling conventions.</li> </ul>"},{"location":"pyr_trainable_assisted_model_b/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.pyr_measurement_layer_b.PyrMeasurementLayerB</code></li> <li><code>Q_Sea_Battle.pyr_combine_layer_b.PyrCombineLayerB</code></li> <li><code>Q_Sea_Battle.pr_assisted_layer.PRAssistedLayer</code></li> <li><code>Q_Sea_Battle.pyr_trainable_assisted_model_a._infer_n2_and_m</code></li> <li><code>Q_Sea_Battle.pyr_trainable_assisted_model_a._validate_power_of_two</code></li> </ul>"},{"location":"pyr_trainable_assisted_model_b/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version; includes robustness patch to accept and forward the Keras <code>training</code> kwarg in <code>call</code> where supported by sublayers.</li> </ul>"},{"location":"pyr_trainable_models/","title":"Q_Sea_Battle.pyr_trainable_models","text":"<p>Role: Re-export hub for Pyramid (Pyr) trainable assisted models for Player A and Player B.</p> <p>Location: <code>Q_Sea_Battle.pyr_trainable_models</code></p>"},{"location":"pyr_trainable_models/#overview","title":"Overview","text":"<p>This module re-exports the trainable pyramid assisted models for Player A and Player B. It also defines project terminology used in the surrounding package: SR (shared resource) refers to any pre-shared auxiliary resource available to both players without communication; \"shared randomness\" is explicitly not used in this project\u2019s terminology.</p>"},{"location":"pyr_trainable_models/#public-api","title":"Public API","text":""},{"location":"pyr_trainable_models/#functions","title":"Functions","text":"<p>Not specified.</p>"},{"location":"pyr_trainable_models/#constants","title":"Constants","text":"<ul> <li><code>__all__</code>: <code>[\"PyrTrainableAssistedModelA\", \"PyrTrainableAssistedModelB\"]</code>   Purpose: Declares the public re-exports of this module.</li> </ul>"},{"location":"pyr_trainable_models/#types","title":"Types","text":"<p>Not specified.</p>"},{"location":"pyr_trainable_models/#dependencies","title":"Dependencies","text":"<ul> <li><code>Q_Sea_Battle.pyr_trainable_assisted_model_a.PyrTrainableAssistedModelA</code></li> <li><code>Q_Sea_Battle.pyr_trainable_assisted_model_b.PyrTrainableAssistedModelB</code></li> </ul>"},{"location":"pyr_trainable_models/#planned-design-spec","title":"Planned (design-spec)","text":"<p>Not specified.</p>"},{"location":"pyr_trainable_models/#deviations","title":"Deviations","text":"<p>Not specified.</p>"},{"location":"pyr_trainable_models/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>This module is a thin re-export layer; keep it free of implementation logic and restrict changes to import wiring and export surface (<code>__all__</code>) unless the package structure changes.</li> </ul>"},{"location":"pyr_trainable_models/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.pyr_trainable_assisted_model_a</code></li> <li><code>Q_Sea_Battle.pyr_trainable_assisted_model_b</code></li> </ul>"},{"location":"pyr_trainable_models/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial re-export module for <code>PyrTrainableAssistedModelA</code> and <code>PyrTrainableAssistedModelB</code>.</li> </ul>"},{"location":"pyramidal_algorithm/","title":"Pyramidal Algorithm in Classical Random Access Codes (RACs)","text":""},{"location":"pyramidal_algorithm/#overview","title":"Overview","text":"<p>In the Information Causality random access task, Alice holds \\(N = 2^n\\) bits and Bob is asked to output one target bit \\(a_b\\) given an index \\(b\\). Alice may send a single classical message bit \\(c\\) to Bob, and Alice and Bob may also use pre-shared no-signaling resources (PR-boxes / shared resources). Pawlowski et al. describe a recursive \"pyramid of boxes\" protocol (Figure 3 caption in [1]) that nests the \\(2 \\to 1\\) protocol across \\(n\\) levels. </p> <p>In QSeaBattle, we implement this nested protocol as a deterministic \"teacher\" rule-set that maps the game state to (i) measurement settings for Alice and Bob at each level, (ii) updated internal state (\"gun\") for the next level, and (iii) an updated one-bit communication value. This document specifies the algorithm as implemented, while referencing the recursive intent described by Pawlowski et al. [1]</p>"},{"location":"pyramidal_algorithm/#the-pyramidal-algorithm","title":"The Pyramidal Algorithm","text":""},{"location":"pyramidal_algorithm/#state-and-notation","title":"State and notation","text":"<ul> <li>Let \\(n^2 = \\texttt{field\\_size}^2\\) be the number of field bits.</li> <li>Let \\(x \\in \\{0,1\\}^{n^2}\\) be Alice's input field bits.</li> <li>Let \\(g \\in \\{0,1\\}^{n^2}\\) be the \"gun\" vector, assumed one-hot (exactly one entry equals \\(1\\)).</li> <li>Let \\(c \\in \\{0,1\\}\\) be the current one-bit communication value.</li> <li>Let \\(s \\in \\{0,1\\}^{n^2/2}\\) denote the shared-resource outputs for a level after Bob's measurement (one output per pair index \\(i\\) on that level).</li> <li>Indices are grouped into pairs \\((2i, 2i+1)\\) for \\(i = 0,1,\\dots,(n^2/2)-1\\).</li> </ul>"},{"location":"pyramidal_algorithm/#level-transition-rules","title":"Level transition rules","text":"<p>At each pyramid level, we reduce the effective field size by pairing indices and producing a new reduced state.</p>"},{"location":"pyramidal_algorithm/#1-alice-measurement-m_ia-from-the-input-field","title":"1) Alice measurement \\(m_i(A)\\) from the input field","text":"<p>For each pair \\((x_{2i}, x_{2i+1})\\), Alice produces a measurement bit: $$ m_i(A) = x_{2i} \\oplus x_{2i+1}. $$</p> <p>Interpretation in the implementation: - If the pair bits are equal, then \\(m_i(A)=0\\). - If the pair bits differ, then \\(m_i(A)=1\\).</p>"},{"location":"pyramidal_algorithm/#2-alice-combine-rule-f_i-pair-compression","title":"2) Alice combine rule \\(f'_i\\) (pair compression)","text":"<p>For each pair index \\(i\\), Alice defines the reduced field bit (for the next level) as: $$ f'i = x \\oplus s_i, $$ where \\(s_i\\) is the shared-resource outcome associated with pair index \\(i\\).</p> <p>Interpretation in the implementation: - The reduced bit uses the even-index field bit \\(x_{2i}\\) and is optionally flipped by the shared-resource outcome. - This mirrors the nesting idea in Pawlowski et al., where intermediate one-bit \"messages\" get re-encoded at higher levels.</p>"},{"location":"pyramidal_algorithm/#3-bob-measurement-m_ib-from-the-gun-pair","title":"3) Bob measurement \\(m_i(B)\\) from the gun pair","text":"<p>Bob's measurement (as implemented) is driven by the current one-hot gun state \\(g\\). For each gun pair \\((g_{2i}, g_{2i+1})\\), define: $$ m_i(B) = \\lnot g_{2i} \\wedge g_{2i+1}. $$</p> <p>Interpretation in the implementation: - \\(m_i(B)=1\\) if and only if the gun pair equals \\((0,1)\\). - Otherwise \\(m_i(B)=0\\).</p>"},{"location":"pyramidal_algorithm/#4-gun-update-g","title":"4) Gun update \\(g'\\)","text":"<p>We compress the gun to the next level using XOR on each pair: $$ g'i = g. $$} \\oplus g_{2i+1</p> <p>Interpretation in the implementation: - If the gun pair is \\((0,0)\\) or \\((1,1)\\), then \\(g'_i=0\\). - If the gun pair is \\((0,1)\\) or \\((1,0)\\), then \\(g'_i=1\\). - If \\(g\\) is one-hot, then \\(g'\\) is also one-hot.</p> <p>The one-hot invariant on \\(g\\) is assumed at all levels, and the update \\(g'_i = g_{2i} \\oplus g_{2i+1}\\) preserves it.</p>"},{"location":"pyramidal_algorithm/#5-shared-resource-usage-and-communication-bit-update","title":"5) Shared-resource usage and communication-bit update","text":"<p>Bob's measurement choice (informally, \"Phigh\" vs \"Plow\") produces a shared-resource output vector \\(s\\) for the current level. This output is not required to equal Alice's shared-resource outcomes, and it is used only to update the one-bit message \\(c\\).</p> <p>Let \\(g'\\) be the next-level (one-hot) gun vector and \\(s\\) the current-level shared-resource output vector (indexed by pair index \\(i\\)). The updated communication bit is: $$ c' = c \\oplus \\left(\\sum_i g'_i s_i\\right) \\bmod 2. $$</p> <p>Interpretation in the implementation: - Because \\(g'\\) is one-hot, the sum selects exactly one component \\(s_{i^*}\\) where \\(g'_{i^*}=1\\). - If the selected shared-resource bit is \\(1\\), the communication bit is flipped; otherwise it is unchanged. - The shared resource does not affect the gun state; it affects only the message bit.</p>"},{"location":"pyramidal_algorithm/#relationship-to-the-pawlowski-et-al-pyramid-conceptual-mapping","title":"Relationship to the Pawlowski et al. Pyramid (Conceptual Mapping)","text":"<p>Pawlowski et al. describe a recursive construction that nests the \\(2 \\to 1\\) protocol across \\(n\\) levels using \\(N-1\\) boxes for \\(N=2^n\\) input bits. Bob's final guess is an XOR of Alice's transmitted bit with one box output per level, and correctness reduces to having an even number of intermediate errors.</p> <p>In QSeaBattle, the nesting is realized operationally as repeated application of the level transition rules above: - Pairwise reduction of the field (\\(x \\mapsto f'\\)). - Pairwise reduction of the index state via the one-hot gun update (\\(g \\mapsto g'\\)). - One-bit message update (\\(c \\mapsto c'\\)) driven by the shared-resource output at the active next-level gun index.</p> <p>This implementation focuses on producing imitation targets for trainable layers, rather than reproducing every paper-specific variable name or diagram element.</p>"},{"location":"pyramidal_algorithm/#conditions-and-limitations","title":"Conditions and Limitations","text":"<ul> <li>The gun vector is assumed to be one-hot at all levels.</li> <li>The implementation uses fixed odd/even pairing \\((2i,2i+1)\\) at each reduction step.</li> <li>The shared-resource output influences only the communication bit and never the gun update.</li> <li>The mapping from Bob's measurement choice (\"Phigh\" vs other setting) to the distribution of \\(s\\) is handled by the shared-resource component; this document specifies only how \\(s\\) is consumed.</li> </ul>"},{"location":"pyramidal_algorithm/#qseabattle-implementation-notes","title":"QSeaBattle implementation notes","text":"<ul> <li>This pyramidal algorithm is used as an assisted strategy for players derived from the class <code>PRAssistedPlayers</code> </li> <li>The algorithm is also used to generate datasets (imitation targets) for trainable components. Functions to generate these datasets are found in <code>pyr_trainable_assisted_imitation_utilities</code></li> </ul>"},{"location":"pyramidal_algorithm/#key-reference","title":"Key Reference","text":"<ol> <li>Pawlowski et al., \"Information Causality as a Physical Principle\", arXiv:0905.2292 (v3, 6 May 2010). The pyramid protocol is described in the caption of Figure 3. </li> </ol>"},{"location":"reference_performance_utilities/","title":"Q_Sea_Battle.reference_performance_utilities","text":"<p>Role: Utility functions for analytic performance benchmarks in QSeaBattle.</p> <p>Location: <code>Q_Sea_Battle.reference_performance_utilities</code></p>"},{"location":"reference_performance_utilities/#overview","title":"Overview","text":"<p>This module provides analytic helper utilities used to benchmark expected player win rates and information-theoretic limits in the QSeaBattle package, including Shannon binary entropy, its inversion on a specified branch, closed-form (or semi-analytic) expected win-rate models for several player types, and an Information Causality success-probability bound.</p>"},{"location":"reference_performance_utilities/#public-api","title":"Public API","text":""},{"location":"reference_performance_utilities/#functions","title":"Functions","text":""},{"location":"reference_performance_utilities/#binary_entropyp-number-float","title":"<code>binary_entropy(p: Number) -&gt; float</code>","text":"<p>Signature: <code>binary_entropy(p: Number) -&gt; float</code> Purpose: Compute Shannon binary entropy \\(H(p)\\) in bits for a Bernoulli random variable with parameter <code>p</code>; returns limiting value <code>0.0</code> for out-of-domain inputs (<code>p &lt;= 0</code> or <code>p &gt;= 1</code>). Arguments: <code>p</code> (Number): Bernoulli parameter. Returns: <code>float</code>: Shannon binary entropy in bits. Errors: Not specified (no explicit exceptions raised; may raise standard exceptions if <code>p</code> is not convertible to <code>float</code>). Example: <pre><code>from Q_Sea_Battle.reference_performance_utilities import binary_entropy\n\nprint(binary_entropy(0.5))  # 1.0\nprint(binary_entropy(0.0))  # 0.0\n</code></pre></p>"},{"location":"reference_performance_utilities/#binary_entropy_reverseh-number-accuracy_in_digits-int-8-float","title":"<code>binary_entropy_reverse(H: Number, accuracy_in_digits: int = 8) -&gt; float</code>","text":"<p>Signature: <code>binary_entropy_reverse(H: Number, accuracy_in_digits: int = 8) -&gt; float</code> Purpose: Invert the binary entropy function on the branch \\(p \\in [0.5, 1.0]\\) using a bisection search to find <code>p</code> such that <code>binary_entropy(p) ~= H</code> within a decimal tolerance determined by <code>accuracy_in_digits</code>. Arguments: <code>H</code> (Number): Target entropy value (must lie in <code>[0.0, 1.0]</code>). <code>accuracy_in_digits</code> (int): Number of decimal digits of accuracy used to set the absolute entropy tolerance (<code>10**(-accuracy_in_digits)</code>). Returns: <code>float</code>: A probability <code>p</code> in <code>[0.5, 1.0]</code> whose binary entropy matches <code>H</code> within tolerance. Errors: <code>ValueError</code>: If <code>H</code> is outside <code>[0.0, 1.0]</code>. <code>RuntimeError</code>: If the search does not converge within 200 iterations. Example: <pre><code>from Q_Sea_Battle.reference_performance_utilities import binary_entropy_reverse, binary_entropy\n\np = binary_entropy_reverse(1.0)\nprint(p)  # 0.5\n\np2 = binary_entropy_reverse(0.0)\nprint(p2)  # 1.0\n\n# Consistency check (approximately)\np3 = binary_entropy_reverse(0.8, accuracy_in_digits=10)\nprint(binary_entropy(p3))\n</code></pre></p>"},{"location":"reference_performance_utilities/#expected_win_rate_simplefield_size-int-comms_size-int-enemy_probability-number-05-channel_noise-number-00-float","title":"<code>expected_win_rate_simple(field_size: int, comms_size: int, enemy_probability: Number = 0.5, channel_noise: Number = 0.0) -&gt; float</code>","text":"<p>Signature: <code>expected_win_rate_simple(field_size: int, comms_size: int, enemy_probability: Number = 0.5, channel_noise: Number = 0.0) -&gt; float</code> Purpose: Compute an analytic expected win rate for <code>SimplePlayers</code> based on a coverage fraction determined by <code>comms_size</code> over the <code>field_size**2</code> cells, a correct-covered probability <code>(1 - channel_noise)</code>, and an uncovered success probability derived from <code>enemy_probability</code>. Arguments: <code>field_size</code> (int): Field side length; must be <code>&gt;= 1</code>. <code>comms_size</code> (int): Number of communicated cells/bits (model-specific); must satisfy <code>1 &lt;= comms_size &lt;= field_size**2</code>. <code>enemy_probability</code> (Number): Probability a cell is an enemy; must lie in <code>[0.0, 1.0]</code>. <code>channel_noise</code> (Number): Binary symmetric channel flip probability; must lie in <code>[0.0, 1.0]</code>. Returns: <code>float</code>: Expected success probability. Errors: <code>ValueError</code>: If <code>field_size &lt; 1</code>, if <code>comms_size</code> is not in <code>[1, field_size**2]</code>, if <code>enemy_probability</code> is not in <code>[0.0, 1.0]</code>, or if <code>channel_noise</code> is not in <code>[0.0, 1.0]</code>. Example: <pre><code>from Q_Sea_Battle.reference_performance_utilities import expected_win_rate_simple\n\n# 4x4 field, communicate 4 cells, unbiased enemies, noiseless channel\nprint(expected_win_rate_simple(field_size=4, comms_size=4, enemy_probability=0.5, channel_noise=0.0))\n</code></pre></p>"},{"location":"reference_performance_utilities/#expected_win_rate_majorityfield_size-int-comms_size-int-enemy_probability-number-05-channel_noise-number-00-float","title":"<code>expected_win_rate_majority(field_size: int, comms_size: int, enemy_probability: Number = 0.5, channel_noise: Number = 0.0) -&gt; float</code>","text":"<p>Signature: <code>expected_win_rate_majority(field_size: int, comms_size: int, enemy_probability: Number = 0.5, channel_noise: Number = 0.0) -&gt; float</code> Purpose: Compute an analytic expected win rate for <code>MajorityPlayers</code> under a block-majority communication model with a binary symmetric channel, averaging over random fields and a random queried cell index. Arguments: <code>field_size</code> (int): Field side length; must be <code>&gt;= 1</code>. <code>comms_size</code> (int): Number of blocks <code>m</code>; must satisfy <code>1 &lt;= comms_size &lt;= field_size**2</code>, and must divide <code>field_size**2</code> (i.e., <code>field_size**2 % comms_size == 0</code>). <code>enemy_probability</code> (Number): Bernoulli parameter <code>p</code> for field cells; must lie in <code>[0.0, 1.0]</code>. <code>channel_noise</code> (Number): Channel flip probability <code>c</code>; must lie in <code>[0.0, 1.0]</code>. Returns: <code>float</code>: Average success probability. Errors: <code>ValueError</code>: If <code>field_size &lt; 1</code>, if <code>comms_size</code> is outside <code>[1, field_size**2]</code>, if <code>field_size**2</code> is not divisible by <code>comms_size</code>, if <code>enemy_probability</code> is not in <code>[0.0, 1.0]</code>, or if <code>channel_noise</code> is not in <code>[0.0, 1.0]</code>. Example: <pre><code>from Q_Sea_Battle.reference_performance_utilities import expected_win_rate_majority\n\n# 4x4 field (16 cells), 4 blocks =&gt; block length L=4\nprint(expected_win_rate_majority(field_size=4, comms_size=4, enemy_probability=0.5, channel_noise=0.1))\n</code></pre></p>"},{"location":"reference_performance_utilities/#expected_win_rate_assistedfield_size-int-comms_size-int-enemy_probability-number-05-channel_noise-number-00-p_high-number-09-float","title":"<code>expected_win_rate_assisted(field_size: int, comms_size: int, enemy_probability: Number = 0.5, channel_noise: Number = 0.0, p_high: Number = 0.9) -&gt; float</code>","text":"<p>Signature: <code>expected_win_rate_assisted(field_size: int, comms_size: int, enemy_probability: Number = 0.5, channel_noise: Number = 0.0, p_high: Number = 0.9) -&gt; float</code> Purpose: Compute an analytic expected win rate for classical <code>AssistedPlayers</code> under a one-bit communication model; currently restricted to <code>comms_size == 1</code> and requires <code>field_size**2</code> be a power of two. Arguments: <code>field_size</code> (int): Field side length; must be <code>&gt;= 1</code>; additionally, <code>field_size**2</code> must be a power of two. <code>comms_size</code> (int): Communication size; must equal <code>1</code>. <code>enemy_probability</code> (Number): Present in signature but not used by the implementation. <code>channel_noise</code> (Number): Channel flip probability <code>c</code>; must lie in <code>[0.0, 1.0]</code>. <code>p_high</code> (Number): Parameter used in computing the ideal success expression; must lie in <code>[0.0, 1.0]</code>. Returns: <code>float</code>: Expected success probability, clamped to <code>[0.0, 1.0]</code>. Errors: <code>ValueError</code>: If <code>field_size &lt; 1</code>, if <code>comms_size != 1</code>, if <code>field_size**2</code> is not a power of two, if <code>channel_noise</code> is not in <code>[0.0, 1.0]</code>, or if <code>p_high</code> is not in <code>[0.0, 1.0]</code>. Example: <pre><code>from Q_Sea_Battle.reference_performance_utilities import expected_win_rate_assisted\n\n# field_size=2 =&gt; n2=4 is a power of two\nprint(expected_win_rate_assisted(field_size=2, comms_size=1, channel_noise=0.05, p_high=0.9))\n</code></pre></p>"},{"location":"reference_performance_utilities/#limit_from_mutual_informationfield_size-int-comms_size-int-channel_noise-number-00-accuracy_in_digits-int-8-float","title":"<code>limit_from_mutual_information(field_size: int, comms_size: int, channel_noise: Number = 0.0, accuracy_in_digits: int = 8) -&gt; float</code>","text":"<p>Signature: <code>limit_from_mutual_information(field_size: int, comms_size: int, channel_noise: Number = 0.0, accuracy_in_digits: int = 8) -&gt; float</code> Purpose: Compute the maximum allowed success probability under Information Causality, given a field size, a communication budget, and an optional binary symmetric channel noise parameter; uses <code>binary_entropy</code> and <code>binary_entropy_reverse</code> to map an effective communication rate to a success bound. Arguments: <code>field_size</code> (int): Field side length; must be <code>&gt;= 1</code>. <code>comms_size</code> (int): Communication size <code>m</code>; must satisfy <code>0 &lt;= comms_size &lt;= field_size**2</code>. <code>channel_noise</code> (Number): Channel flip probability <code>c</code>; must lie in <code>[0.0, 1.0]</code>. <code>accuracy_in_digits</code> (int): Passed through to <code>binary_entropy_reverse</code>. Returns: <code>float</code>: Success-probability bound in <code>[0.5, 1.0]</code> (as implemented, returns <code>0.5</code> in several limiting cases and <code>1.0</code> when effective communication exceeds the field size). Errors: <code>ValueError</code>: If <code>field_size &lt; 1</code>, if <code>comms_size</code> is not in <code>[0, field_size**2]</code>, or if <code>channel_noise</code> is not in <code>[0.0, 1.0]</code>. <code>ValueError</code>/<code>RuntimeError</code>: May propagate from <code>binary_entropy_reverse</code> depending on internal target and convergence. Example: <pre><code>from Q_Sea_Battle.reference_performance_utilities import limit_from_mutual_information\n\nprint(limit_from_mutual_information(field_size=4, comms_size=0))         # 0.5\nprint(limit_from_mutual_information(field_size=4, comms_size=16))        # 1.0\nprint(limit_from_mutual_information(field_size=4, comms_size=4, channel_noise=0.1))\n</code></pre></p>"},{"location":"reference_performance_utilities/#constants","title":"Constants","text":"<p>Not specified.</p>"},{"location":"reference_performance_utilities/#types","title":"Types","text":""},{"location":"reference_performance_utilities/#number","title":"<code>Number</code>","text":"<p>Definition: <code>Number = Union[float, int]</code> Purpose: Convenience type alias for numeric parameters accepted by the module\u2019s public functions.</p>"},{"location":"reference_performance_utilities/#dependencies","title":"Dependencies","text":"<ul> <li>Standard library: <code>math</code>, <code>typing.Union</code>, <code>__future__.annotations</code></li> </ul>"},{"location":"reference_performance_utilities/#planned-design-spec","title":"Planned (design-spec)","text":"<p>Not specified.</p>"},{"location":"reference_performance_utilities/#deviations","title":"Deviations","text":"<ul> <li><code>expected_win_rate_assisted</code> includes an <code>enemy_probability</code> parameter but does not use it in the current implementation.</li> <li><code>expected_win_rate_assisted</code> explicitly restricts <code>comms_size</code> to <code>1</code> (no support for other values).</li> <li><code>expected_win_rate_majority</code> imports <code>math</code> inside the function even though <code>math</code> is already imported at module scope.</li> </ul>"},{"location":"reference_performance_utilities/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>Keep numeric stability in mind: <code>expected_win_rate_majority</code> uses <code>lgamma</code> and log-space computation for the binomial PMF to reduce overflow/underflow risks.</li> <li>If extending <code>expected_win_rate_assisted</code> beyond <code>comms_size == 1</code>, update input validation and document the new model assumptions.</li> <li>When changing tolerances or iteration counts in <code>binary_entropy_reverse</code>, ensure callers relying on <code>accuracy_in_digits</code> remain consistent.</li> </ul>"},{"location":"reference_performance_utilities/#related","title":"Related","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"reference_performance_utilities/#changelog","title":"Changelog","text":"<ul> <li>Version 0.1: Initial implementation (per module docstring).</li> </ul>"},{"location":"shared_resources/","title":"Shared Resources","text":"<p>This page documents the types of shared resources used in QSeaBattle and clarifies their conceptual meaning and implementation scope.</p>"},{"location":"shared_resources/#overview","title":"Overview","text":"<p>A shared resource is any pre-established correlation available to both players before the game starts and usable during play without communication. Different resource classes define different power levels for player strategies.</p> <p>The hierarchy below is ordered from weakest to strongest.</p>"},{"location":"shared_resources/#no-shared-resource-local-deterministic","title":"No shared resource (Local / deterministic)","text":"<p>Resource None.</p> <p>Meaning Players act independently. Outputs are deterministic (or equivalently locally random) functions of their own inputs.</p> <p>Status in QSeaBattle</p> <ul> <li>Baseline classical strategies.</li> <li>Used implicitly when no shared resource object is present.</li> </ul>"},{"location":"shared_resources/#shared-classical-randomness-sr","title":"Shared classical randomness (SR)","text":"<p>Resource A shared classical random variable (common seed).</p> <p>Meaning Players correlate their actions via pre-agreed randomness, but correlations are entirely classical and local.</p> <p>Key property</p> <ul> <li>Does not violate Bell inequalities.</li> <li>Optimal strategies can be assumed deterministic given the shared seed.</li> </ul> <p>Status in QSeaBattle</p> <ul> <li>Not yet implemented.</li> </ul>"},{"location":"shared_resources/#entanglement-assisted-resources-ea","title":"Entanglement-assisted resources (EA)","text":"<p>Resource A shared quantum state with local measurements.</p> <p>Meaning Players exploit quantum correlations that exceed classical limits but respect Tsirelson bounds.</p> <p>Key property</p> <ul> <li>No signalling.</li> <li>Correlations achievable by quantum mechanics.</li> </ul> <p>Status in QSeaBattle</p> <ul> <li>Not yet implemented.</li> </ul>"},{"location":"shared_resources/#pr-assisted-resources-pr","title":"PR-assisted resources (PR)","text":"<p>Resource A classical no-signalling box with correlations stronger than classical and tunable via a parameter.</p> <p>Meaning Models post-quantum correlations (PR-box\u2013like) while remaining classical and efficiently simulable.</p> <p>Key property</p> <ul> <li>No signalling.</li> <li>Can exceed classical and quantum bounds depending on configuration.</li> <li>Strictly stronger than shared randomness.</li> </ul> <p>Status in QSeaBattle</p> <ul> <li>Implemented by <code>PRAssisted</code>.</li> <li> <p>Used by:</p> </li> <li> <p><code>PRAssistedPlayers</code></p> </li> <li><code>PRAssistedPlayerA</code></li> <li><code>PRAssistedPlayerB</code></li> </ul>"},{"location":"shared_resources/#summary-table-conceptual","title":"Summary table (conceptual)","text":"<ul> <li>Local: no correlation</li> <li>SR: classical correlation only (deprecated)</li> <li>EA: quantum correlation (reference)</li> <li>PR: post-quantum, no-signalling correlation (active model)</li> </ul>"},{"location":"shared_resources/#changelog","title":"Changelog","text":"<ul> <li>2026-01-10 \u2014 Author: Rob Hendriks \u2014 Initial documentation of shared resource classes and terminology.</li> </ul>"},{"location":"simple_algorithm/","title":"Simple (Subset-Revealing) Algorithm in Classical Random Access Codes (RACs)","text":""},{"location":"simple_algorithm/#overview","title":"Overview","text":"<p>In a classical \\(n \\to m\\) random access code (RAC), Alice holds an \\(n\\)-bit string and is allowed to send an \\(m\\)-bit classical message to Bob. Bob is then asked to output the value of any one of Alice\u2019s input bits, specified by a query index, and the performance of the protocol is measured by the average success probability over uniformly random inputs and queries.</p> <p>Besides optimal strategies such as majority encoding, the RAC literature also considers simple deterministic baseline strategies. One of the most basic of these is the subset-revealing (or identity-based) algorithm, in which Alice directly communicates a fixed subset of her input bits, and Bob outputs the communicated bit whenever possible, otherwise guessing according to a fixed prior.</p> <p>This algorithm is strictly suboptimal compared to majority encoding, but it is widely used in the literature as a classical baseline and as a reference point when proving optimality results or demonstrating quantum advantages.</p>"},{"location":"simple_algorithm/#the-simple-algorithm-encoding-decoding","title":"The Simple Algorithm \u2013 Encoding &amp; Decoding","text":"<p>Algorithm (Subset-revealing encoding, identity decoding):</p> <p>Alice selects a fixed subset of \\(m\\) positions from her \\(n\\)-bit input string and sends the corresponding \\(m\\) bits directly to Bob. Bob, upon receiving the message and a query index \\(j\\), outputs the communicated bit if \\(j\\) lies in the revealed subset; otherwise, Bob outputs a default value or guesses according to a fixed prior probability.</p> <p>This strategy can be summarized as:</p> <ul> <li>Encoding: transmit raw input bits from predetermined positions</li> <li>Decoding: direct lookup if available, otherwise a fixed probabilistic guess</li> </ul>"},{"location":"simple_algorithm/#example-binary-case","title":"Example (Binary case)","text":"<p>Let Alice\u2019s input be a 5-bit string <code>x = 1 0 1 1 0</code> Suppose Alice is allowed to send \\(m = 1\\) bit and always transmits the first bit of her string. She sends: <code>comm = 1</code></p> <ul> <li>If Bob is asked for index 0, he outputs <code>1</code> and is correct.</li> <li>If Bob is asked for any other index, he has no information and must guess (e.g. with probability 1/2).</li> </ul> <p>The average success probability of this strategy is therefore limited by the probability that the queried index lies in the revealed subset.</p>"},{"location":"simple_algorithm/#interpretation-as-a-classical-rac-baseline","title":"Interpretation as a Classical RAC Baseline","text":"<p>This strategy corresponds to what is often referred to in the literature as a trivial or identity-based classical RAC:</p> <p>Alice sends one of her input bits; Bob outputs that bit if it is queried, and otherwise guesses.</p> <p>Such strategies are conceptually simple and easy to analyze, making them natural baseline protocols when comparing different RAC constructions.</p> <p>Importantly, the performance of this algorithm scales linearly with the fraction of revealed bits: $$ P_{\\text{succ}} = \\frac{m}{n} + \\left(1 - \\frac{m}{n}\\right) p_{\\text{guess}}, $$ where \\(p_{\\text{guess}}\\) is the success probability of Bob\u2019s default guess.</p>"},{"location":"simple_algorithm/#relation-to-optimality-results-in-the-literature","title":"Relation to Optimality Results in the Literature","text":""},{"location":"simple_algorithm/#comparison-with-majority-encoding","title":"Comparison with Majority Encoding","text":"<p>In their study of classical and quantum RACs, Tavakoli et al. (2015) explicitly contrast simple classical strategies with majority encoding. While their focus is on identifying optimal strategies, they describe majority encoding as outperforming strategies in which Alice merely outputs part of her input:</p> <p>\u201cIntuition strongly suggests that an optimal strategy is for Alice to use majority encoding\u2026\u201d \u2014 Phys. Rev. Lett. 114, 170502 (2015)</p> <p>This statement appears in the context of comparing majority encoding against simpler classical encodings, such as sending individual symbols or fixed components of the input. These simpler encodings correspond precisely to subset-revealing strategies like the one described here.</p>"},{"location":"simple_algorithm/#classification-in-the-optimality-proof","title":"Classification in the Optimality Proof","text":"<p>The rigorous proof of optimality by Ambainis, Kravchenko &amp; Rai (2015) analyzes the space of all deterministic classical RAC strategies. In doing so, the authors explicitly consider strategies in which Alice\u2019s message is directly identified with one of the input symbols or otherwise depends on only a limited part of the input.</p> <p>Within this classification framework, strategies that merely reveal a subset of bits are shown to be valid classical RACs but provably suboptimal:</p> <ul> <li>They achieve a success probability bounded by the fraction of information directly revealed.</li> <li>They are strictly dominated by majority-based strategies under uniform inputs.</li> </ul> <p>This analysis appears in the sections where deterministic encodings are enumerated and compared before establishing the optimality of majority encoding.</p>"},{"location":"simple_algorithm/#conditions-and-limitations","title":"Conditions and Limitations","text":"<ul> <li>The simple subset-revealing algorithm does not exploit correlations between input bits.</li> <li>Its performance depends directly on how often the queried index lies in the revealed subset.</li> <li>Under the standard RAC assumptions (uniform inputs and uniform query indices), this strategy cannot achieve the maximal average success probability.</li> </ul> <p>As a result, it serves primarily as: - a didactic example, - a baseline classical protocol, or - a lower bound when demonstrating the advantage of more sophisticated classical or quantum strategies.</p>"},{"location":"simple_algorithm/#qseabattle-implementation","title":"QSeaBattle Implementation","text":"<p>In QSeaBattle, this algorithm is implemented as a deterministic strategy in which:</p> <ul> <li>Alice transmits the first <code>m</code> bits of the flattened field.</li> <li>Bob outputs the communicated bit if the gun index lies within those <code>m</code> positions.</li> <li>Otherwise, Bob decides according to a fixed enemy probability.</li> </ul> <p>This implementation directly realizes the subset-revealing classical RAC described above and is used as a simple reference strategy against which more advanced algorithms (such as majority encoding or trainable models) can be compared.</p>"},{"location":"simple_algorithm/#key-references","title":"Key References","text":"<ul> <li> <p>Tavakoli et al., \u201cQuantum Random Access Codes using Single d-level Systems,\u201d Phys. Rev. Lett. 114, 170502 (2015).   \u2014 Introduces majority encoding and contrasts it with simpler classical strategies such as sending individual input symbols. The discussion motivating majority encoding implicitly treats subset-revealing strategies as natural but suboptimal baselines.</p> </li> <li> <p>Ambainis, Kravchenko &amp; Rai, \u201cQuantum Advantages in (n, d)\u21921 Random Access Codes,\u201d arXiv:1510.03045 (2015).   \u2014 Provides a complete characterization of optimal classical RAC strategies. Deterministic strategies that reveal only part of the input are included in the analysis and shown to be strictly suboptimal compared to majority-based encodings.</p> </li> </ul>"},{"location":"simple_player_a/","title":"SimplePlayerA","text":"<p>Role: Deterministic Player A that encodes the first \\(m\\) bits of the flattened field into the communication vector.</p> <p>Location: <code>Q_Sea_Battle.simple_player_a.SimplePlayerA</code></p>"},{"location":"simple_player_a/#constructor","title":"Constructor","text":"Parameter Type Description game_layout GameLayout, constraints Not specified, shape Not applicable Game configuration for this player. <p>Preconditions</p> <ul> <li><code>game_layout</code> is a <code>GameLayout</code> instance compatible with <code>PlayerA</code> and provides <code>comms_size</code> (type constraints Not specified).</li> </ul> <p>Postconditions</p> <ul> <li>The instance is initialized via <code>PlayerA.__init__(game_layout)</code>.</li> <li><code>self.game_layout</code> is available (inherited; exact storage not specified in this module).</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.simple_player_a import SimplePlayerA\nfrom Q_Sea_Battle.game_layout import GameLayout\n\ngl = GameLayout(...)  # Not specified in this module\nplayer = SimplePlayerA(gl)\n</code></pre>"},{"location":"simple_player_a/#public-methods","title":"Public Methods","text":""},{"location":"simple_player_a/#decide","title":"decide","text":"Parameter Type Description field np.ndarray, dtype convertible to int, constraints values intended {0,1}, shape (any, ...) Field array of 0/1 values; any shape is accepted and will be flattened internally. supp Any or None, constraints Optional, shape Not applicable Optional supporting information (unused). <p>Returns</p> <ul> <li>np.ndarray, dtype int, constraints derived from <code>field</code> after <code>np.asarray(..., dtype=int)</code>, shape (m,) where \\(m = \\text{self.game_layout.comms_size}\\).</li> </ul> <p>Preconditions</p> <ul> <li><code>self.game_layout.comms_size</code> exists and is an integer \\(m\\) with \\(0 \\le m\\) (upper bound not specified).</li> <li><code>field</code> is array-like and convertible via <code>np.asarray(field, dtype=int)</code>.</li> </ul> <p>Postconditions</p> <ul> <li>Returns a copy of the first \\(m\\) elements of <code>np.asarray(field, dtype=int).ravel()</code>.</li> </ul> <p>Errors</p> <ul> <li>May raise any exception propagated by <code>np.asarray(field, dtype=int)</code> if conversion fails (exact exception types not specified).</li> <li>May raise an exception if <code>self.game_layout</code> or <code>self.game_layout.comms_size</code> is missing (exact exception types not specified).</li> </ul> <p>Example</p> <pre><code>import numpy as np\nfrom Q_Sea_Battle.simple_player_a import SimplePlayerA\n\nplayer = SimplePlayerA(game_layout)  # game_layout must provide comms_size\nfield = np.array([[1, 0, 1], [0, 1, 0]])\ncomms = player.decide(field)\n</code></pre>"},{"location":"simple_player_a/#data-state","title":"Data &amp; State","text":"<ul> <li>Inherits state from <code>PlayerA</code> (not defined in this module).</li> <li>Reads <code>self.game_layout.comms_size</code> during <code>decide</code> to determine \\(m\\) (type/validation not specified here).</li> </ul>"},{"location":"simple_player_a/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"simple_player_a/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"simple_player_a/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>Keep <code>decide</code> deterministic: it should depend only on <code>field</code> and <code>self.game_layout.comms_size</code>.</li> <li><code>supp</code> is currently unused; if future behavior uses it, update the method contract accordingly.</li> </ul>"},{"location":"simple_player_a/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.players_base.PlayerA</code> (base class; not included in this module text)</li> <li><code>Q_Sea_Battle.game_layout.GameLayout</code> (configuration type; not included in this module text)</li> </ul>"},{"location":"simple_player_a/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial deterministic implementation encoding the first \\(m\\) flattened field cells into the communication vector.</li> </ul>"},{"location":"simple_player_b/","title":"SimplePlayerB","text":"<p>Role: Deterministic Player B that maps early gun positions to communication bits and otherwise shoots probabilistically.</p> <p>Location: <code>Q_Sea_Battle.simple_player_b.SimplePlayerB</code></p>"},{"location":"simple_player_b/#constructor","title":"Constructor","text":"Parameter Type Description game_layout GameLayout, constraints: Not specified, shape: N/A Game configuration for this player; passed to the base <code>PlayerB</code> constructor. <p>Preconditions</p> <ul> <li><code>game_layout</code> must be compatible with <code>PlayerB.__init__</code> (exact requirements not specified in this module).</li> </ul> <p>Postconditions</p> <ul> <li>The instance is initialized via <code>PlayerB</code> with the provided <code>game_layout</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified (any exceptions raised by <code>PlayerB.__init__</code> may propagate).</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.game_layout import GameLayout\nfrom Q_Sea_Battle.simple_player_b import SimplePlayerB\n\nlayout = GameLayout(...)  # Not specified in this module\nplayer = SimplePlayerB(layout)\n</code></pre>"},{"location":"simple_player_b/#public-methods","title":"Public Methods","text":""},{"location":"simple_player_b/#decide","title":"decide","text":"<p>Decide whether to shoot based on the gun position and the received communication vector.</p> Parameter Type Description gun np.ndarray, dtype int, constraints: intended one-hot, shape (n2,) after flattening Flattened one-hot gun vector; the index of the maximum value is used as the gun cell index. comm np.ndarray, dtype int, constraints: values not specified, shape (m,) after flattening Communication vector from Player A; if the gun index is within the first \\(m\\) cells, the corresponding bit is returned. supp Any or None, constraints: unused, shape: N/A Optional supporting information; ignored by this implementation. <p>Returns</p> <ul> <li>int, constraints: {0,1}, shape: scalar; <code>1</code> means shoot, <code>0</code> means do not shoot.</li> </ul> <p>Behavior</p> <ul> <li>Let <code>gun_index = argmax(gun)</code> after <code>gun</code> is converted to <code>np.ndarray</code> with <code>dtype=int</code> and flattened.</li> <li>Let <code>m = self.game_layout.comms_size</code>.</li> <li>If <code>gun_index &lt; m</code>, return <code>int(comm[gun_index])</code> after <code>comm</code> is converted to <code>np.ndarray</code> with <code>dtype=int</code> and flattened.</li> <li>Otherwise, return <code>1</code> with probability <code>p = self.game_layout.enemy_probability</code> and <code>0</code> with probability <code>1 - p</code>, using <code>np.random.rand()</code>.</li> </ul> <p>Preconditions</p> <ul> <li><code>self.game_layout</code> must provide <code>comms_size</code> (used as <code>m</code>) and <code>enemy_probability</code> (used as <code>p</code>); types/constraints are not specified in this module.</li> <li><code>comm</code> must have at least <code>m</code> elements after flattening to avoid index errors when <code>gun_index &lt; m</code>.</li> <li>The intended input is a valid one-hot <code>gun</code>, but the code does not validate one-hotness.</li> </ul> <p>Errors</p> <ul> <li><code>IndexError</code> if <code>gun_index &lt; m</code> and <code>comm</code> is shorter than <code>m</code> after flattening.</li> <li>Other NumPy-related errors may occur if inputs are not array-like; not specified further.</li> </ul> <p>Example</p> <pre><code>import numpy as np\nfrom Q_Sea_Battle.simple_player_b import SimplePlayerB\n\nplayer = SimplePlayerB(game_layout)  # game_layout must provide comms_size and enemy_probability\n\ngun = np.zeros(25, dtype=int)\ngun[3] = 1\ncomm = np.array([1, 0, 1, 0], dtype=int)\n\nshoot = player.decide(gun=gun, comm=comm)\n</code></pre>"},{"location":"simple_player_b/#data-state","title":"Data &amp; State","text":"<ul> <li>Inherited state from <code>PlayerB</code> (not specified in this module).</li> <li><code>self.game_layout</code>: GameLayout, constraints: must expose <code>comms_size</code> and <code>enemy_probability</code> for <code>decide</code>, shape: N/A.</li> </ul>"},{"location":"simple_player_b/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"simple_player_b/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"simple_player_b/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li><code>decide</code> uses <code>np.argmax</code> on <code>gun</code> after flattening; if <code>gun</code> is not strictly one-hot, the behavior follows the maximum element rather than validating the encoding.</li> <li>Randomness is sourced from <code>np.random.rand()</code>; seeding and reproducibility controls are not handled in this class.</li> </ul>"},{"location":"simple_player_b/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.simple_player_b.SimplePlayerB</code></li> <li><code>Q_Sea_Battle.players_base.PlayerB</code> (base class; behavior not specified here)</li> <li><code>Q_Sea_Battle.game_layout.GameLayout</code> (provides <code>comms_size</code> and <code>enemy_probability</code>)</li> </ul>"},{"location":"simple_player_b/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial documented version based on provided module text.</li> </ul>"},{"location":"simple_players/","title":"SimplePlayers","text":"<p>Role: Factory that produces a matched <code>(SimplePlayerA, SimplePlayerB)</code> pair sharing a <code>GameLayout</code>. Location: <code>Q_Sea_Battle.simple_players.SimplePlayers</code></p>"},{"location":"simple_players/#constructor","title":"Constructor","text":"Parameter Type Description game_layout GameLayout | None, constraint: may be <code>None</code> Optional shared configuration; if <code>None</code>, a default <code>GameLayout</code> is created by the base class. <p>Preconditions</p> <ul> <li>Not specified.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.game_layout</code> is initialized via the base <code>Players</code> constructor (exact initialization behavior is defined by <code>Players</code>).</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.simple_players import SimplePlayers\n\nfactory = SimplePlayers()\nplayer_a, player_b = factory.players()\n</code></pre>"},{"location":"simple_players/#public-methods","title":"Public Methods","text":""},{"location":"simple_players/#players","title":"players","text":"<p>Create a <code>(SimplePlayerA, SimplePlayerB)</code> pair that shares the same <code>GameLayout</code>.</p> <p>Returns</p> <ul> <li><code>Tuple[PlayerA, PlayerB]</code>, constraint: length exactly <code>2</code>, shape: <code>(2,)</code> as a fixed-size 2-tuple of <code>(player_a, player_b)</code>.</li> </ul> <p>Preconditions</p> <ul> <li>Not specified.</li> </ul> <p>Postconditions</p> <ul> <li>Returns two newly created player objects, both constructed with <code>self.game_layout</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>factory = SimplePlayers()\nplayer_a, player_b = factory.players()\n</code></pre>"},{"location":"simple_players/#data-state","title":"Data &amp; State","text":"<ul> <li><code>game_layout</code>: <code>GameLayout</code>, constraints: not specified in this module; provided/managed by the base class <code>Players</code>.</li> </ul>"},{"location":"simple_players/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>None specified.</li> </ul>"},{"location":"simple_players/#deviations","title":"Deviations","text":"<ul> <li>None identified (no design notes provided beyond empty placeholder).</li> </ul>"},{"location":"simple_players/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>This class delegates <code>GameLayout</code> initialization to <code>Players.__init__</code>; changes to default layout behavior should be implemented in <code>Players</code>, not here.</li> <li>The concrete player types are hard-coded as <code>SimplePlayerA</code> and <code>SimplePlayerB</code> within <code>players()</code>.</li> </ul>"},{"location":"simple_players/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.players_base.Players</code></li> <li><code>Q_Sea_Battle.players_base.PlayerA</code></li> <li><code>Q_Sea_Battle.players_base.PlayerB</code></li> <li><code>Q_Sea_Battle.game_layout.GameLayout</code></li> <li><code>Q_Sea_Battle.simple_player_a.SimplePlayerA</code></li> <li><code>Q_Sea_Battle.simple_player_b.SimplePlayerB</code></li> </ul>"},{"location":"simple_players/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version (module docstring).</li> </ul>"},{"location":"tournament/","title":"Tournament","text":"<p>Role: Run a multi-game QSeaBattle tournament by repeatedly executing <code>Game.play()</code> and recording outcomes in a <code>TournamentLog</code>. Location: <code>Q_Sea_Battle.tournament.Tournament</code></p>"},{"location":"tournament/#constructor","title":"Constructor","text":"Parameter Type Description game_env <code>GameEnv</code>, constraints: instance of <code>Q_Sea_Battle.game_env.GameEnv</code>, shape: N/A Game environment instance reused across games. players <code>Players</code>, constraints: instance of <code>Q_Sea_Battle.players_base.Players</code>, shape: N/A Players factory/provider for player A and B; reused across games. game_layout <code>GameLayout</code>, constraints: instance of <code>Q_Sea_Battle.game_layout.GameLayout</code>, shape: N/A Configuration specifying tournament length (used to determine number of games). <p>Preconditions: <code>game_env</code>, <code>players</code>, and <code>game_layout</code> are non-<code>None</code> objects compatible with downstream calls in <code>tournament()</code> (e.g., <code>Game(self.game_env, self.players)</code> and <code>TournamentLog(self.game_layout)</code> must be constructible).</p> <p>Postconditions: <code>self.game_env</code>, <code>self.players</code>, and <code>self.game_layout</code> are set to the provided instances.</p> <p>Errors: Not specified (any exceptions raised by attribute access or object construction will propagate).</p> <p>Example</p> <pre><code>from Q_Sea_Battle.tournament import Tournament\nfrom Q_Sea_Battle.game_env import GameEnv\nfrom Q_Sea_Battle.players_base import Players\nfrom Q_Sea_Battle.game_layout import GameLayout\n\ngame_env = GameEnv()\nplayers = Players()\ngame_layout = GameLayout()\n\nt = Tournament(game_env=game_env, players=players, game_layout=game_layout)\nlog = t.tournament()\n</code></pre>"},{"location":"tournament/#public-methods","title":"Public Methods","text":""},{"location":"tournament/#tournament_1","title":"tournament","text":"<p>Execute a full tournament and return its log.</p> <p>Parameter: None.</p> <p>Returns: <code>TournamentLog</code>, constraints: instance of <code>Q_Sea_Battle.tournament_log.TournamentLog</code>, shape: N/A; contains results for all games played in the tournament.</p> <p>Side effects: Constructs <code>TournamentLog</code> and <code>Game</code>; repeatedly calls <code>Game.play()</code>; updates <code>TournamentLog</code> with per-game results and optional extra data when available from <code>players</code>.</p> <p>Preconditions: <code>self.game_layout.number_of_games_in_tournament</code> exists and is usable as the <code>range()</code> bound (i.e., an <code>int</code> or <code>__index__</code>-compatible type). <code>Game(self.game_env, self.players).play()</code> returns a 5-tuple <code>(reward, field, gun, comm, shoot)</code> such that <code>gun == 1</code> is a valid boolean mask over <code>field</code> and <code>field[gun == 1][0]</code> exists. <code>TournamentLog(self.game_layout)</code> supports <code>update(...)</code>, <code>update_indicators(...)</code>, and optionally <code>update_log_probs(...)</code> and <code>update_log_prev(...)</code> depending on <code>players</code> capabilities.</p> <p>Postconditions: The returned <code>TournamentLog</code> has been updated once per game with: base game outcome (via <code>update</code>), identifiers (via <code>update_indicators</code>), and optionally log-probabilities (via <code>update_log_probs</code>) and previous measurement/outcome data (via <code>update_log_prev</code>).</p> <p>Errors: Not specified; exceptions from <code>Game.play()</code>, numpy-like indexing operations, <code>players</code> methods (e.g., <code>players.players()</code>, <code>get_log_prob()</code>, <code>get_prev()</code>), or <code>TournamentLog</code> update methods may propagate.</p>"},{"location":"tournament/#data-state","title":"Data &amp; State","text":"<ul> <li><code>game_env</code>: <code>GameEnv</code>, constraints: instance of <code>Q_Sea_Battle.game_env.GameEnv</code>, shape: N/A; stored reference used for constructing a <code>Game</code>.</li> <li><code>players</code>: <code>Players</code>, constraints: instance of <code>Q_Sea_Battle.players_base.Players</code>, shape: N/A; stored reference used for constructing a <code>Game</code> and optional per-game logging (<code>has_log_probs</code>, <code>has_prev</code>).</li> <li><code>game_layout</code>: <code>GameLayout</code>, constraints: instance of <code>Q_Sea_Battle.game_layout.GameLayout</code>, shape: N/A; stored reference used to create <code>TournamentLog</code> and determine <code>n_games</code> via <code>number_of_games_in_tournament</code>.</li> </ul>"},{"location":"tournament/#planned-design-spec","title":"Planned (design-spec)","text":"<p>Not specified.</p>"},{"location":"tournament/#deviations","title":"Deviations","text":"<p>Not specified.</p>"},{"location":"tournament/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>The implementation uses fixed identifiers <code>tournament_id = 0</code> and <code>meta_id = 0</code> for all games; comments indicate these may be extended later.</li> <li>Optional logging is enabled via <code>getattr(self.players, \"has_log_probs\", False)</code> and <code>getattr(self.players, \"has_prev\", False)</code>; when present, the code assumes child players implement <code>get_log_prob()</code> (for A and B) and <code>get_prev()</code> (for A).</li> <li><code>cell_value</code> is derived as <code>int(field[gun == 1][0])</code>; ensure <code>gun</code> contains at least one element equal to <code>1</code> and that the masking semantics are valid for the <code>field</code>/<code>gun</code> types returned by <code>Game.play()</code>.</li> </ul>"},{"location":"tournament/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.game.Game</code></li> <li><code>Q_Sea_Battle.tournament_log.TournamentLog</code></li> <li><code>Q_Sea_Battle.game_env.GameEnv</code></li> <li><code>Q_Sea_Battle.game_layout.GameLayout</code></li> <li><code>Q_Sea_Battle.players_base.Players</code></li> </ul>"},{"location":"tournament/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version (module header indicates Version: 0.1).</li> </ul>"},{"location":"tournament_log/","title":"TournamentLog","text":"<p>Role: Structured log for storing QSeaBattle tournament results. Location: <code>Q_Sea_Battle.tournament_log.TournamentLog</code></p>"},{"location":"tournament_log/#constructor","title":"Constructor","text":"Parameter Type Description game_layout GameLayout, not specified, shape N/A Layout providing the log column names; used to set <code>self.game_layout</code> and to initialize an empty <code>pd.DataFrame</code> with <code>columns=game_layout.log_columns</code>. <p>Preconditions</p> <ul> <li><code>game_layout.log_columns</code> exists and is compatible with <code>pd.DataFrame(columns=...)</code>.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.game_layout</code> is set to the provided <code>game_layout</code>.</li> <li><code>self.log</code> is a <code>pd.DataFrame</code> with columns <code>game_layout.log_columns</code> and zero rows.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.tournament_log import TournamentLog\nfrom Q_Sea_Battle.game_layout import GameLayout\n\ngame_layout = GameLayout(...)  # Not specified\ntlog = TournamentLog(game_layout=game_layout)\n</code></pre>"},{"location":"tournament_log/#public-methods","title":"Public Methods","text":""},{"location":"tournament_log/#update","title":"update","text":"<p>Append a new game result to the log.</p> <p>Signature: <code>update(field, gun, comm, shoot, cell_value, reward) -&gt; None</code></p> <p>Parameters</p> <ul> <li><code>field</code>: np.ndarray, dtype not specified, shape not specified; stored under column <code>\"field\"</code>.</li> <li><code>gun</code>: np.ndarray, dtype not specified, shape not specified; stored under column <code>\"gun\"</code>.</li> <li><code>comm</code>: np.ndarray, dtype not specified, shape not specified; stored under column <code>\"comm\"</code>.</li> <li><code>shoot</code>: int, constraints not specified, shape N/A; converted via <code>int(shoot)</code> and stored under column <code>\"shoot\"</code>.</li> <li><code>cell_value</code>: int, constraints not specified, shape N/A; converted via <code>int(cell_value)</code> and stored under column <code>\"cell_value\"</code>.</li> <li><code>reward</code>: float, constraints not specified, shape N/A; converted via <code>float(reward)</code> and stored under column <code>\"reward\"</code>.</li> </ul> <p>Returns</p> <ul> <li><code>None</code>: NoneType, shape N/A.</li> </ul> <p>Preconditions</p> <ul> <li><code>self.log</code> is a <code>pd.DataFrame</code> that can accept assignment at index <code>len(self.log)</code> with the constructed <code>row</code> mapping.</li> <li><code>self.log</code> has (or can accept) the columns: <code>\"field\"</code>, <code>\"gun\"</code>, <code>\"comm\"</code>, <code>\"shoot\"</code>, <code>\"cell_value\"</code>, <code>\"reward\"</code>, <code>\"logprob_comm\"</code>, <code>\"logprob_shoot\"</code>, <code>\"game_id\"</code>, <code>\"tournament_id\"</code>, <code>\"meta_id\"</code>, <code>\"game_uid\"</code>, <code>\"prev_measurements\"</code>, <code>\"prev_outcomes\"</code>.</li> </ul> <p>Postconditions</p> <ul> <li>Appends exactly one new row to <code>self.log</code> with the provided values and with the following columns set to <code>None</code>: <code>\"logprob_comm\"</code>, <code>\"logprob_shoot\"</code>, <code>\"game_id\"</code>, <code>\"tournament_id\"</code>, <code>\"meta_id\"</code>, <code>\"game_uid\"</code>, <code>\"prev_measurements\"</code>, <code>\"prev_outcomes\"</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>import numpy as np\nfrom Q_Sea_Battle.tournament_log import TournamentLog\n\ntlog = TournamentLog(game_layout=game_layout)  # game_layout not specified\nfield = np.zeros((5, 5), dtype=int)\ngun = np.array([0, 1], dtype=int)\ncomm = np.array([1, 0], dtype=int)\n\ntlog.update(field=field, gun=gun, comm=comm, shoot=3, cell_value=0, reward=1.0)\n</code></pre>"},{"location":"tournament_log/#update_log_probs","title":"update_log_probs","text":"<p>Update log-probabilities for the last logged game.</p> <p>Signature: <code>update_log_probs(logprob_comm, logprob_shoot) -&gt; None</code></p> <p>Parameters</p> <ul> <li><code>logprob_comm</code>: float, constraints not specified, shape N/A; converted via <code>float(logprob_comm)</code> and stored in column <code>\"logprob_comm\"</code> for the last row.</li> <li><code>logprob_shoot</code>: float, constraints not specified, shape N/A; converted via <code>float(logprob_shoot)</code> and stored in column <code>\"logprob_shoot\"</code> for the last row.</li> </ul> <p>Returns</p> <ul> <li><code>None</code>: NoneType, shape N/A.</li> </ul> <p>Preconditions</p> <ul> <li><code>self.log</code> is not empty.</li> </ul> <p>Postconditions</p> <ul> <li>The last row in <code>self.log</code> has updated values in <code>\"logprob_comm\"</code> and <code>\"logprob_shoot\"</code>.</li> </ul> <p>Errors</p> <ul> <li><code>RuntimeError</code>: raised if no rows have been logged yet (propagated from <code>_last_row_index</code>).</li> </ul> <p>Example</p> <pre><code>tlog.update_log_probs(logprob_comm=-0.12, logprob_shoot=-1.34)\n</code></pre>"},{"location":"tournament_log/#update_log_prev","title":"update_log_prev","text":"<p>Update previous measurements/outcomes for the last game.</p> <p>Signature: <code>update_log_prev(prev_meas, prev_out) -&gt; None</code></p> <p>Parameters</p> <ul> <li><code>prev_meas</code>: Any, constraints not specified, shape not specified; stored in column <code>\"prev_measurements\"</code> for the last row.</li> <li><code>prev_out</code>: Any, constraints not specified, shape not specified; stored in column <code>\"prev_outcomes\"</code> for the last row.</li> </ul> <p>Returns</p> <ul> <li><code>None</code>: NoneType, shape N/A.</li> </ul> <p>Preconditions</p> <ul> <li><code>self.log</code> is not empty.</li> </ul> <p>Postconditions</p> <ul> <li>The last row in <code>self.log</code> has updated values in <code>\"prev_measurements\"</code> and <code>\"prev_outcomes\"</code>.</li> </ul> <p>Errors</p> <ul> <li><code>RuntimeError</code>: raised if no rows have been logged yet (propagated from <code>_last_row_index</code>).</li> </ul> <p>Example</p> <pre><code>tlog.update_log_prev(prev_meas={\"layer0\": [1, 2]}, prev_out={\"layer0\": [0]})\n</code></pre>"},{"location":"tournament_log/#update_indicators","title":"update_indicators","text":"<p>Update identifier fields for the last logged game and generate a unique <code>game_uid</code>.</p> <p>Signature: <code>update_indicators(game_id, tournament_id, meta_id) -&gt; None</code></p> <p>Parameters</p> <ul> <li><code>game_id</code>: int, constraints not specified, shape N/A; converted via <code>int(game_id)</code> and stored in column <code>\"game_id\"</code> for the last row.</li> <li><code>tournament_id</code>: int, constraints not specified, shape N/A; converted via <code>int(tournament_id)</code> and stored in column <code>\"tournament_id\"</code> for the last row.</li> <li><code>meta_id</code>: int, constraints not specified, shape N/A; converted via <code>int(meta_id)</code> and stored in column <code>\"meta_id\"</code> for the last row.</li> </ul> <p>Returns</p> <ul> <li><code>None</code>: NoneType, shape N/A.</li> </ul> <p>Preconditions</p> <ul> <li><code>self.log</code> is not empty.</li> </ul> <p>Postconditions</p> <ul> <li>The last row in <code>self.log</code> has updated values in <code>\"game_id\"</code>, <code>\"tournament_id\"</code>, and <code>\"meta_id\"</code>.</li> <li>The last row in <code>self.log</code> has <code>\"game_uid\"</code> set to <code>uuid.uuid4().hex</code> (str, hex characters, length not specified by the code).</li> </ul> <p>Errors</p> <ul> <li><code>RuntimeError</code>: raised if no rows have been logged yet (propagated from <code>_last_row_index</code>).</li> </ul> <p>Example</p> <pre><code>tlog.update_indicators(game_id=7, tournament_id=2, meta_id=42)\n</code></pre>"},{"location":"tournament_log/#outcome","title":"outcome","text":"<p>Compute aggregate statistics over the tournament.</p> <p>Signature: <code>outcome() -&gt; Tuple[float, float]</code></p> <p>Parameters</p> <ul> <li>None.</li> </ul> <p>Returns</p> <ul> <li><code>Tuple[float, float]</code>: tuple, shape (2,); <code>(mean_reward, std_error)</code> where <code>mean_reward</code> is the mean of the <code>\"reward\"</code> column and <code>std_error</code> is \\(s / \\sqrt{n}\\) with sample standard deviation <code>s</code> computed using <code>ddof=1</code> when <code>n &gt; 1</code>, otherwise <code>0.0</code>.</li> </ul> <p>Preconditions</p> <ul> <li>If <code>self.log</code> is non-empty, column <code>\"reward\"</code> exists and values are convertible to <code>float</code> via <code>astype(float)</code>.</li> </ul> <p>Postconditions</p> <ul> <li>Does not mutate <code>self.log</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <pre><code>mean_reward, std_error = tlog.outcome()\n</code></pre>"},{"location":"tournament_log/#data-state","title":"Data &amp; State","text":"<ul> <li><code>game_layout</code>: GameLayout, not specified, shape N/A; layout instance provided at construction time.</li> <li><code>log</code>: pd.DataFrame, constraints not specified, shape (n_games, n_cols); DataFrame containing one row per game, initially empty with <code>columns=game_layout.log_columns</code>, and later appended/updated by methods in this class.</li> </ul>"},{"location":"tournament_log/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"tournament_log/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"tournament_log/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li><code>_last_row_index()</code> is a private helper that raises <code>RuntimeError</code> when <code>self.log</code> is empty; public mutators that update the \"last row\" rely on this behavior.</li> <li><code>update()</code> sets several fields to <code>None</code> regardless of <code>game_layout.log_columns</code>; ensure <code>game_layout.log_columns</code> is compatible with the columns written by this class to avoid assignment/column-mismatch issues.</li> </ul>"},{"location":"tournament_log/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.game_layout.GameLayout</code> (provides <code>log_columns</code> used to initialize <code>self.log</code>).</li> </ul>"},{"location":"tournament_log/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial implementation with row append, last-row updates (log probs, previous values, identifiers), and aggregate outcome statistics.</li> </ul>"},{"location":"trainable_assisted_player_a/","title":"TrainableAssistedPlayerA","text":"<p>Role: Player A wrapper that computes communication bits from a binary field using a trainable model, optionally sampling for exploration, and stores intermediate tensors on its parent for Player B.</p> <p>Location: <code>Q_Sea_Battle.trainable_assisted_player_a.TrainableAssistedPlayerA</code></p>"},{"location":"trainable_assisted_player_a/#derived-constraints","title":"Derived constraints","text":"<ul> <li>Let field_size be <code>int(game_layout.field_size)</code>, comms_size be <code>int(game_layout.comms_size)</code>, \\(n2 = field\\_size^2\\), and \\(m = comms\\_size\\).</li> <li>decide() requires field to be <code>np.ndarray, dtype int {0,1}, shape (n2,)</code> and returns <code>np.ndarray, dtype int {0,1}, shape (m,)</code>.</li> </ul>"},{"location":"trainable_assisted_player_a/#constructor","title":"Constructor","text":"Parameter Type Description game_layout Any, constraints: must provide attributes <code>field_size</code> and <code>comms_size</code> readable via <code>getattr</code>, shape: N/A Game layout object used to derive \\(n2\\) and \\(m\\). model_a LinTrainableAssistedModelA, constraints: must provide <code>compute_with_internal(field_batch)</code> returning <code>(comm_logits, meas_list, out_list)</code>, shape: N/A Trainable model used to compute communication logits and internal tensors. <p>Preconditions</p> <ul> <li>game_layout must have attributes <code>field_size</code> and <code>comms_size</code> convertible to int.</li> </ul> <p>Postconditions</p> <ul> <li>self.game_layout is set to game_layout.</li> <li>self.model_a is set to model_a.</li> <li>self.parent is set to None.</li> <li>self.last_logprob_comm is set to None.</li> <li>self.explore is set to False.</li> </ul> <p>Errors</p> <ul> <li>Not specified (constructor does not explicitly raise; attribute access failures may surface later in decide()).</li> </ul> <p>Example</p> <p>Constructing a TrainableAssistedPlayerA</p> <pre><code>from Q_Sea_Battle.trainable_assisted_player_a import TrainableAssistedPlayerA\nfrom Q_Sea_Battle.lin_trainable_assisted_model_a import LinTrainableAssistedModelA\n\ngame_layout = ...  # must define field_size and comms_size\nmodel_a = LinTrainableAssistedModelA(...)\nplayer_a = TrainableAssistedPlayerA(game_layout=game_layout, model_a=model_a)\n</code></pre>"},{"location":"trainable_assisted_player_a/#public-methods","title":"Public Methods","text":""},{"location":"trainable_assisted_player_a/#decidefield-suppnone-explorenone","title":"decide(field, supp=None, explore=None)","text":"<p>Decide communication bits based on the field, either greedily (threshold at 0.5) or by sampling independent Bernoulli bits, and record the log-probability of the chosen bits under the model logits.</p> Parameter Type Description field np.ndarray, dtype int {0,1}, shape (n2,) Flattened binary field input; must have shape \\((n2,)\\) and contain only 0/1. supp Any | None, constraints: ignored, shape: N/A Ignored support argument. explore bool | None, constraints: if not None overrides <code>self.explore</code>, shape: N/A Optional override controlling exploration (sampling) vs greedy selection. <p>Returns</p> <ul> <li>np.ndarray, dtype int {0,1}, shape (m,) communication bits.</li> </ul> <p>Preconditions</p> <ul> <li>field must satisfy <code>field.shape == (n2,)</code> where \\(n2 = field\\_size^2\\).</li> <li>field must contain only values in {0,1}.</li> <li>model_a.compute_with_internal must accept <code>tf.Tensor, dtype float32, shape (1, n2)</code> and return <code>comm_logits</code> compatible with shape <code>(1, m)</code>.</li> </ul> <p>Postconditions</p> <ul> <li>self.last_logprob_comm is set to the scalar log-probability (Python float) of the returned bits under independent Bernoulli with the computed logits.</li> <li>If self.parent is not None, then <code>self.parent.previous</code> is set to <code>(meas_list, out_list)</code> as returned by the model.</li> </ul> <p>Errors</p> <ul> <li>ValueError if field shape is not <code>(n2,)</code>.</li> <li>ValueError if field contains values other than 0/1.</li> </ul> <p>Example</p> <p>Deciding communication bits</p> <pre><code>import numpy as np\n\nfield_size = int(getattr(player_a.game_layout, \"field_size\"))\nn2 = field_size ** 2\nfield = np.zeros((n2,), dtype=np.int32)\n\ncomm_bits = player_a.decide(field, explore=True)\n</code></pre>"},{"location":"trainable_assisted_player_a/#get_log_prob","title":"get_log_prob()","text":"<p>Return the log-probability of the last taken communication action.</p> <p>Parameters</p> <ul> <li>None.</li> </ul> <p>Returns</p> <ul> <li>float, constraints: finite scalar expected, shape: scalar.</li> </ul> <p>Preconditions</p> <ul> <li>decide() must have been called since the last reset() such that self.last_logprob_comm is not None.</li> </ul> <p>Postconditions</p> <ul> <li>No state change.</li> </ul> <p>Errors</p> <ul> <li>RuntimeError if self.last_logprob_comm is None (e.g., decide() has not been called since reset()).</li> </ul> <p>Example</p> <p>Reading last log-probability</p> <pre><code>lp = player_a.get_log_prob()\n</code></pre>"},{"location":"trainable_assisted_player_a/#get_prev","title":"get_prev()","text":"<p>Return the parent previous tensors if available.</p> <p>Parameters</p> <ul> <li>None.</li> </ul> <p>Returns</p> <ul> <li>Any | None, constraints: if not None then a 2-tuple <code>(meas_list, out_list)</code> as stored on <code>self.parent.previous</code>, shape: N/A.</li> </ul> <p>Preconditions</p> <ul> <li>None.</li> </ul> <p>Postconditions</p> <ul> <li>No state change.</li> </ul> <p>Errors</p> <ul> <li>Not specified (method is non-blocking and returns None when unavailable).</li> </ul> <p>Example</p> <p>Accessing previous tensors</p> <pre><code>prev = player_a.get_prev()\nif prev is not None:\n    meas_list, out_list = prev\n</code></pre>"},{"location":"trainable_assisted_player_a/#reset","title":"reset()","text":"<p>Reset internal state.</p> <p>Parameters</p> <ul> <li>None.</li> </ul> <p>Returns</p> <ul> <li>None.</li> </ul> <p>Preconditions</p> <ul> <li>None.</li> </ul> <p>Postconditions</p> <ul> <li>self.last_logprob_comm is set to None.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul> <p>Example</p> <p>Resetting</p> <pre><code>player_a.reset()\n</code></pre>"},{"location":"trainable_assisted_player_a/#data-state","title":"Data &amp; State","text":"<ul> <li>game_layout: Any, constraints: should provide <code>field_size</code> and <code>comms_size</code>, shape: N/A; set at construction.</li> <li>model_a: LinTrainableAssistedModelA, constraints: must implement <code>compute_with_internal</code>, shape: N/A; set at construction.</li> <li>parent: Any | None, constraints: if not None may be expected to have attribute <code>previous</code>, shape: N/A; default None and intended to be set externally.</li> <li>last_logprob_comm: float | None, constraints: None before decide() or after reset(), otherwise scalar float, shape: scalar.</li> <li>explore: bool, constraints: when True decide() samples, when False decide() is greedy, shape: scalar.</li> </ul>"},{"location":"trainable_assisted_player_a/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified (no design notes provided).</li> </ul>"},{"location":"trainable_assisted_player_a/#deviations","title":"Deviations","text":"<ul> <li>Not specified (no design notes provided to compare against).</li> </ul>"},{"location":"trainable_assisted_player_a/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>decide() computes \\(n2\\) and \\(m\\) dynamically from game_layout on each call; changes to game_layout at runtime will affect validation and output dimensionality.</li> <li>get_prev() reads <code>self.parent.previous</code> via getattr and returns None if unavailable; callers should handle None.</li> </ul>"},{"location":"trainable_assisted_player_a/#related","title":"Related","text":"<ul> <li>LinTrainableAssistedModelA</li> <li>PlayerA (base class, imported from <code>.players</code> if available; otherwise a fallback stub exists in-module)</li> <li>bernoulli_log_prob_from_logits (imported from <code>.logit_utils</code> if available; otherwise a fallback implementation exists in-module)</li> </ul>"},{"location":"trainable_assisted_player_a/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial implementation per module docstring.</li> </ul>"},{"location":"trainable_assisted_player_b/","title":"TrainableAssistedPlayerB","text":"<p>Role: Trainable Player B wrapper that consumes Player A \"previous\" tensors plus local measurements to decide whether to shoot, while tracking the last action log-probability.</p> <p>Location: <code>Q_Sea_Battle.trainable_assisted_player_b.TrainableAssistedPlayerB</code></p>"},{"location":"trainable_assisted_player_b/#constructor","title":"Constructor","text":"Parameter Type Description <code>game_layout</code> <code>Any</code>, not specified, shape N/A Object providing <code>field_size</code> and <code>comms_size</code> attributes used to derive \\(n2 = field\\_size^2\\) and \\(m = comms\\_size\\). <code>model_b</code> <code>LinTrainableAssistedModelB</code>, not specified, shape N/A Trainable model called as <code>model_b([gun_batch, comm_batch, prev_meas_batch, prev_out_batch])</code> to produce a shoot logit. <p>Preconditions</p> <ul> <li><code>game_layout</code> should expose <code>field_size</code> and <code>comms_size</code> attributes convertible to <code>int</code>; otherwise behavior is not specified (will likely raise at runtime).</li> <li><code>model_b</code> must be callable and return a Tensor compatible with shape <code>(1, 1)</code> when invoked from <code>decide()</code>.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.game_layout</code> is set to <code>game_layout</code>.</li> <li><code>self.model_b</code> is set to <code>model_b</code>.</li> <li><code>self.parent</code> is set to <code>None</code>.</li> <li><code>self.last_logprob_shoot</code> is set to <code>None</code>.</li> <li><code>self.explore</code> is set to <code>False</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified by constructor code.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.trainable_assisted_player_b import TrainableAssistedPlayerB\n\n# game_layout must provide .field_size and .comms_size; model_b must be a LinTrainableAssistedModelB\nplayer_b = TrainableAssistedPlayerB(game_layout=layout, model_b=model_b)\n</code></pre>"},{"location":"trainable_assisted_player_b/#public-methods","title":"Public Methods","text":""},{"location":"trainable_assisted_player_b/#decidegun-comm-suppnone-explorenone","title":"decide(gun, comm, supp=None, explore=None)","text":"<p>Decide whether to shoot (<code>0</code> or <code>1</code>) based on <code>gun</code> + <code>comm</code> + <code>parent.previous</code> tensors.</p> <p>Parameters</p> <ul> <li><code>gun</code>: <code>np.ndarray</code>, dtype int, values in <code>{0,1}</code>, shape <code>(n2,)</code>, where \\(n2 = field\\_size^2\\).</li> <li><code>comm</code>: <code>np.ndarray</code>, dtype not specified, shape <code>(m,)</code>, where \\(m = comms\\_size\\); values are validated for shape only (docstring notes ints in <code>{0,1}</code> or floats in <code>[0,1]</code> for DRU).</li> <li><code>supp</code>: <code>Any | None</code>, ignored, shape N/A.</li> <li><code>explore</code>: <code>bool | None</code>, if not <code>None</code> overrides <code>self.explore</code>, shape N/A.</li> </ul> <p>Returns</p> <ul> <li><code>int</code>, constraints <code>{0,1}</code>, shape <code>()</code>.</li> </ul> <p>Preconditions</p> <ul> <li><code>self.parent</code> is not <code>None</code> and <code>self.parent.previous</code> is not <code>None</code>.</li> <li><code>getattr(self.game_layout, \"field_size\")</code> and <code>getattr(self.game_layout, \"comms_size\")</code> exist and are convertible to <code>int</code>.</li> <li><code>gun.shape == (n2,)</code> and <code>gun</code> contains only <code>0/1</code>.</li> <li><code>comm.shape == (m,)</code>.</li> <li><code>self.parent.previous</code> is a tuple-like <code>(prev_meas_list, prev_out_list)</code> where each is a <code>list</code> (enforced before later normalization).</li> <li><code>len(prev_meas_list) &gt;= 1</code> and <code>len(prev_out_list) &gt;= 1</code>.</li> </ul> <p>Postconditions</p> <ul> <li>Computes <code>shoot_logit = self.model_b([gun_batch, comm_batch, prev_meas_batch, prev_out_batch])</code> where <code>gun_batch</code> has shape <code>(1, n2)</code> and <code>comm_batch</code> has shape <code>(1, m)</code>.</li> <li>Sets <code>self.last_logprob_shoot</code> to the log-probability (Python <code>float</code>) of the returned action under the computed logits.</li> <li>Returns <code>shoot</code> as greedy (<code>shoot_prob &gt;= 0.5</code>) if not exploring, else samples via <code>Uniform(0,1) &lt; shoot_prob</code>.</li> </ul> <p>Errors</p> <ul> <li><code>ValueError</code>: if <code>gun</code> shape mismatches <code>(n2,)</code>.</li> <li><code>ValueError</code>: if <code>gun</code> contains values other than <code>0/1</code>.</li> <li><code>ValueError</code>: if <code>comm</code> shape mismatches <code>(m,)</code>.</li> <li><code>RuntimeError</code>: if <code>self.parent is None</code> or <code>self.parent.previous is None</code>.</li> <li><code>TypeError</code>: if <code>self.parent.previous</code> is not <code>(list, list)</code> at the initial type check.</li> <li><code>ValueError</code>: if either list in <code>self.parent.previous</code> has length <code>&lt; 1</code>.</li> </ul> <p>Example</p> <pre><code>import numpy as np\n\n# Assume player_b.parent has been set and player_a has already populated parent.previous\nn2 = int(player_b.game_layout.field_size) ** 2\nm = int(player_b.game_layout.comms_size)\n\ngun = np.zeros((n2,), dtype=int)\ncomm = np.zeros((m,), dtype=int)\n\nshoot = player_b.decide(gun=gun, comm=comm, explore=True)\nlogp = player_b.get_log_prob()\n</code></pre>"},{"location":"trainable_assisted_player_b/#get_log_prob","title":"get_log_prob()","text":"<p>Return log-probability of the last taken shoot action (as set by <code>decide()</code>).</p> <p>Parameters</p> <ul> <li>None.</li> </ul> <p>Returns</p> <ul> <li><code>float</code>, constraints not specified (log-probability), shape <code>()</code>.</li> </ul> <p>Preconditions</p> <ul> <li><code>self.last_logprob_shoot</code> is not <code>None</code> (i.e., <code>decide()</code> has been called since the last <code>reset()</code>).</li> </ul> <p>Errors</p> <ul> <li><code>RuntimeError</code>: if <code>self.last_logprob_shoot is None</code>.</li> </ul>"},{"location":"trainable_assisted_player_b/#reset","title":"reset()","text":"<p>Reset internal state.</p> <p>Parameters</p> <ul> <li>None.</li> </ul> <p>Returns</p> <ul> <li><code>None</code>, shape N/A.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.last_logprob_shoot</code> is set to <code>None</code>.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul>"},{"location":"trainable_assisted_player_b/#data-state","title":"Data &amp; State","text":"<ul> <li><code>game_layout</code>: <code>Any</code>, constraints not specified, shape N/A; must provide <code>field_size</code> and <code>comms_size</code> attributes used by <code>decide()</code>.</li> <li><code>model_b</code>: <code>LinTrainableAssistedModelB</code>, constraints not specified, shape N/A; called by <code>decide()</code> to produce a shoot logit tensor.</li> <li><code>parent</code>: <code>Any | None</code>, constraints not specified, shape N/A; expected (by <code>decide()</code>) to provide <code>.previous</code> containing prior tensors from Player A.</li> <li><code>last_logprob_shoot</code>: <code>float | None</code>, constraints not specified, shape <code>()</code>; updated by <code>decide()</code>, cleared by <code>reset()</code>.</li> <li><code>explore</code>: <code>bool</code>, constraints <code>{False, True}</code>, shape <code>()</code>; default <code>False</code>, optionally overridden per-call via <code>decide(..., explore=...)</code>.</li> </ul>"},{"location":"trainable_assisted_player_b/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified (no design notes provided).</li> </ul>"},{"location":"trainable_assisted_player_b/#deviations","title":"Deviations","text":"<ul> <li>The docstring claims <code>parent</code> is of type <code>TrainableAssistedPlayers</code> and is set by <code>TrainableAssistedPlayers.players()</code>, but the module only types it as <code>Any | None</code> and does not define or enforce this contract.</li> <li><code>parent.previous</code> is initially required to be <code>(list, list)</code> via an explicit <code>isinstance(..., list)</code> check, yet later code contains normalization for non-list/tuple values (\"linear case: single tensor \u2192 list of length 1\") that is unreachable if the earlier check fails; these two behaviors conflict.</li> </ul>"},{"location":"trainable_assisted_player_b/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li>Symbols used: \\(n2 = field\\_size^2\\) and \\(m = comms\\_size\\) are derived inside <code>decide()</code> from <code>self.game_layout</code>.</li> <li><code>decide()</code> expects <code>self.parent.previous</code> to be populated before it is called; ensure Player A executes first in the calling sequence.</li> <li><code>bernoulli_log_prob_from_logits</code> may be imported from <code>.logit_utils</code> or fall back to a local implementation; changing either affects <code>last_logprob_shoot</code> semantics.</li> </ul>"},{"location":"trainable_assisted_player_b/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.trainable_assisted_player_b.bernoulli_log_prob_from_logits</code> (imported if available; otherwise locally defined fallback)</li> <li><code>Q_Sea_Battle.trainable_assisted_player_b.LinTrainableAssistedModelB</code> (dependency)</li> <li><code>Q_Sea_Battle.trainable_assisted_player_b.PlayerB</code> (base class, imported if available; otherwise fallback)</li> </ul>"},{"location":"trainable_assisted_player_b/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version (per module docstring).</li> </ul>"},{"location":"trainable_assisted_players/","title":"TrainableAssistedPlayers","text":"<p>Role: Players-style wrapper that wires a trainable sender (A) and receiver (B) assisted player pair and owns their underlying models and shared state.</p> <p>Location: <code>Q_Sea_Battle.trainable_assisted_players.TrainableAssistedPlayers</code></p>"},{"location":"trainable_assisted_players/#constructor","title":"Constructor","text":"Parameter Type Description game_layout Any, not specified, must provide attributes <code>field_size</code> and <code>comms_size</code> convertible to <code>int</code> Game-layout-like object used to parameterize default models and passed into player wrappers. p_high float, not specified Forward-compatibility parameter; currently unused by this class when building default linear models. num_iterations Optional[int], not specified Forward-compatibility parameter; currently unused by this class when building default linear models. hidden_dim int, not specified Forward-compatibility parameter; currently unused by this class when building default linear models. L_meas Optional[int], not specified Forward-compatibility parameter; currently unused by this class when building default linear models. model_a Optional[LinTrainableAssistedModelA], default <code>None</code> If provided, used as the A-side model; otherwise a default <code>LinTrainableAssistedModelA</code> is constructed from <code>game_layout.field_size</code> and <code>game_layout.comms_size</code>. model_b Optional[LinTrainableAssistedModelB], default <code>None</code> If provided, used as the B-side model; otherwise a default <code>LinTrainableAssistedModelB</code> is constructed from <code>game_layout.field_size</code> and <code>game_layout.comms_size</code>. <p>Preconditions</p> <ul> <li><code>game_layout</code> has attributes <code>field_size</code> and <code>comms_size</code> such that <code>int(getattr(game_layout, \"field_size\"))</code> and <code>int(getattr(game_layout, \"comms_size\"))</code> succeed when default models are constructed.</li> </ul> <p>Postconditions</p> <ul> <li><code>self.game_layout</code> is set to <code>game_layout</code> (type: Any, constraints: not specified).</li> <li><code>self.explore</code> is initialized to <code>False</code> (type: bool, constraints: {True, False}).</li> <li><code>self.model_a</code> is set (type: LinTrainableAssistedModelA, constraints: not specified).</li> <li><code>self.model_b</code> is set (type: LinTrainableAssistedModelB, constraints: not specified).</li> <li><code>self.previous</code> is initialized to <code>None</code> (type: Any | None, constraints: typically either <code>None</code> or a tuple <code>(measurements_per_layer, outcomes_per_layer)</code> where both are Python lists of tensors each shaped <code>(B, n2)</code>).</li> <li>Lazy player wrappers <code>self._playerA</code> and <code>self._playerB</code> are initialized to <code>None</code> (type: Optional[TrainableAssistedPlayerA] and Optional[TrainableAssistedPlayerB], constraints: not specified).</li> <li><code>self.has_prev</code> is initialized to <code>True</code> (type: bool, constraints: {True, False}).</li> </ul> <p>Errors</p> <ul> <li>Any exception raised by <code>int(getattr(game_layout, \"field_size\"))</code> or <code>int(getattr(game_layout, \"comms_size\"))</code> when constructing default models.</li> <li>Any exception raised by <code>LinTrainableAssistedModelA(...)</code> or <code>LinTrainableAssistedModelB(...)</code> constructors.</li> </ul> <p>Example</p> <pre><code>from Q_Sea_Battle.trainable_assisted_players import TrainableAssistedPlayers\n\nclass Layout:\n    field_size = 10\n    comms_size = 2\n\ntap = TrainableAssistedPlayers(Layout())\nplayer_a, player_b = tap.players()\ntap.set_explore(True)\ntap.reset()\n</code></pre>"},{"location":"trainable_assisted_players/#public-methods","title":"Public Methods","text":""},{"location":"trainable_assisted_players/#check_model_correspondence","title":"check_model_correspondence","text":"<p>Check that model A and B are compatible by comparing exposed <code>field_size</code> and <code>comms_size</code> attributes if present.</p> <p>Arguments</p> <ul> <li>None</li> </ul> <p>Returns</p> <ul> <li>bool, constraints: {True, False}, shape: scalar.</li> </ul> <p>Errors</p> <ul> <li>Not specified; internal exceptions while accessing model attributes are caught and result in <code>True</code>.</li> </ul>"},{"location":"trainable_assisted_players/#players","title":"players","text":"<p>Return the (PlayerA, PlayerB) wrappers, constructing them lazily and persisting wrapper state until <code>reset()</code>.</p> <p>Arguments</p> <ul> <li>None</li> </ul> <p>Returns</p> <ul> <li>Tuple[TrainableAssistedPlayerA, TrainableAssistedPlayerB], constraints: 2-tuple <code>(player_a, player_b)</code>, shape: length 2.</li> </ul> <p>Errors</p> <ul> <li>Any exception raised by <code>TrainableAssistedPlayerA(self.game_layout, model_a=self.model_a)</code> or <code>TrainableAssistedPlayerB(self.game_layout, model_b=self.model_b)</code> during lazy construction.</li> </ul>"},{"location":"trainable_assisted_players/#reset","title":"reset","text":"<p>Reset internal state between games by resetting both wrappers (if they exist) and clearing <code>previous</code>.</p> <p>Arguments</p> <ul> <li>None</li> </ul> <p>Returns</p> <ul> <li>NoneType, constraints: always <code>None</code>, shape: scalar.</li> </ul> <p>Errors</p> <ul> <li>Any exception raised by <code>self._playerA.reset()</code> or <code>self._playerB.reset()</code> if those wrappers exist.</li> </ul>"},{"location":"trainable_assisted_players/#set_explore","title":"set_explore","text":"<p>Set the exploration flag for both players and the wrapper itself.</p> <p>Arguments</p> <ul> <li>flag: bool, constraints: {True, False}, shape: scalar.</li> </ul> <p>Returns</p> <ul> <li>NoneType, constraints: always <code>None</code>, shape: scalar.</li> </ul> <p>Errors</p> <ul> <li>Not specified.</li> </ul>"},{"location":"trainable_assisted_players/#data-state","title":"Data &amp; State","text":"<ul> <li>has_log_probs: bool, constraints: {True, False}, shape: scalar; class attribute set to <code>True</code>.</li> <li>game_layout: Any, constraints: not specified; expected to provide <code>field_size</code> and <code>comms_size</code> attributes.</li> <li>model_a: LinTrainableAssistedModelA, constraints: not specified.</li> <li>model_b: LinTrainableAssistedModelB, constraints: not specified.</li> <li>explore: bool, constraints: {True, False}, shape: scalar; shared exploration flag propagated to wrappers when they exist.</li> <li>previous: Any | None, constraints: typically <code>(measurements_per_layer, outcomes_per_layer)</code> or <code>None</code>; when present, expected contract is <code>(meas_list, out_list)</code> where both are Python lists of tensors each shaped <code>(B, n2)</code>.</li> <li>has_prev: bool, constraints: {True, False}, shape: scalar; initialized to <code>True</code>.</li> <li>_playerA: Optional[TrainableAssistedPlayerA], constraints: either <code>None</code> or an instantiated wrapper; created lazily by <code>players()</code>.</li> <li>_playerB: Optional[TrainableAssistedPlayerB], constraints: either <code>None</code> or an instantiated wrapper; created lazily by <code>players()</code>.</li> </ul>"},{"location":"trainable_assisted_players/#planned-design-spec","title":"Planned (design-spec)","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"trainable_assisted_players/#deviations","title":"Deviations","text":"<ul> <li>Not specified.</li> </ul>"},{"location":"trainable_assisted_players/#notes-for-contributors","title":"Notes for Contributors","text":"<ul> <li><code>p_high</code>, <code>num_iterations</code>, <code>hidden_dim</code>, and <code>L_meas</code> are accepted by the constructor for forward compatibility but are currently unused by this class when creating default linear models.</li> <li>The <code>previous</code> state is intended to be produced by Player A and consumed by Player B via the <code>parent</code> reference set in <code>players()</code>; this module does not enforce tensor types beyond the documented contract in the module docstring.</li> </ul>"},{"location":"trainable_assisted_players/#related","title":"Related","text":"<ul> <li><code>Q_Sea_Battle.trainable_assisted_player_a.TrainableAssistedPlayerA</code></li> <li><code>Q_Sea_Battle.trainable_assisted_player_b.TrainableAssistedPlayerB</code></li> <li><code>Q_Sea_Battle.lin_trainable_assisted_model_a.LinTrainableAssistedModelA</code></li> <li><code>Q_Sea_Battle.lin_trainable_assisted_model_b.LinTrainableAssistedModelB</code></li> </ul>"},{"location":"trainable_assisted_players/#changelog","title":"Changelog","text":"<ul> <li>0.1: Initial version (as indicated by module docstring).</li> </ul>"}]}