{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "956a036c",
   "metadata": {},
   "source": [
    "# Tutorial: DIAL‑trained Neural Models\n",
    "##  Comparing DIAL‑trained Neural Models, Majority Baselines and Information-Theoretic Limits\n",
    "\n",
    "This notebook provides a **step‑by‑step tutorial** for reproducing the experiments that compare:\n",
    "\n",
    "1. **MajorityPlayers** — analytical baseline players with no learning.\n",
    "2. **NeuralNetPlayers trained with DIAL + DRU** — communication‑learning agents following the approach introduced in *Foerster et al., 2016*.\n",
    "3. **Information‑theoretic upper limits** on achievable performance.\n",
    "\n",
    "We explain the motivation for each component and annotate each section so the workflow is clear and reproducible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d41d2a5",
   "metadata": {},
   "source": [
    "## Summary of Results\n",
    "\n",
    "The table below (generated later in the notebook) reports:\n",
    "- The **Majority baseline** performance.\n",
    "- The **NeuralNet DIAL+DRU training performance**.\n",
    "- The **information‑theoretic bound**, computed using mutual information.\n",
    "\n",
    "These metrics allow us to evaluate how close learned communication strategies approach the theoretical optimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87051b2",
   "metadata": {},
   "source": [
    "### Results for comms_size = 1\n",
    "\n",
    "| field_size | comms_size | maj_mean | nn_dial_mean | info_limit |\n",
    "|------------|------------|----------|--------------|------------|\n",
    "| 4          | 1          | 0.5988   | 0.6037       | 0.6461     |\n",
    "| 8          | 1          | 0.5526   | 0.5475       | 0.5735     |\n",
    "| 16         | 1          | 0.5270   | 0.5284       | 0.5368     |\n",
    "| 32         | 1          | 0.5169   | 0.5093       | 0.5184     |\n",
    "\n",
    "### Results for comms_size = 2\n",
    "\n",
    "| field_size | comms_size | maj_mean | nn_dial_mean | info_limit |\n",
    "|------------|------------|----------|--------------|------------|\n",
    "| 4          | 2          | 0.6360   | 0.6122       | 0.7051     |\n",
    "| 8          | 2          | 0.5770   | 0.5607       | 0.6037     |\n",
    "| 16         | 2          | 0.5354   | 0.5401       | 0.5520     |\n",
    "| 32         | 2          | 0.5008   | 0.5157       | 0.5260     |\n",
    "\n",
    "### Results for comms_size = 4\n",
    "\n",
    "| field_size | comms_size | maj_mean | nn_dial_mean | info_limit |\n",
    "|------------|------------|----------|--------------|------------|\n",
    "| 4          | 4          | 0.6885   | 0.6222       | 0.7855     |\n",
    "| 8          | 4          | 0.5994   | 0.5560       | 0.6461     |\n",
    "| 16         | 4          | 0.5574   | 0.5202       | 0.5735     |\n",
    "| 32         | 4          | 0.5236   | 0.5147       | 0.5368     |\n",
    "\n",
    "### Full table\n",
    "\n",
    "| field_size | comms_size | maj_mean | nn_dial_mean | info_limit |\n",
    "|------------|------------|----------|--------------|------------|\n",
    "| 4          | 1          | 0.5988   | 0.6037       | 0.6461     |\n",
    "| 4          | 2          | 0.6442   | 0.6231       | 0.7051     |\n",
    "| 4          | 4          | 0.6823   | 0.6784       | 0.7855     |\n",
    "| 4          | 8          | 0.7422   | 0.6498       | 0.8900     |\n",
    "| 8          | 1          | 0.5541   | 0.5540       | 0.5735     |\n",
    "| 8          | 2          | 0.5755   | 0.5628       | 0.6037     |\n",
    "| 8          | 4          | 0.5955   | 0.5801       | 0.6461     |\n",
    "| 8          | 8          | 0.6388   | 0.5580       | 0.7051     |\n",
    "| 16         | 1          | 0.5287   | 0.5238       | 0.5368     |\n",
    "| 16         | 2          | 0.5311   | 0.5284       | 0.5520     |\n",
    "| 16         | 4          | 0.5596   | 0.5337       | 0.5735     |\n",
    "| 16         | 8          | 0.5662   | 0.5424       | 0.6037     |\n",
    "| 32         | 1          | 0.5129   | 0.5182       | 0.5184     |\n",
    "| 32         | 2          | 0.5194   | 0.5131       | 0.5260     |\n",
    "| 32         | 4          | 0.5161   | 0.5137       | 0.5368     |\n",
    "| 32         | 8          | 0.5301   | 0.5253       | 0.5520     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9207d677",
   "metadata": {},
   "source": [
    "## Overview of the Experimental Procedure\n",
    "\n",
    "This notebook builds and evaluates a full communication-learning pipeline:\n",
    "\n",
    "1. **Define a game layout** — field size, communication bandwidth, and number of games.\n",
    "2. **Train NeuralNetPlayers under DIAL + DRU** — sending *continuous* communication during training, with noise added via DRU to encourage discretisation.\n",
    "3. **Extract and freeze the trained models** — enabling deterministic communication at evaluation time.\n",
    "4. **Run a Tournament** — evaluating trained players, majority baselines, and computing theoretical limits.\n",
    "\n",
    "These components follow the structure proposed in referential-communication literature such as:\n",
    "- Foerster et al. *Learning to Communicate with Deep Multi‑Agent Reinforcement Learning* (2016).\n",
    "- Lowe et al. *Multi-Agent Actor‑Critic for Mixed Cooperation‑Competition* (2017).\n",
    "\n",
    "The goal is to assess how communication bandwidth and noise influence the emergence of useful signalling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f5bc32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWD set to: c:\\Users\\nly99857\\OneDrive - Philips\\SW Projects\\QSeaBattle\n",
      "sys.path[0]: C:\\Users\\nly99857\\OneDrive - Philips\\SW Projects\\QSeaBattle\\src\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def change_to_repo_root(marker: str = \"src\") -> None:\n",
    "    \"\"\"Change CWD to the repository root (parent of `src`).\"\"\"\n",
    "    here = Path.cwd()\n",
    "    for parent in [here] + list(here.parents):\n",
    "        if (parent / marker).is_dir():\n",
    "            os.chdir(parent)\n",
    "            break\n",
    "\n",
    "change_to_repo_root()\n",
    "print(\"CWD set to:\", os.getcwd())\n",
    "\n",
    "src_path = Path(\"src\").resolve()\n",
    "if str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "print(\"sys.path[0]:\", sys.path[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a0d223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy random seed: 1232\n",
      "Python random seed: 2464\n",
      "TF random seed: 4928\n",
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "import Q_Sea_Battle as qsb\n",
    "from Q_Sea_Battle.dru_utilities import dru_train\n",
    "from Q_Sea_Battle.reference_performance_utilities import limit_from_mutual_information\n",
    "\n",
    "SEED = 1232\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED * 2)\n",
    "tf.random.set_seed(SEED * 4)\n",
    "\n",
    "tf.config.run_functions_eagerly(True)\n",
    "\n",
    "print(\"NumPy random seed:\", SEED)\n",
    "print(\"Python random seed:\", SEED * 2)\n",
    "print(\"TF random seed:\", SEED * 4)\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f959465",
   "metadata": {},
   "outputs": [],
   "source": [
    "FIELD_SIZES = [4, 8, 16, 32]\n",
    "COMMS_SIZES = [1,2,4,8]\n",
    "\n",
    "# RL hyperparameters (can be tuned)\n",
    "NUM_EPOCHS = 128\n",
    "BATCHES_PER_EPOCH = 40\n",
    "BATCH_SIZE = 2048*16\n",
    "SIGMA_TRAIN = 2.0\n",
    "CLIP_RANGE = (-10.0, 10.0)\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "# Sigma annealing for DRU: high noise -> low noise over epochs\n",
    "SIGMA_START = 2.0\n",
    "SIGMA_END = 0.3\n",
    "\n",
    "N_GAMES_TOURNAMENT = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cca7c5",
   "metadata": {},
   "source": [
    "## DIAL + DRU Training Helpers\n",
    "\n",
    "This section defines the helper functions used for training the neural agents via **DIAL (Differentiable Inter‑Agent Learning)**.\n",
    "\n",
    "DIAL allows gradients to flow **through communication channels**, enabling agents to *learn* what messages to send. However, communication must ultimately be discrete during evaluation. To bridge this, we use **DRU (Discretise / Regularise Unit)**:\n",
    "\n",
    "- During **training**, DRU adds continuous noise, encouraging messages to become separable.\n",
    "- During **evaluation**, DRU reduces to a hard threshold, producing discrete bits.\n",
    "\n",
    "The helper functions here:\n",
    "- simulate batches of games,\n",
    "- run model A to generate communication logits,\n",
    "- pass DRU‑processed messages to model B,\n",
    "- compute loss from observed rewards,\n",
    "- update the parameters via policy gradients.\n",
    "\n",
    "This mirrors the training structure of the original DIAL paper but adapted for the QSeaBattle game setup.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2254403",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from Q_Sea_Battle.tournament import Tournament\n",
    "from Q_Sea_Battle.majority_players import MajorityPlayers\n",
    "from Q_Sea_Battle.neural_net_players import NeuralNetPlayers\n",
    "\n",
    "\n",
    "def sample_batch(layout: qsb.GameLayout, batch_size: int):\n",
    "    \"\"\"Sample (fields, guns, cell_values) for the given layout.\n",
    "\n",
    "    - fields: shape (B, n²), Bernoulli(enemy_probability)\n",
    "    - guns:   shape (B, n²), one-hot\n",
    "    - cell_values: shape (B, 1), field value at the gun index\n",
    "    \"\"\"\n",
    "    n2 = layout.field_size ** 2\n",
    "    p = layout.enemy_probability\n",
    "\n",
    "    fields = np.random.binomial(1, p, size=(batch_size, n2)).astype(\"float32\")\n",
    "\n",
    "    guns = np.zeros((batch_size, n2), dtype=\"float32\")\n",
    "    gun_indices = np.random.randint(0, n2, size=(batch_size,))\n",
    "    guns[np.arange(batch_size), gun_indices] = 1.0\n",
    "\n",
    "    cell_values = (fields * guns).sum(axis=1, keepdims=True).astype(\"float32\")\n",
    "    return fields, guns, cell_values\n",
    "\n",
    "\n",
    "def evaluate_players_in_tournament(\n",
    "    layout: qsb.GameLayout,\n",
    "    players_factory,\n",
    "    label: str = \"\",\n",
    ") -> float:\n",
    "    \"\"\"Run a tournament and return mean reward.\n",
    "\n",
    "    `players_factory` is any Players subclass instance, e.g.\n",
    "    MajorityPlayers(layout) or NeuralNetPlayers(layout, ...).\n",
    "    \"\"\"\n",
    "    game_env = qsb.GameEnv(game_layout=layout)\n",
    "    tournament = Tournament(game_env=game_env, players=players_factory, game_layout=layout)\n",
    "    log = tournament.tournament()\n",
    "    mean_reward, std_err = log.outcome()\n",
    "\n",
    "    if label:\n",
    "        print(\n",
    "            f\"{label}: mean reward = {mean_reward:.4f} ± {std_err:.4f} \"\n",
    "        )\n",
    "    else:\n",
    "        print(\n",
    "            f\"mean reward = {mean_reward:.4f} ± {std_err:.4f} \"\n",
    "        )\n",
    "\n",
    "    return mean_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760b4d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dial_pg_update(\n",
    "    model_a: tf.keras.Model,\n",
    "    model_b: tf.keras.Model,\n",
    "    optimizer: tf.keras.optimizers.Optimizer,\n",
    "    layout: qsb.GameLayout,\n",
    "    batch_size: int = 512,\n",
    "    sigma: float = 2.0,\n",
    "    clip_range=(-10.0, 10.0),\n",
    "    entropy_coeff: float = 0.01,\n",
    "    normalize_adv: bool = True,\n",
    ") -> tuple[float, float]:\n",
    "    \"\"\"Improved DIAL-style policy-gradient update step with DRU.\n",
    "\n",
    "    Compared to the earlier version, this adds:\n",
    "    - optional advantage normalization (variance reduction),\n",
    "    - an entropy bonus on the shoot policy.\n",
    "    \"\"\"\n",
    "    fields_np, guns_np, cell_values_np = sample_batch(layout, batch_size)\n",
    "\n",
    "    fields_tf = tf.convert_to_tensor(fields_np, dtype=tf.float32)\n",
    "    fields_scaled = fields_tf - 0.5  # scale {0,1} -> [-0.5, 0.5] to match NeuralNetPlayerA\n",
    "    guns_tf = tf.convert_to_tensor(guns_np, dtype=tf.float32)\n",
    "    cell_values_tf = tf.convert_to_tensor(cell_values_np, dtype=tf.float32)\n",
    "\n",
    "    n2 = layout.field_size ** 2\n",
    "    denom = float(max(1, n2 - 1))\n",
    "    gun_indices = tf.argmax(guns_tf, axis=1, output_type=tf.int32)\n",
    "    gun_idx_norm = tf.cast(gun_indices, tf.float32) / denom\n",
    "    gun_idx_norm = tf.reshape(gun_idx_norm, (-1, 1))\n",
    "\n",
    "\n",
    "    eps = 1e-8\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        # A produces communication logits\n",
    "        comm_logits = model_a(fields_scaled, training=True)          # (B, m)\n",
    "\n",
    "        # DRU (train mode): logits + noise -> logistic\n",
    "        comm_cont = dru_train(comm_logits, sigma=sigma, clip_range=clip_range)\n",
    "        comm_cont = tf.cast(comm_cont, tf.float32)               # (B, m) in (0,1)\n",
    "\n",
    "        # B receives gun + continuous comm\n",
    "        x_b = tf.concat([gun_idx_norm, comm_cont], axis=1)          # (B, 1 + m)\n",
    "        shoot_logits = model_b(x_b, training=True)               # (B, 1)\n",
    "\n",
    "        probs = tf.nn.sigmoid(shoot_logits)\n",
    "        rnd = tf.random.uniform(tf.shape(probs))\n",
    "        actions = tf.cast(rnd < probs, tf.float32)               # (B, 1) in {0,1}\n",
    "\n",
    "        # Team reward: 1 if correct guess of the field bit at the gun index\n",
    "        rewards = tf.cast(tf.equal(actions, cell_values_tf), tf.float32)\n",
    "\n",
    "        # Baseline and advantage\n",
    "        baseline = tf.reduce_mean(rewards)\n",
    "        advantages = rewards - baseline\n",
    "\n",
    "        if normalize_adv:\n",
    "            adv_std = tf.math.reduce_std(advantages) + 1e-8\n",
    "            advantages = advantages / adv_std\n",
    "\n",
    "        advantages = tf.stop_gradient(advantages)\n",
    "\n",
    "        # Log-prob of the sampled action under Bernoulli(probs)\n",
    "        log_probs = (\n",
    "            actions * tf.math.log(probs + eps)\n",
    "            + (1.0 - actions) * tf.math.log(1.0 - probs + eps)\n",
    "        )\n",
    "\n",
    "        # Policy entropy for Bernoulli(probs)\n",
    "        entropy = -(\n",
    "            probs * tf.math.log(probs + eps)\n",
    "            + (1.0 - probs) * tf.math.log(1.0 - probs + eps)\n",
    "        )\n",
    "\n",
    "        # REINFORCE loss with entropy regularization\n",
    "        loss_pg = -tf.reduce_mean(log_probs * advantages)\n",
    "        loss_ent = -tf.reduce_mean(entropy)   # negative so adding pushes up entropy\n",
    "        loss = loss_pg + entropy_coeff * loss_ent\n",
    "\n",
    "    params = model_a.trainable_variables + model_b.trainable_variables\n",
    "    grads = tape.gradient(loss, params)\n",
    "    optimizer.apply_gradients(zip(grads, params))\n",
    "\n",
    "    mean_reward = float(tf.reduce_mean(rewards).numpy())\n",
    "    loss_value = float(loss.numpy())\n",
    "    return mean_reward, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a80e05b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running updated sweep with advantage normalization, entropy bonus, and sigma annealing...\n",
      "Field_size = 4, comms_size = 1\n",
      "\tMajorityPlayers: mean reward = 0.5988 ± 0.0049 \n",
      "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.6037 ± 0.0049 \n",
      "\tInfo-theoretic upper bound (noiseless channel): 0.6461\n",
      "Saved models to notebooks/models/neural_net_model_a_f4_c1.keras and notebooks/models/neural_net_model_b_f4_c1.keras\n",
      "Field_size = 4, comms_size = 2\n",
      "\tMajorityPlayers: mean reward = 0.6442 ± 0.0048 \n",
      "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.6231 ± 0.0048 \n",
      "\tInfo-theoretic upper bound (noiseless channel): 0.7051\n",
      "Saved models to notebooks/models/neural_net_model_a_f4_c2.keras and notebooks/models/neural_net_model_b_f4_c2.keras\n",
      "Field_size = 4, comms_size = 4\n",
      "\tMajorityPlayers: mean reward = 0.6823 ± 0.0047 \n",
      "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.6784 ± 0.0047 \n",
      "\tInfo-theoretic upper bound (noiseless channel): 0.7855\n",
      "Saved models to notebooks/models/neural_net_model_a_f4_c4.keras and notebooks/models/neural_net_model_b_f4_c4.keras\n",
      "Field_size = 4, comms_size = 8\n",
      "\tMajorityPlayers: mean reward = 0.7422 ± 0.0044 \n",
      "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.6498 ± 0.0048 \n",
      "\tInfo-theoretic upper bound (noiseless channel): 0.8900\n",
      "Saved models to notebooks/models/neural_net_model_a_f4_c8.keras and notebooks/models/neural_net_model_b_f4_c8.keras\n",
      "Field_size = 8, comms_size = 1\n",
      "\tMajorityPlayers: mean reward = 0.5541 ± 0.0050 \n",
      "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5540 ± 0.0050 \n",
      "\tInfo-theoretic upper bound (noiseless channel): 0.5735\n",
      "Saved models to notebooks/models/neural_net_model_a_f8_c1.keras and notebooks/models/neural_net_model_b_f8_c1.keras\n",
      "Field_size = 8, comms_size = 2\n",
      "\tMajorityPlayers: mean reward = 0.5755 ± 0.0049 \n",
      "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5628 ± 0.0050 \n",
      "\tInfo-theoretic upper bound (noiseless channel): 0.6037\n",
      "Saved models to notebooks/models/neural_net_model_a_f8_c2.keras and notebooks/models/neural_net_model_b_f8_c2.keras\n",
      "Field_size = 8, comms_size = 4\n",
      "\tMajorityPlayers: mean reward = 0.5955 ± 0.0049 \n",
      "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5801 ± 0.0049 \n",
      "\tInfo-theoretic upper bound (noiseless channel): 0.6461\n",
      "Saved models to notebooks/models/neural_net_model_a_f8_c4.keras and notebooks/models/neural_net_model_b_f8_c4.keras\n",
      "Field_size = 8, comms_size = 8\n",
      "\tMajorityPlayers: mean reward = 0.6388 ± 0.0048 \n",
      "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5580 ± 0.0050 \n",
      "\tInfo-theoretic upper bound (noiseless channel): 0.7051\n",
      "Saved models to notebooks/models/neural_net_model_a_f8_c8.keras and notebooks/models/neural_net_model_b_f8_c8.keras\n",
      "Field_size = 16, comms_size = 1\n",
      "\tMajorityPlayers: mean reward = 0.5287 ± 0.0050 \n",
      "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5238 ± 0.0050 \n",
      "\tInfo-theoretic upper bound (noiseless channel): 0.5368\n",
      "Saved models to notebooks/models/neural_net_model_a_f16_c1.keras and notebooks/models/neural_net_model_b_f16_c1.keras\n",
      "Field_size = 16, comms_size = 2\n",
      "\tMajorityPlayers: mean reward = 0.5311 ± 0.0050 \n",
      "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5284 ± 0.0050 \n",
      "\tInfo-theoretic upper bound (noiseless channel): 0.5520\n",
      "Saved models to notebooks/models/neural_net_model_a_f16_c2.keras and notebooks/models/neural_net_model_b_f16_c2.keras\n",
      "Field_size = 16, comms_size = 4\n",
      "\tMajorityPlayers: mean reward = 0.5596 ± 0.0050 \n",
      "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5337 ± 0.0050 \n",
      "\tInfo-theoretic upper bound (noiseless channel): 0.5735\n",
      "Saved models to notebooks/models/neural_net_model_a_f16_c4.keras and notebooks/models/neural_net_model_b_f16_c4.keras\n",
      "Field_size = 16, comms_size = 8\n",
      "\tMajorityPlayers: mean reward = 0.5662 ± 0.0050 \n",
      "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5424 ± 0.0050 \n",
      "\tInfo-theoretic upper bound (noiseless channel): 0.6037\n",
      "Saved models to notebooks/models/neural_net_model_a_f16_c8.keras and notebooks/models/neural_net_model_b_f16_c8.keras\n",
      "Field_size = 32, comms_size = 1\n",
      "\tMajorityPlayers: mean reward = 0.5129 ± 0.0050 \n",
      "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5182 ± 0.0050 \n",
      "\tInfo-theoretic upper bound (noiseless channel): 0.5184\n",
      "Saved models to notebooks/models/neural_net_model_a_f32_c1.keras and notebooks/models/neural_net_model_b_f32_c1.keras\n",
      "Field_size = 32, comms_size = 2\n",
      "\tMajorityPlayers: mean reward = 0.5194 ± 0.0050 \n",
      "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5131 ± 0.0050 \n",
      "\tInfo-theoretic upper bound (noiseless channel): 0.5260\n",
      "Saved models to notebooks/models/neural_net_model_a_f32_c2.keras and notebooks/models/neural_net_model_b_f32_c2.keras\n",
      "Field_size = 32, comms_size = 4\n",
      "\tMajorityPlayers: mean reward = 0.5161 ± 0.0050 \n",
      "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5137 ± 0.0050 \n",
      "\tInfo-theoretic upper bound (noiseless channel): 0.5368\n",
      "Saved models to notebooks/models/neural_net_model_a_f32_c4.keras and notebooks/models/neural_net_model_b_f32_c4.keras\n",
      "Field_size = 32, comms_size = 8\n",
      "\tMajorityPlayers: mean reward = 0.5301 ± 0.0050 \n",
      "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5253 ± 0.0050 \n",
      "\tInfo-theoretic upper bound (noiseless channel): 0.5520\n",
      "Saved models to notebooks/models/neural_net_model_a_f32_c8.keras and notebooks/models/neural_net_model_b_f32_c8.keras\n"
     ]
    }
   ],
   "source": [
    "def sigma_for_epoch(epoch: int, num_epochs: int) -> float:\n",
    "    \"\"\"Linearly interpolate sigma from SIGMA_START to SIGMA_END over epochs.\"\"\"\n",
    "    t = epoch / max(1, num_epochs)\n",
    "    return SIGMA_START * (1.0 - t) + SIGMA_END * t\n",
    "\n",
    "\n",
    "print(\"\\nRunning updated sweep with advantage normalization, entropy bonus, and sigma annealing...\")\n",
    "\n",
    "results_updated = []\n",
    "\n",
    "for n in FIELD_SIZES:\n",
    "    for m in COMMS_SIZES:\n",
    "        print(f\"Field_size = {n}, comms_size = {m}\")\n",
    "\n",
    "        layout = qsb.GameLayout(\n",
    "            field_size=n,\n",
    "            comms_size=m,\n",
    "            enemy_probability=0.5,\n",
    "            channel_noise=0.0,\n",
    "            number_of_games_in_tournament=N_GAMES_TOURNAMENT,\n",
    "        )\n",
    "\n",
    "        # Majority players as before\n",
    "        majority_players = MajorityPlayers(layout)\n",
    "        maj_mean = evaluate_players_in_tournament(\n",
    "            layout, majority_players,\n",
    "            label=\"\\tMajorityPlayers\"\n",
    "        )\n",
    "\n",
    "        # Fresh NeuralNetPlayers and models\n",
    "        nn_players = NeuralNetPlayers(game_layout=layout, explore=True)\n",
    "        player_a, player_b = nn_players.players()\n",
    "        model_a = nn_players.model_a\n",
    "        model_b = nn_players.model_b\n",
    "        assert model_a is not None and model_b is not None\n",
    "\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "        for epoch in range(1, NUM_EPOCHS + 1):\n",
    "            sigma_now = sigma_for_epoch(epoch, NUM_EPOCHS)\n",
    "            epoch_rewards = []\n",
    "            epoch_losses = []\n",
    "\n",
    "            for _ in range(BATCHES_PER_EPOCH):\n",
    "                r, l = dial_pg_update(\n",
    "                    model_a=model_a,\n",
    "                    model_b=model_b,\n",
    "                    optimizer=optimizer,\n",
    "                    layout=layout,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    sigma=sigma_now,\n",
    "                    clip_range=CLIP_RANGE,\n",
    "                    entropy_coeff=0.01,\n",
    "                    normalize_adv=True,\n",
    "                )\n",
    "                epoch_rewards.append(r)\n",
    "                epoch_losses.append(l)\n",
    "\n",
    "        # Evaluate trained neural nets (greedy play)\n",
    "        nn_players_eval = NeuralNetPlayers(\n",
    "            game_layout=layout,\n",
    "            model_a=model_a,\n",
    "            model_b=model_b,\n",
    "            explore=False,\n",
    "        )\n",
    "        nn_mean = evaluate_players_in_tournament(\n",
    "            layout, nn_players_eval,\n",
    "            label=\"\\tNeuralNetPlayers (DIAL+DRU)\"\n",
    "        )\n",
    "\n",
    "        info_limit = limit_from_mutual_information(\n",
    "            field_size=n,\n",
    "            comms_size=m,\n",
    "            channel_noise=0.0,\n",
    "            accuracy_in_digits=10,\n",
    "        )\n",
    "\n",
    "        print(\n",
    "            f\"\\tInfo-theoretic upper bound (noiseless channel): {info_limit:.4f}\"\n",
    "        )\n",
    "\n",
    "        results_updated.append(\n",
    "            {\n",
    "                \"field_size\": n,\n",
    "                \"comms_size\": m,\n",
    "                \"maj_mean\": maj_mean,\n",
    "                \"nn_dial_mean\": nn_mean,\n",
    "                \"info_limit\": info_limit,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Store trained models for this (field_size, comms_size) setting\n",
    "        filename_a = f\"notebooks/models/neural_net_model_a_f{n}_c{m}.keras\"\n",
    "        filename_b = f\"notebooks/models/neural_net_model_b_f{n}_c{m}.keras\"\n",
    "        nn_players.store_models(filename_a, filename_b)\n",
    "        print(f\"Saved models to {filename_a} and {filename_b}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bb6c48b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>field_size</th>\n",
       "      <th>comms_size</th>\n",
       "      <th>maj_mean</th>\n",
       "      <th>nn_dial_mean</th>\n",
       "      <th>info_limit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5988</td>\n",
       "      <td>0.6037</td>\n",
       "      <td>0.646103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.6442</td>\n",
       "      <td>0.6231</td>\n",
       "      <td>0.705074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.6823</td>\n",
       "      <td>0.6784</td>\n",
       "      <td>0.785498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0.7422</td>\n",
       "      <td>0.6498</td>\n",
       "      <td>0.889972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5541</td>\n",
       "      <td>0.5540</td>\n",
       "      <td>0.573455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5755</td>\n",
       "      <td>0.5628</td>\n",
       "      <td>0.603692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5955</td>\n",
       "      <td>0.5801</td>\n",
       "      <td>0.646103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.6388</td>\n",
       "      <td>0.5580</td>\n",
       "      <td>0.705074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5287</td>\n",
       "      <td>0.5238</td>\n",
       "      <td>0.536777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5311</td>\n",
       "      <td>0.5284</td>\n",
       "      <td>0.551988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5596</td>\n",
       "      <td>0.5337</td>\n",
       "      <td>0.573455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>0.5662</td>\n",
       "      <td>0.5424</td>\n",
       "      <td>0.603692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>0.5129</td>\n",
       "      <td>0.5182</td>\n",
       "      <td>0.518395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0.5194</td>\n",
       "      <td>0.5131</td>\n",
       "      <td>0.526011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5161</td>\n",
       "      <td>0.5137</td>\n",
       "      <td>0.536777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>0.5301</td>\n",
       "      <td>0.5253</td>\n",
       "      <td>0.551988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    field_size  comms_size  maj_mean  nn_dial_mean  info_limit\n",
       "0            4           1    0.5988        0.6037    0.646103\n",
       "1            4           2    0.6442        0.6231    0.705074\n",
       "2            4           4    0.6823        0.6784    0.785498\n",
       "3            4           8    0.7422        0.6498    0.889972\n",
       "4            8           1    0.5541        0.5540    0.573455\n",
       "5            8           2    0.5755        0.5628    0.603692\n",
       "6            8           4    0.5955        0.5801    0.646103\n",
       "7            8           8    0.6388        0.5580    0.705074\n",
       "8           16           1    0.5287        0.5238    0.536777\n",
       "9           16           2    0.5311        0.5284    0.551988\n",
       "10          16           4    0.5596        0.5337    0.573455\n",
       "11          16           8    0.5662        0.5424    0.603692\n",
       "12          32           1    0.5129        0.5182    0.518395\n",
       "13          32           2    0.5194        0.5131    0.526011\n",
       "14          32           4    0.5161        0.5137    0.536777\n",
       "15          32           8    0.5301        0.5253    0.551988"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_results_updated = pd.DataFrame(results_updated)\n",
    "df_results_updated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3465703e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates how **communication‑learning agents** can be trained and compared against both **analytical baselines** and **information‑theoretic limits**.\n",
    "\n",
    "With the updated model interface, the pipeline remains unchanged except for input preprocessing, making the tutorial stable for future experiments.\n",
    "\n",
    "You can now adjust:\n",
    "- communication bandwidth,\n",
    "- field size,\n",
    "- DRU noise parameters,\n",
    "to explore the emergence of communication under different constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e71674",
   "metadata": {},
   "source": [
    "#Result from 840 mins run\n",
    "\n",
    "##Settings\n",
    "\n",
    "FIELD_SIZES = [4, 8, 16, 32]\n",
    "COMMS_SIZES = [1,2,4,8]\n",
    "\n",
    "* RL hyperparameters (can be tuned)\n",
    "NUM_EPOCHS = 128\n",
    "BATCHES_PER_EPOCH = 40\n",
    "BATCH_SIZE = 2048*16\n",
    "SIGMA_TRAIN = 2.0\n",
    "CLIP_RANGE = (-10.0, 10.0)\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "* Sigma annealing for DRU: high noise -> low noise over epochs\n",
    "SIGMA_START = 2.0\n",
    "SIGMA_END = 0.3\n",
    "\n",
    "N_GAMES_TOURNAMENT = 10000\n",
    "\n",
    "## \n",
    "(range is st deviation / sqr(number of games))\n",
    "Running updated sweep with advantage normalization, entropy bonus, and sigma annealing...\n",
    "Field_size = 4, comms_size = 1\n",
    "\tMajorityPlayers: mean reward = 0.5988 ± 0.0049 \n",
    "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.6037 ± 0.0049 \n",
    "\tInfo-theoretic upper bound (noiseless channel): 0.6461\n",
    "Saved models to notebooks/models/neural_net_model_a_f4_c1.keras and notebooks/models/neural_net_model_b_f4_c1.keras\n",
    "Field_size = 4, comms_size = 2\n",
    "\tMajorityPlayers: mean reward = 0.6442 ± 0.0048 \n",
    "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.6231 ± 0.0048 \n",
    "\tInfo-theoretic upper bound (noiseless channel): 0.7051\n",
    "Saved models to notebooks/models/neural_net_model_a_f4_c2.keras and notebooks/models/neural_net_model_b_f4_c2.keras\n",
    "Field_size = 4, comms_size = 4\n",
    "\tMajorityPlayers: mean reward = 0.6823 ± 0.0047 \n",
    "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.6784 ± 0.0047 \n",
    "\tInfo-theoretic upper bound (noiseless channel): 0.7855\n",
    "Saved models to notebooks/models/neural_net_model_a_f4_c4.keras and notebooks/models/neural_net_model_b_f4_c4.keras\n",
    "Field_size = 4, comms_size = 8\n",
    "\tMajorityPlayers: mean reward = 0.7422 ± 0.0044 \n",
    "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.6498 ± 0.0048 \n",
    "\tInfo-theoretic upper bound (noiseless channel): 0.8900\n",
    "Saved models to notebooks/models/neural_net_model_a_f4_c8.keras and notebooks/models/neural_net_model_b_f4_c8.keras\n",
    "Field_size = 8, comms_size = 1\n",
    "\tMajorityPlayers: mean reward = 0.5541 ± 0.0050 \n",
    "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5540 ± 0.0050 \n",
    "\tInfo-theoretic upper bound (noiseless channel): 0.5735\n",
    "Saved models to notebooks/models/neural_net_model_a_f8_c1.keras and notebooks/models/neural_net_model_b_f8_c1.keras\n",
    "Field_size = 8, comms_size = 2\n",
    "\tMajorityPlayers: mean reward = 0.5755 ± 0.0049 \n",
    "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5628 ± 0.0050 \n",
    "\tInfo-theoretic upper bound (noiseless channel): 0.6037\n",
    "Saved models to notebooks/models/neural_net_model_a_f8_c2.keras and notebooks/models/neural_net_model_b_f8_c2.keras\n",
    "Field_size = 8, comms_size = 4\n",
    "\tMajorityPlayers: mean reward = 0.5955 ± 0.0049 \n",
    "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5801 ± 0.0049 \n",
    "\tInfo-theoretic upper bound (noiseless channel): 0.6461\n",
    "Saved models to notebooks/models/neural_net_model_a_f8_c4.keras and notebooks/models/neural_net_model_b_f8_c4.keras\n",
    "Field_size = 8, comms_size = 8\n",
    "\tMajorityPlayers: mean reward = 0.6388 ± 0.0048 \n",
    "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5580 ± 0.0050 \n",
    "\tInfo-theoretic upper bound (noiseless channel): 0.7051\n",
    "Saved models to notebooks/models/neural_net_model_a_f8_c8.keras and notebooks/models/neural_net_model_b_f8_c8.keras\n",
    "Field_size = 16, comms_size = 1\n",
    "\tMajorityPlayers: mean reward = 0.5287 ± 0.0050 \n",
    "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5238 ± 0.0050 \n",
    "\tInfo-theoretic upper bound (noiseless channel): 0.5368\n",
    "Saved models to notebooks/models/neural_net_model_a_f16_c1.keras and notebooks/models/neural_net_model_b_f16_c1.keras\n",
    "Field_size = 16, comms_size = 2\n",
    "\tMajorityPlayers: mean reward = 0.5311 ± 0.0050 \n",
    "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5284 ± 0.0050 \n",
    "\tInfo-theoretic upper bound (noiseless channel): 0.5520\n",
    "Saved models to notebooks/models/neural_net_model_a_f16_c2.keras and notebooks/models/neural_net_model_b_f16_c2.keras\n",
    "Field_size = 16, comms_size = 4\n",
    "\tMajorityPlayers: mean reward = 0.5596 ± 0.0050 \n",
    "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5337 ± 0.0050 \n",
    "\tInfo-theoretic upper bound (noiseless channel): 0.5735\n",
    "Saved models to notebooks/models/neural_net_model_a_f16_c4.keras and notebooks/models/neural_net_model_b_f16_c4.keras\n",
    "Field_size = 16, comms_size = 8\n",
    "\tMajorityPlayers: mean reward = 0.5662 ± 0.0050 \n",
    "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5424 ± 0.0050 \n",
    "\tInfo-theoretic upper bound (noiseless channel): 0.6037\n",
    "Saved models to notebooks/models/neural_net_model_a_f16_c8.keras and notebooks/models/neural_net_model_b_f16_c8.keras\n",
    "Field_size = 32, comms_size = 1\n",
    "\tMajorityPlayers: mean reward = 0.5129 ± 0.0050 \n",
    "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5182 ± 0.0050 \n",
    "\tInfo-theoretic upper bound (noiseless channel): 0.5184\n",
    "Saved models to notebooks/models/neural_net_model_a_f32_c1.keras and notebooks/models/neural_net_model_b_f32_c1.keras\n",
    "Field_size = 32, comms_size = 2\n",
    "\tMajorityPlayers: mean reward = 0.5194 ± 0.0050 \n",
    "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5131 ± 0.0050 \n",
    "\tInfo-theoretic upper bound (noiseless channel): 0.5260\n",
    "Saved models to notebooks/models/neural_net_model_a_f32_c2.keras and notebooks/models/neural_net_model_b_f32_c2.keras\n",
    "Field_size = 32, comms_size = 4\n",
    "\tMajorityPlayers: mean reward = 0.5161 ± 0.0050 \n",
    "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5137 ± 0.0050 \n",
    "\tInfo-theoretic upper bound (noiseless channel): 0.5368\n",
    "Saved models to notebooks/models/neural_net_model_a_f32_c4.keras and notebooks/models/neural_net_model_b_f32_c4.keras\n",
    "Field_size = 32, comms_size = 8\n",
    "\tMajorityPlayers: mean reward = 0.5301 ± 0.0050 \n",
    "\tNeuralNetPlayers (DIAL+DRU): mean reward = 0.5253 ± 0.0050 \n",
    "\tInfo-theoretic upper bound (noiseless channel): 0.5520\n",
    "Saved models to notebooks/models/neural_net_model_a_f32_c8.keras and notebooks/models/neural_net_model_b_f32_c8.keras\n",
    "\n",
    "\n",
    "\n",
    "## Table\n",
    "\n",
    "field_size\tcomms_size\tmaj_mean\tnn_dial_mean\tinfo_limit\n",
    "0\t4\t1\t0.5988\t0.6037\t0.646103\n",
    "1\t4\t2\t0.6442\t0.6231\t0.705074\n",
    "2\t4\t4\t0.6823\t0.6784\t0.785498\n",
    "3\t4\t8\t0.7422\t0.6498\t0.889972\n",
    "4\t8\t1\t0.5541\t0.5540\t0.573455\n",
    "5\t8\t2\t0.5755\t0.5628\t0.603692\n",
    "6\t8\t4\t0.5955\t0.5801\t0.646103\n",
    "7\t8\t8\t0.6388\t0.5580\t0.705074\n",
    "8\t16\t1\t0.5287\t0.5238\t0.536777\n",
    "9\t16\t2\t0.5311\t0.5284\t0.551988\n",
    "10\t16\t4\t0.5596\t0.5337\t0.573455\n",
    "11\t16\t8\t0.5662\t0.5424\t0.603692\n",
    "12\t32\t1\t0.5129\t0.5182\t0.518395\n",
    "13\t32\t2\t0.5194\t0.5131\t0.526011\n",
    "14\t32\t4\t0.5161\t0.5137\t0.536777\n",
    "15\t32\t8\t0.5301\t0.5253\t0.551988"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kernel_qseabattle",
   "language": "python",
   "name": "kernel_qseabattle"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
